{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm52p-D24JMJ",
    "tags": []
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6242,
     "status": "ok",
     "timestamp": 1681340988065,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "hHHkZLtiDKyI",
    "outputId": "1778deac-7ebd-4ed8-edc4-60d88afe3d16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 11:38:02.970537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 11:38:07.348478: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-25 11:38:16.742048: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bmoxon/anaconda3/envs/SWISC_310_tf210/lib/libfabric:\n",
      "2024-04-25 11:38:16.743127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bmoxon/anaconda3/envs/SWISC_310_tf210/lib/libfabric:\n",
      "2024-04-25 11:38:16.743155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n",
      "3.8.4\n"
     ]
    }
   ],
   "source": [
    "# import packages \n",
    "\n",
    "%load_ext autoreload\n",
    "# OS Imports\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "\n",
    "# Garbage collection \n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Basic math/data processing packages\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import matplotlib\n",
    "import random\n",
    "import time \n",
    "import datetime\n",
    "from collections import deque\n",
    "\n",
    "# Imports for CSV Processing\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm, PowerNorm\n",
    "import matplotlib.style as mplstyle\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle\n",
    "import openpyxl\n",
    "\n",
    "# Imports for Deep Learning for data preparation and SVM \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (mean_squared_error, confusion_matrix, \n",
    "    ConfusionMatrixDisplay, classification_report, \n",
    "    cohen_kappa_score, matthews_corrcoef)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Keras/TF Packages\n",
    "import keras\n",
    "\n",
    "from keras.regularizers import l1\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import initializers\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import (Dense, Dropout, Activation, Flatten, Input, \n",
    "                          TimeDistributed, Reshape, Permute, Flatten, \n",
    "                          RepeatVector, Bidirectional, InputLayer,  \n",
    "                          AlphaDropout, Normalization, MaxPooling2D, Embedding, \n",
    "                          ConvLSTM1D, Attention, TimeDistributed, LocallyConnected1D,\n",
    "                          LSTM, GRU)\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.optimizers import Nadam, Adam, SGD, Adadelta, Adamax, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "import tensorflow as tf\n",
    "\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n",
    "\n",
    "print(matplotlib.__version__)\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1681341029181,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "SgsbVTAIE7oT",
    "outputId": "a2ffc773-ff81-4593-abac-d970bf868b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 25 11:38:23 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:81:00.0  On |                  N/A |\n",
      "| 30%   27C    P8     1W /  38W |    244MiB /  1994MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2475      G   /usr/lib/xorg/Xorg                 29MiB |\n",
      "|    0   N/A  N/A      3285      G   /usr/lib/xorg/Xorg                 92MiB |\n",
      "|    0   N/A  N/A      3500      G   /usr/bin/gnome-shell               26MiB |\n",
      "|    0   N/A  N/A      4554      G   ...--variations-seed-version        7MiB |\n",
      "|    0   N/A  N/A     33513      G   ...on=20240424-104239.999000       74MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "GPU name:  []\n",
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 11:38:24.379330: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bmoxon/anaconda3/envs/SWISC_310_tf210/lib/libfabric:\n",
      "2024-04-25 11:38:24.379445: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Print GPU details if recognized\n",
    "# Ensure GPU RAM is >10 GB\n",
    "!nvidia-smi\n",
    "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Setup CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fTYy8yQP7r5u"
   },
   "outputs": [],
   "source": [
    "# Consistent random seed selection improves reliability of Keras training performance\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "fTYy8yQP7r5u"
   },
   "outputs": [],
   "source": [
    "def setPaths(data_path,epoch_length):\n",
    "    # Set paths to preprocessed data based on epoch length\n",
    "    match epoch_length:\n",
    "        case 20:\n",
    "            data_path='D:/Final_Export/'\n",
    "            %cd $data_path\n",
    "\n",
    "        case 4:\n",
    "            data_path='D:/npy_no_z_4/'\n",
    "            %cd $data_path\n",
    "\n",
    "    # Set paths to Fourier-extracted data\n",
    "    path_Fourier_scored=data_path+'npy_newest_scored/Feats_Fourier_and_PSD/'\n",
    "    path_Fourier_unscored=data_path+'npy_newest_unscored/Feats_Fourier_and_PSD/'\n",
    "\n",
    "    # Set output paths for data, variables, and results\n",
    "    path_output=data_path+'T_CSV_Outputs/'\n",
    "    path_variables=data_path+'T_Variables/'\n",
    "    path_results=data_path+'T_Results_GPU/'\n",
    "\n",
    "    # Create folders if they do not exist\n",
    "    for path_iterable in [path_output,path_variables,path_results]:\n",
    "        if os.path.exists(path_iterable)==False:\n",
    "            os.mkdir(path_iterable)\n",
    "\n",
    "    # Return paths\n",
    "    return data_path, path_Fourier_scored, path_Fourier_unscored, path_output, path_variables, path_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1yPsGaUgF9M1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Final_Export\n",
      "D:\\Final_Export\n"
     ]
    }
   ],
   "source": [
    "# Set paths based on base path where data is stored\n",
    "data_path, path_Fourier_scored, path_Fourier_unscored, path_output, path_variables, path_results = setPaths(data_path='D:/', epoch_length=20)\n",
    "%cd $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "1yPsGaUgF9M1"
   },
   "outputs": [],
   "source": [
    "# Number of hours of recording per file\n",
    "train_recording_hours=12\n",
    "\n",
    "# Length of epochs in seconds\n",
    "train_recording_epoch_seconds=20\n",
    "\n",
    "# Total number of epochs per recording\n",
    "train_recording_epoch_count=int( (train_recording_hours*3600)/train_recording_epoch_seconds )\n",
    "\n",
    "# List of all viable integer codes for scores. Any other integers in \"score\" array will be rejected\n",
    "viable_scores=[1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "1yPsGaUgF9M1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Final_Export/CSV_Outputs/csv_full/\n"
     ]
    }
   ],
   "source": [
    "# Path to existing trained model file\n",
    "modelpath=f'{path_results}Final Model/Models/BiLSTM_size_200___BiLSTM_200_win3.h5'\n",
    "\n",
    "# Establish CSV folder structure\n",
    "csv_folder=f'{data_path}CSV_Outputs/'\n",
    "\n",
    "path_csv=f'{csv_folder}csv_classifier_prev_un_scored/'\n",
    "path_holes=f'{csv_folder}csv_trainset_classifier_scored/'\n",
    "mouse_sort_csvs=f'{csv_folder}csv_full/'\n",
    "path_expert=f'{csv_folder}csv_trainset_expert_scored/'\n",
    "path_figs=f'{csv_folder}figures/'\n",
    "path_conf=f'{csv_folder}score_conf/'\n",
    "path_excel=f'{csv_folder}excel/'\n",
    "\n",
    "# Create CSV and output folders\n",
    "for folder in [path_csv, path_holes, mouse_sort_csvs, path_expert, path_figs, path_excel, path_conf]:\n",
    "  if os.path.exists(folder)==0:\n",
    "    os.mkdir(folder)\n",
    "\n",
    "print(mouse_sort_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcNfdlAB3A-c"
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcNfdlAB3A-c"
   },
   "source": [
    "## Array Manipulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rcNfdlAB3A-c"
   },
   "outputs": [],
   "source": [
    "def consecutive(data, stepsize=1):\n",
    "    # Simple function for finding blocks of consecutive numbers in an array\n",
    "    # Find indicies where numbers differ\n",
    "    inds =  np.where(np.diff(data) != stepsize)[0]+1\n",
    "    # Split data on these indicies\n",
    "    consec = np.split(data, inds)\n",
    "    return consec, inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "KGFWQf-z3L5F"
   },
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "  \n",
    "    # Intilize a null list \n",
    "    global unique_list  \n",
    "    unique_list = [] \n",
    "    \n",
    "    # Traverse for all elements \n",
    "    for x in list1: \n",
    "        # Check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "            \n",
    "    return sorted(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KGFWQf-z3L5F"
   },
   "outputs": [],
   "source": [
    "def list_prune_via_substrings(list_to_be_scanned, substring_list):\n",
    "  result_list=[]\n",
    "\n",
    "  if type(list_to_be_scanned[0])!=str:\n",
    "    list_to_be_scanned=str(list_to_be_scanned)\n",
    "\n",
    "  if type(substring_list[0])!=str:\n",
    "    substring_list=[str(sub) for sub in substring_list]\n",
    "\n",
    "  for scan_member in list_to_be_scanned:\n",
    "    present=1\n",
    "    for member in substring_list:\n",
    "      if member in scan_member:\n",
    "        present=0\n",
    "    \n",
    "    if present==1:\n",
    "      result_list.append(scan_member)\n",
    "\n",
    "  return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KGFWQf-z3L5F"
   },
   "outputs": [],
   "source": [
    "def get_start_end(data_path):\n",
    "    # Load .xlsx containing start and end dates of recording for each animal\n",
    "    Excel=f'{data_path}/Start and End Dates.xlsx'\n",
    "    df = pd.read_excel(Excel)\n",
    "    start_dates=[]\n",
    "    end_dates=[]\n",
    "    # Transfer start and end dates to array for parsing\n",
    "    for i in range(len(df)):\n",
    "        start_dates.append(df['Name'][i])\n",
    "        start_dates.append(df['Start'][i])\n",
    "        end_dates.append(df['Name'][i])\n",
    "        end_dates.append(df['End'][i])\n",
    "    return start_dates,end_dates\n",
    "\n",
    "def get_manual_dates(data_path):\n",
    "    # Load .xlsx containing dates where animal data was manually scored\n",
    "    Excel=f'{data_path}/Manual Dates.xlsx'\n",
    "    df = pd.read_excel(Excel)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ydr2zEvA3P5E"
   },
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ydr2zEvA3P5E"
   },
   "outputs": [],
   "source": [
    "def load_Fourier_xy(path, file_list, train_recording_epoch_count, viable_scores, standardize=True):\n",
    "\n",
    "    total_files = len(file_list)  # Length of file list\n",
    "    print(total_files)\n",
    "    \n",
    "    epochs = train_recording_epoch_count   \n",
    "    total_epoch_count = int(epochs*(total_files)) # Total length of data array in 20 second epochs\n",
    "    \n",
    "    # Create displays at top of notebook output for reverse counter and current status\n",
    "    dh1 = display(f'Items left: {total_files}',display_id=True)\n",
    "    dh2 = display('Loading...',display_id=True)\n",
    "\n",
    "    # Counting variable for progress\n",
    "    file_counter = 0\n",
    "\n",
    "    # Initialize arrays for x and y, total epochs in datset by number of features in vector \n",
    "    x = np.zeros((total_epoch_count, 100))\n",
    "    y = np.zeros((total_epoch_count, 1))\n",
    "    exclusion = False\n",
    "    \n",
    "    # Iterate over all Fourier transformed files\n",
    "    for item in (sorted(file_list)):\n",
    "        # Increment count down to completion of file loading\n",
    "        total_files -= 1\n",
    "        # Update display for files remaining\n",
    "        dh1.update(f'Items left: {total_files}')\n",
    "\n",
    "        # Load in epoch scores to check for invalid scores\n",
    "        item = item.replace(\"x_ffnorm\", \"y_ffnorm\") \n",
    "        current_file_y = np.load(f\"{path}{item}\")\n",
    "        item = item.replace(\"y_ffnorm\", \"x_ffnorm\") \n",
    "\n",
    "        # If invalid score is present, skip file\n",
    "        for i in current_file_y:\n",
    "            \n",
    "            if i not in viable_scores:\n",
    "                exclusion=True\n",
    "\n",
    "        if exclusion==False:\n",
    "            # Load Fourier feature vectors\n",
    "            current_file_x = np.load(f\"{path}{item}\")\n",
    "\n",
    "            # Load feature vectors for each file\n",
    "            # If standardization is on, standardize values to 0-1 range for quicker training and evaluation\n",
    "            if standardize==True:\n",
    "                x[file_counter*epochs:((file_counter+1)*epochs)] = StandardScaler().fit_transform(current_file_x)\n",
    "            else:\n",
    "                x[file_counter*epochs:((file_counter+1)*epochs)] = current_file_x\n",
    "\n",
    "            \n",
    "            # load Sleep/Seizure score vectors\n",
    "            y[file_counter*epochs:((file_counter+1)*epochs)] = current_file_y-1\n",
    "\n",
    "            file_counter += 1\n",
    "            # Update display for current status\n",
    "            dh2.update(f'loaded x and y for {item}')\n",
    "            # Periodically print progress \n",
    "            if file_counter%50 == 0:\n",
    "                print(file_counter)\n",
    "\n",
    "        else: \n",
    "            exclusion = False\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Ydr2zEvA3P5E"
   },
   "outputs": [],
   "source": [
    "def generate_sequences(input_array, windows, x_or_y, max_feats=None, make_seq=True):\n",
    "    if make_seq==True:\n",
    "        if x_or_y not in ['X', 'Y']:\n",
    "            print(\"Please designate whether input is X or Y array using x_or_y parameter\")\n",
    "            return \n",
    "\n",
    "        for window in [windows]: # for loop to test varying window lengths\n",
    "            classes=5\n",
    "\n",
    "            shift=window*2 # All x variable rows will be sampled with sliding sequences \n",
    "            # If analyzing epoch 3, epochs from -window_length (0) to \n",
    "            # +window_length (6) around each epoch of interest will be sampled\n",
    "            # Therefore, the windowing cannot begin earlier than epoch # [window_length]\n",
    "\n",
    "\n",
    "            if x_or_y in ['X']:\n",
    "                if max_feats == None:\n",
    "                    # Get maximum numnber of features from X array if not specified\n",
    "                    max_feats=input_array.shape[1]\n",
    "\n",
    "                # Generate sliding window array \n",
    "                output_array=np.zeros((len(input_array)-shift,window*2+1,max_feats))\n",
    "                \n",
    "                # Cannot window on both sides unless first array taken has full window length prior to and after it, so for loop starts at window index and ends at end-window\n",
    "                for i in range(window,len(input_array)-window):\n",
    "                    output_array[i-window]=input_array[i-window:i+window+1,0:max_feats]\n",
    "        \n",
    "        \n",
    "            if x_or_y in ['Y']:\n",
    "                output_array=np.zeros((len(input_array)-shift, classes))\n",
    "                for i in range(window,len(input_array)-window):\n",
    "                    output_array[i-window]=input_array[i]\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        return output_array\n",
    "\n",
    "    elif make_seq==False:\n",
    "        print('no sequence generated')\n",
    "        return input_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ydr2zEvA3P5E"
   },
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "j76F6-xP30q1"
   },
   "outputs": [],
   "source": [
    "def common_training_parameters():\n",
    "    # Keras metrics for training evaluation\n",
    "    metrics = [\n",
    "        'accuracy',\n",
    "        keras.metrics.TruePositives(name='tp'),\n",
    "        keras.metrics.FalsePositives(name='fp'),\n",
    "        keras.metrics.TrueNegatives(name='tn'),\n",
    "        keras.metrics.FalseNegatives(name='fn'),\n",
    "        keras.metrics.CategoricalAccuracy(name='categorical_accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc', curve=\"PR\"),\n",
    "        keras.metrics.CategoricalCrossentropy(name='categorical_crossentropy')]\n",
    "\n",
    "    # Batch size of training/evaluation = one 12 hour recording file worth of 20 second epochs\n",
    "    batch_size = 2160\n",
    "\n",
    "    # Set training optimizer\n",
    "    optimizer=Nadam(learning_rate=0.00001)\n",
    "    # Set regularization function for training\n",
    "    activity_regularizer=l1(0.0001)\n",
    "\n",
    "    return batch_size, metrics, optimizer, activity_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "j76F6-xP30q1"
   },
   "outputs": [],
   "source": [
    "def norm_sklearn_classweight(y_train, mu=False):\n",
    "    \n",
    "    # Get maximum range of class labels \n",
    "    classes = unique(np.argmax(y_train,1))\n",
    "\n",
    "    # Convert scores back from one-hot to ints\n",
    "    y_train_classes = [np.argmax(z) for z in y_train]\n",
    "    unique(y_train_classes)\n",
    "\n",
    "    \n",
    "    # Assign balanced class weights\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_classes)\n",
    "    print(weights)\n",
    "    class_weight = {}\n",
    "    ind = 0\n",
    "\n",
    "    \n",
    "    # Adjust weights based upon mu parameter (logarithmic normalization of class weights) or the minimum weight present\n",
    "    mu=weights[0]/math.exp(1)\n",
    "    for i in classes:\n",
    "        j = int(np.where(classes==i)[0])\n",
    "        score = math.log(weights[j]/mu)\n",
    "        class_weight[j] = score \n",
    "\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "eU1iiHC95mBs"
   },
   "outputs": [],
   "source": [
    "def create_train_Dense_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n",
    "                       steps_per_epoch=None, layer_size=8, batch_size=2160, optimizer=Nadam(learning_rate=0.00001), metrics=['accuracy'], activity_regularizer=l1(0.0001), prefix=\"\", layers=3, data_path=''):\n",
    "        \n",
    "        # Get dimensions for model input layer from input array\n",
    "        data_width=X_train.shape[1]\n",
    "   \n",
    "        if len(X_train.shape)>2:\n",
    "            data_depth=X_train.shape[2]\n",
    "            shape=(data_width,data_depth)\n",
    "        else: \n",
    "            data_width=X_train.shape[1]\n",
    "            shape=(data_width,)\n",
    "\n",
    "        # Create Sequential Model \n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=shape))\n",
    "        model.add(Dense(layer_size))\n",
    "        model.add(Flatten())           \n",
    "        model.add(Dense(5))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "        model.build()\n",
    "        model.summary()\n",
    "\n",
    "        # Name model with logging based on training time and date and metrics to track\n",
    "        model_name=f\"Dense_size_{layer_size}_{prefix}_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        filepath = f\"{model_name}\"+\"_weights-auc-{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n",
    "\n",
    "        # Keras model checkpoint function\n",
    "        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n",
    "                              monitor='val_loss', verbose=1, \n",
    "                              save_best_only=True, save_weights_only=False, mode='min')\n",
    "\n",
    "        # Early stopping parameters\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                    patience=5, min_delta=0.001,restore_best_weights=True)\n",
    "\n",
    "        # Tensorboard logging variables\n",
    "        log_dir = f\"./Logs/{model_name}\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        logpath=f\"./Logs/\"\n",
    "        csv_path=f\"{model_name}_model.csv\"\n",
    "        csv_logger=CSVLogger(logpath+csv_path)\n",
    "\n",
    "        # Fitting function\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n",
    "\n",
    "        return model, history, logpath, csv_path, model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jbEevLAk0OtB"
   },
   "outputs": [],
   "source": [
    "def create_train_LSTM_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n",
    "                       steps_per_epoch=None, layer_size=200, batch_size=2160, optimizer=Nadam(learning_rate=0.00001), metrics=['accuracy'], activity_regularizer=l1(0.0001), prefix=\"\", layers=1, data_path=''):\n",
    "\n",
    "        # Get dimensions for model input layer from input array\n",
    "        data_width = X_train.shape[1]\n",
    "        data_depth = X_train.shape[2]\n",
    "        shape = (data_width,data_depth)\n",
    "\n",
    "            \n",
    "        # Create Sequential Model \n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=shape))\n",
    "        model.add(LSTM(int(layer_size), activity_regularizer=activity_regularizer, return_sequences=True))\n",
    "        model.add(Dropout(.4))\n",
    "        model.add(Flatten())     \n",
    "        model.add(Dense(5))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "        model.build()\n",
    "        model.summary()\n",
    "        \n",
    "        # Name model with logging based on training time and date and metrics to track\n",
    "        model_name=f\"Single_LSTM_size_{LSTM_size}_{prefix}_\"\n",
    "        filepath = f\"{model_name}\"+\"_weights.auc:{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n",
    "        \n",
    "        # Keras model checkpoint function\n",
    "        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n",
    "                              monitor='val_loss', verbose=1, \n",
    "                              save_best_only=True, save_weights_only=False, mode='min')       \n",
    "    \n",
    "        # Early stopping parameters              \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                    patience=5, min_delta=0.001,restore_best_weights=True)\n",
    "    \n",
    "        # Tensorboard logging variables    \n",
    "        log_dir = f\"./Logs/{model_name}\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        logpath=f\"./Logs/\"   \n",
    "        csv_path=f\"{model_name}_model.csv\"\n",
    "        csv_logger=CSVLogger(logpath+csv_path)\n",
    "        \n",
    "        # Model fitting call\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n",
    "\n",
    "        return model, history, logpath, csv_path, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "OK2i_YQn4RRP"
   },
   "outputs": [],
   "source": [
    "def create_train_BiLSTM_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n",
    "                       steps_per_epoch=None, layer_size=200, batch_size=2160, optimizer=Nadam(learning_rate=0.00001), metrics=['accuracy'], activity_regularizer=l1(0.0001), prefix=\"\", layers=1, data_path=''):\n",
    "        \n",
    "        # Get dimensions for model input layer from input array\n",
    "        data_width = X_train.shape[1]\n",
    "        data_depth = X_train.shape[2]\n",
    "        shape = (data_width,data_depth)\n",
    "\n",
    "        # Create Sequential Model \n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=shape))\n",
    "        forward_layer = LSTM(int(layer_size/factor), activity_regularizer=activity_regularizer, return_sequences=True)\n",
    "        backward_layer = LSTM(int(layer_size/factor), activity_regularizer=activity_regularizer, return_sequences=True,\n",
    "                              go_backwards=True)\n",
    "        model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "        model.add(Dropout(.4))\n",
    "        model.add(Flatten())     \n",
    "        model.add(Dense(5))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "        \n",
    "        model.build()\n",
    "        model.summary()\n",
    "        \n",
    "        # Name model with logging based on training time and date and metrics to track\n",
    "        model_name=f\"BiLSTM_size_{LSTM_size}_{prefix}_\"\n",
    "        filepath = f\"{model_name}\"+\"_weights-auc-{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n",
    "\n",
    "        # Keras model checkpoint function\n",
    "        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n",
    "                              monitor='val_loss', verbose=1, \n",
    "                              save_best_only=True, save_weights_only=False, mode='min')     \n",
    "    \n",
    "        # Early stopping parameters                                            \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                    patience=5, min_delta=0.001,restore_best_weights=True)\n",
    "\n",
    "        # Tensorboard logging variables        \n",
    "        log_dir = f\"./Logs/{model_name}\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        logpath=f\"./Logs/\"\n",
    "        csv_path=f\"{model_name}_model.csv\"\n",
    "        csv_logger=CSVLogger(logpath+csv_path)\n",
    "        \n",
    "        # Model fitting call\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n",
    "\n",
    "        return model, history, logpath, csv_path, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "CKj7p0GV5ewF"
   },
   "outputs": [],
   "source": [
    "def create_train_flat_BiLSTM_model(X_train, X_test, y_train, y_test, class_weight, epochs, \n",
    "                       steps_per_epoch=None, layer_size=200, batch_size=2160, optimizer=Nadam(learning_rate=0.00001), metrics=['accuracy'], activity_regularizer=l1(0.0001), prefix=\"\", layers=3, data_path=''):\n",
    "\n",
    "        # Get dimensions for model input layer from input array\n",
    "        data_width = X_train.shape[1]\n",
    "        data_depth = X_train.shape[2]\n",
    "        shape = (data_width,data_depth)\n",
    "            \n",
    "        # Create Sequential Model \n",
    "        # Model creation with optional stacking of Bi-LSTM/Dropout pairs\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=shape))\n",
    "        model.add(Flatten())           \n",
    "        model.add(RepeatVector(1)) \n",
    "\n",
    "        # Each Bi-LSTM in cascade gets smaller by a factor of 2\n",
    "        if layers in range(1,5):\n",
    "            forward_layer = LSTM(int(LSTM_size), return_sequences=True)\n",
    "            backward_layer = LSTM(int(LSTM_size), return_sequences=True,\n",
    "                                  go_backwards=True)\n",
    "            model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "            model.add(RepeatVector(1)) \n",
    "            model.add(Dropout(.4))\n",
    "\n",
    "            factor = 2\n",
    "            if layers in range (2,5):\n",
    "              forward_layer = LSTM(int(LSTM_size/factor), return_sequences=True)\n",
    "              backward_layer = LSTM(int(LSTM_size/factor), return_sequences=True,\n",
    "                                    go_backwards=True)\n",
    "              model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "              model.add(Dropout(.4))\n",
    "              model.add(RepeatVector(1)) \n",
    "\n",
    "              factor = 4\n",
    "              if layers in range (3,5):\n",
    "                forward_layer = LSTM(int(LSTM_size/factor), return_sequences=True)\n",
    "                backward_layer = LSTM(int(LSTM_size/factor), return_sequences=True,\n",
    "                                      go_backwards=True)\n",
    "                model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "                model.add(Dropout(.4))\n",
    "                model.add(RepeatVector(1)) \n",
    "\n",
    "                factor = 8\n",
    "                if layers in range (4,5):\n",
    "                    forward_layer = LSTM(int(LSTM_size/factor), return_sequences=True)\n",
    "                    backward_layer = LSTM(int(LSTM_size/factor), return_sequences=True,\n",
    "                                          go_backwards=True)\n",
    "                    model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "                    model.add(Dropout(.4))\n",
    "                    model.add(RepeatVector(1)) \n",
    "                \n",
    "        model.add(Flatten())     \n",
    "        model.add(Dense(5))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=METRICS)\n",
    "        \n",
    "        model.build()\n",
    "        model.summary()\n",
    "                \n",
    "        # Name model with logging based on training time and date and metrics to track\n",
    "        model_name=f\"flat_LSTM_size_{LSTM_size}_{prefix}_\"\n",
    "        filepath = f\"{model_name}\"+\"_weights-auc-{auc}-{categorical_accuracy:.6f}--val-{val_auc}-{val_categorical_accuracy:.6f}.h5\"\n",
    "\n",
    "        # Keras model checkpoint function\n",
    "        mcp = ModelCheckpoint(f\"./Checkpoints/{filepath}\", \n",
    "                              monitor='val_loss', verbose=1, \n",
    "                              save_best_only=True, save_weights_only=False, mode='min')      \n",
    "    \n",
    "        # Early stopping parameters                                            \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                    patience=5, min_delta=0.001,restore_best_weights=True)\n",
    "\n",
    "        # Tensorboard logging variables        \n",
    "        log_dir = f\"./Logs/{model_name}\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        logpath=f\"./Logs/\"\n",
    "        csv_path=f\"{model_name}_flat_model.csv\"\n",
    "        csv_logger=CSVLogger(logpath+csv_path)\n",
    "        \n",
    "        # Model fitting call\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, class_weight=class_weight, epochs=epochs, \n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=(X_test, y_test), verbose=2, callbacks=[es, mcp, csv_logger, tensorboard_callback])\n",
    "\n",
    "\n",
    "        return model, history, logpath, csv_path, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwelEPME5qSV"
   },
   "source": [
    "## Training Bookkeeping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "BwelEPME5qSV"
   },
   "outputs": [],
   "source": [
    "def save_model_report_seq(model, model_history, ctrls=0, run=2, x=[], y=[], X_train_seq=[], X_val_seq=[], y_train_seq=[], y_val_seq=[], X_test_seq=[], y_test_seq=[], model_name='test',i=1,model_num=8,data_path='',logpath='', csvpath='',savepath='./Results/'):  \n",
    "    \n",
    "    # Print model name to output\n",
    "    print(model_name)\n",
    "    \n",
    "    # Assess Confusion Matrices with limited states for full-saline cohorts \n",
    "    if ctrls==1:\n",
    "        num_classes=3\n",
    "    else:\n",
    "        num_classes=5\n",
    "\n",
    "    # Name files based on dataset \n",
    "    if ctrls==0:\n",
    "        charpath=f'Kainic Acid'\n",
    "    elif ctrls==1:\n",
    "        charpath=f'Control'\n",
    "    elif ctrls==2:\n",
    "        charpath='AllData'\n",
    "\n",
    "\n",
    "    # Make output directories for models, confusion matrices, and softmax confidence outputs\n",
    "    if os.path.exists(savepath)==False:\n",
    "        os.mkdir(savepath)\n",
    "    if os.path.exists(f'{savepath}Models/')==False:\n",
    "        os.mkdir(f'{savepath}Models/')\n",
    "    if os.path.exists(f'{savepath}Softmax/')==False:\n",
    "        os.mkdir(f'{savepath}Softmax/')\n",
    "\n",
    "    # Codes for generating and naming confusion matrices\n",
    "    codes=np.array(['Wake','NREM','REM','Seizure','Post-Ictal'])\n",
    "\n",
    "    # Save hdf5 model within output folder\n",
    "    if os.path.exists(f'{savepath}Models/{model_name}.h5')==False:\n",
    "       model.save(f'{savepath}Models/{model_name}.h5')  \n",
    "\n",
    "    # Save Model History dictionary \n",
    "    if model_history!=None:\n",
    "        with open(f'{savepath}{model_name}{model_num}historydict','wb') as file_pi:\n",
    "            pickle.dump(model_history.history, file_pi)\n",
    "          \n",
    "        shutil.copyfile(logpath+csvpath, f'{savepath}{csvpath}')\n",
    "        \n",
    "    # Reconstitute entire dataset for evaluation\n",
    "    x_new=np.concatenate((X_test_seq,X_val_seq, X_train_seq))\n",
    "    y_new_1hot=np.concatenate((y_test_seq,y_val_seq, y_train_seq))\n",
    "      \n",
    "    # With model report text file open, generate report text\n",
    "    with open(f'{savepath}Report_{model_name}.txt','w') as f:\n",
    "        # Iterate over train/test/val and combined datasets \n",
    "        # Create Matrices/Reports for all datasets in train/test/val, as well as overall statistics\n",
    "        for selection in range(0,4):\n",
    "            gc.collect()\n",
    "            names=['Saline-KA Holdout Testing Dataset', 'Saline Control Validation Dataset', 'Training Dataset', 'Training, Validation, and Testing Overall']\n",
    "            X_check, y_check = [[X_test_seq, y_test_seq], [X_val_seq, y_val_seq],[X_train_seq, y_train_seq], [x_new, y_new_1hot]][selection]\n",
    "\n",
    "            # Predict scores \n",
    "            y_pred=model.predict(X_check, batch_size=2160)\n",
    "            # Save softmax results\n",
    "            y_pred.tofile(f'{savepath}Softmax/results_{model_name}.csv', sep=',')\n",
    "            \n",
    "            # Get codes from softmax results\n",
    "            y_pred_codes=np.argmax(y_pred,1)\n",
    "            y_test_codes=np.argmax(y_check,1)\n",
    "            \n",
    "            # Print classification reports to Report txt\n",
    "            print(f'\\n {names[selection]} \\n', file=f)\n",
    "            print(classification_report(y_test_codes, y_pred_codes), file=f)\n",
    "\n",
    "            # Save classification report as dataframe to excel \n",
    "            dict1 = classification_report(y_test_codes, y_pred_codes, output_dict=True)\n",
    "            df = pd.DataFrame(data=dict1)\n",
    "            df = (df.T)\n",
    "            print (df)\n",
    "            dataset_name=names[selection]\n",
    "            df.to_excel(f'{savepath}{dataset_name}{model_name}.xlsx')\n",
    "\n",
    "            # Generate normalized confusion matrix \n",
    "            cm=confusion_matrix(y_test_codes,y_pred_codes,normalize='true')\n",
    "\n",
    "            # Sanity check that all states in dataset are accounted for in results\n",
    "            print(f'unique ground truth labels: {unique(y_test_codes)}')\n",
    "            print(f'unique predicted labels: {unique(y_pred_codes)}')\n",
    "    \n",
    "            # Display confusion matrix with whichever dataset has more labels in it - otherwise it will not plot \n",
    "            if len(unique(y_pred_codes))>len(unique(y_test_codes)):\n",
    "              disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=codes[unique(y_pred_codes)])\n",
    "            else:\n",
    "              disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=codes[unique(y_test_codes)])\n",
    "    \n",
    "            disp.plot()\n",
    "\n",
    "            # Plot and save confusion matrix as .png and .svg for publication\n",
    "            print(f'{names[selection]}')\n",
    "            plt.title(f'Confusion Matrix for {names[selection]}')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.savefig(f'{savepath}{model_name}_{names[selection]}_CM_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.png', dpi=700)\n",
    "            plt.savefig(f'{savepath}{model_name}_{names[selection]}_CM_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.svg', dpi=700)\n",
    "    \n",
    "    \n",
    "            plt.close()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "QBWKQ3Xh5sv8"
   },
   "outputs": [],
   "source": [
    "def save_model_report_svg(model, model_history, ctrls=0, run=2, x=[], y=[], X_train_seq=[], X_val_seq=[], y_train_seq=[], y_val_seq=[], X_test_seq=[], y_test_seq=[], model_name='test',i=1,model_num=8,data_path='', savepath='./Results/',textsize = 15):  \n",
    "\n",
    "    # Same function as save_model_report_seq, but without saving of history dicts and models, just for confusion matrix output\n",
    "    \n",
    "    # Print model name to output\n",
    "    print(model_name)\n",
    "\n",
    "    # Assess Confusion Matrices with limited states for full-saline cohorts \n",
    "    if ctrls==1:\n",
    "        num_classes=3\n",
    "    else:\n",
    "        num_classes=5\n",
    "\n",
    "    # Name files based on dataset \n",
    "    if ctrls==0:\n",
    "        charpath=f'Kainic Acid'\n",
    "    elif ctrls==1:\n",
    "        charpath=f'Control'\n",
    "    elif ctrls==2:\n",
    "        charpath='AllData'\n",
    "    \n",
    "    # Make output directories for confusion matrices and softmax confidence outputs\n",
    "    if os.path.exists(savepath)==False:\n",
    "        os.mkdir(savepath)\n",
    "\n",
    "    # Codes for generating and naming confusion matrices\n",
    "    codes=np.array(['Wake','NREM','REM','Seizure','Post-Ictal'])\n",
    "\n",
    "\n",
    "    # Reconstitute entire dataset for evaluation\n",
    "    x_new=np.concatenate((X_test_seq,X_val_seq, X_train_seq))\n",
    "    y_new_1hot=np.concatenate((y_test_seq,y_val_seq, y_train_seq))\n",
    "  \n",
    "    # With model report text file open, generate report text\n",
    "    with open(f'{savepath}Report_{model_name}.txt','w') as f:\n",
    "        # Iterate over train/test/val and combined datasets \n",
    "        # Create Matrices/Reports for all datasets in train/test/val, as well as overall statistics\n",
    "        for selection in range(0,4):\n",
    "            gc.collect()\n",
    "            names=['Saline-KA Holdout Testing Dataset', 'Saline Control Validation Dataset', 'Training Dataset', 'Training, Validation, and Testing Overall']\n",
    "            X_check, y_check = [[X_test_seq, y_test_seq], [X_val_seq, y_val_seq],[X_train_seq, y_train_seq], [x_new, y_new_1hot]][selection]\n",
    "            \n",
    "            # Predict scores \n",
    "            y_pred=model.predict(X_check, batch_size=2160)\n",
    "            # Save softmax results\n",
    "            y_pred.tofile(f'{savepath}Softmax/results_{model_name}.csv', sep=',')\n",
    "    \n",
    "            # Get codes from softmax results\n",
    "            y_pred_codes=np.argmax(y_pred,1)\n",
    "            y_test_codes=np.argmax(y_check,1)\n",
    "\n",
    "            # Print classification reports to Report txt\n",
    "            print(f'\\n {names[selection]} \\n', file=f)\n",
    "            print(classification_report(y_test_codes, y_pred_codes), file=f)\n",
    "            \n",
    "            # Save classification report as dataframe to excel \n",
    "            dict1 = classification_report(y_test_codes, y_pred_codes, output_dict=True)\n",
    "            df = pd.DataFrame(data=dict1)\n",
    "            df = (df.T)\n",
    "            print (df)\n",
    "            dataset_name=names[selection]\n",
    "            df.to_excel(f'{savepath}{dataset_name}{model_name}.xlsx')\n",
    "            \n",
    "            # Generate normalized confusion matrix \n",
    "            cm=confusion_matrix(y_test_codes,y_pred_codes,normalize='true')\n",
    "\n",
    "            # Sanity check that all states in dataset are accounted for in results\n",
    "            print(f'unique ground truth labels: {unique(y_test_codes)}')\n",
    "            print(f'unique predicted labels: {unique(y_pred_codes)}')\n",
    "    \n",
    "            # Display confusion matrix with whichever dataset has more labels in it - otherwise it will not plot \n",
    "            if len(unique(y_pred_codes))>len(unique(y_test_codes)):\n",
    "              disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=codes[unique(y_pred_codes)])\n",
    "            else:\n",
    "              disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=codes[unique(y_test_codes)])\n",
    "    \n",
    "            \n",
    "            # Plot and save confusion matrix as .png and .svg for publication\n",
    "            disp.plot()\n",
    "            print(f'{names[selection]}')\n",
    "            plt.title(f'Confusion Matrix for {names[selection]}')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.savefig(f'{savepath}{model_name}_{names[selection]}_CM_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.png', dpi=700)\n",
    "            plt.savefig(f'{savepath}{model_name}_{names[selection]}_CM_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.svg', dpi=700)\n",
    "    \n",
    "    \n",
    "            plt.close()\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "QBWKQ3Xh5sv8"
   },
   "outputs": [],
   "source": [
    "def print_conf_mat(y_test_codes, y_pred_codes, legend, output_name):\n",
    "\n",
    "      \n",
    "        labelmax=max(max(y_test_codes),max(y_pred_codes))\n",
    "        print(labelmax)\n",
    "        print(codes[0:labelmax+1])\n",
    "\n",
    "        disp=ConfusionMatrixDisplay.from_predictions(y_test_codes,y_pred_codes, labels=[0,1,2,3,4], normalize='true', display_labels=codes)\n",
    "\n",
    "        plt.title(f'{legend}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "\n",
    "        plt.savefig(f'{output_name}_ConfMat.png', dpi=700)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBWKQ3Xh5sv8"
   },
   "source": [
    "## Post-Training Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "QBWKQ3Xh5sv8"
   },
   "outputs": [],
   "source": [
    "def score_data(paths, errors, load_dropped_data=0, held_dates=[], scored=0):\n",
    "    \n",
    "    # Print model summary to output\n",
    "    model.summary()\n",
    "\n",
    "    # Get all paths\n",
    "    data_path,path_Fourier_scored,path_Fourier_unscored,path_output,path_conf=paths\n",
    "\n",
    "    # Evaluate based on designated category of existing scoring\n",
    "    # 1 saves classifier scores for previously-scored data\n",
    "    if scored==1:\n",
    "      strpart='trainset_classifier'\n",
    "      path_Fourier=path_Fourier_scored\n",
    "    # 2 saves manual expert scores for previously-scored data\n",
    "    elif scored==2:\n",
    "      strpart='trainset_expert'\n",
    "      path_Fourier=path_Fourier_scored\n",
    "    # 0 saves classifier scores for previously-unscored data\n",
    "    else:\n",
    "      strpart='classifier_prev_un'\n",
    "      path_Fourier=path_Fourier_unscored\n",
    "\n",
    "    # Establish counting variables for tracking data processing\n",
    "    %matplotlib inline\n",
    "    score=0\n",
    "    good=0\n",
    "\n",
    "    # Generate path for scoring subtype\n",
    "    path_csv=f'{path_output}'+f'csv_{strpart}_scored/'\n",
    "    for path_iterate in [path_conf,path_csv]:\n",
    "        if os.path.isdir(path_iterate) == False:\n",
    "          os.mkdir(path_iterate)\n",
    "    print(path_csv)\n",
    "\n",
    "    print(path_Fourier)\n",
    "    fourier_files = [f for f in listdir(path_Fourier) if isfile(join(path_Fourier, f))]\n",
    "    fourier_files = [f for f in fourier_files if \"x_ffnorm\" in f]   # find all x arrays in fourier_files\n",
    "\n",
    "    # If load_dropped_data is 1, only load dates within held_dates, if not then exclude held_dates\n",
    "    if load_dropped_data==1:\n",
    "        print('loading dropped data')\n",
    "        fourier_files_pruned=[]\n",
    "        for d in held_dates:\n",
    "            fourier_files=[f for f in fourier_files if d in f]\n",
    "        fourier_files_pruned=fourier_files\n",
    "    else:\n",
    "        for d in held_dates:\n",
    "            fourier_files = [f for f in fourier_files if d not in f]\n",
    "        fourier_files_pruned=fourier_files\n",
    "\n",
    "    test_length=len(fourier_files_pruned)  \n",
    "    print(test_length)\n",
    "\n",
    "    # Create display handles and variables for loop tracking\n",
    "    dh1 = display(f'Items left: {test_length}',display_id=True)\n",
    "    dh = display('Loading...',display_id=True)\n",
    "    y_done=0\n",
    "    x_done=0\n",
    "    global_counter = 0\n",
    "    \n",
    "    window=3\n",
    "    shift=window*2\n",
    "    max_feats=100\n",
    "    classes=5\n",
    "\n",
    "    for item in (sorted(fourier_files_pruned)):\n",
    "        # Reset file exclusion flag\n",
    "        exclude_file = 0\n",
    "\n",
    "        # Update displays with current remaining files\n",
    "        test_length-=1\n",
    "        dh1.update(f'Items left: {test_length}')\n",
    "\n",
    "        # If file is already scored, set exclusion flag to 1\n",
    "        item = item.replace(\"x_ffnorm\", \"y_ffnorm\")\n",
    "        if os.path.isfile(f'{path_csv}{item}_{strpart}_scored.csv')==True:\n",
    "          exclude_file=1\n",
    "        item = item.replace(\"y_ffnorm\", \"x_ffnorm\")\n",
    "\n",
    "        # Skip file if already scored\n",
    "        if exclude_file==0:\n",
    "            # Load \n",
    "            current_file_x, x_done, item = load_file(path_Fourier, item, scored)\n",
    "            \n",
    "            if scored > 0:\n",
    "                current_file_y = np.load(f\"{path_Fourier}{item}\")\n",
    "            y_done=1\n",
    "\n",
    "            X_score3=np.zeros((len(current_file_x)-shift,window*2+1,max_feats))\n",
    "\n",
    "            for i in range(window,len(current_file_x)-window):\n",
    "                X_score3[i-window]=current_file_x[i-window:i+window+1,0:max_feats]\n",
    "          \n",
    "            if x_done == 1 and y_done == 1:\n",
    "                global_counter+=1\n",
    "\n",
    "                num_classes=5\n",
    "\n",
    "                if scored in [0,1]:\n",
    "                    y_pred=model.predict(X_score3, 2160, verbose=0)\n",
    "\n",
    "                    y_pred_codes=np.argmax(y_pred,1)\n",
    "                    np.save(f'{path_conf}conf_{item}',y_pred)\n",
    "                    y_pred_codes.tofile(f'{path_csv}{item}_{strpart}_scored.csv', sep = ',')\n",
    "\n",
    "                else:\n",
    "                  y_pred_codes=current_file_y-1\n",
    "                  y_pred_codes.tofile(f'{path_csv}{item}_{strpart}_scored.csv', sep = ',')\n",
    "\n",
    "                dh.update(f'loaded x and y for {item}')\n",
    "\n",
    "                if global_counter%50==0:\n",
    "                    print(global_counter)\n",
    "                x_done=0\n",
    "                y_done=0\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f'{item} excluded')\n",
    "            excl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1JPqzdiLZjH"
   },
   "source": [
    "## Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Dp6lyBvbG4r5"
   },
   "outputs": [],
   "source": [
    "def R_and_K_evaluation(y_pred, violations_total):\n",
    "    y_pred_scores = y_pred\n",
    "    # Store existing scores \n",
    "    y_pred_scores_baseline = y_pred_scores\n",
    "\n",
    "    # Rechtshaffen and Kales Critera\n",
    "    violations=np.zeros((3))\n",
    "    rem_violations=0\n",
    "    nrem_violations=0\n",
    "    wake_violations=0\n",
    "    \n",
    "    \n",
    "    for idx in range(0,len(y_pred_scores)-1):\n",
    "        if idx>0:\n",
    "            # REM Must Follow NREM\n",
    "            # \"If REM is preceded by Wake, score as Wake\"\n",
    "            if y_pred_scores[idx]==2 and y_pred_scores[idx-1]==0:\n",
    "                rem_violations+=1\n",
    "\n",
    "                y_pred_scores[idx]=0\n",
    "\n",
    "            # In order to score NREM, there must be 2 consecutive epochs\n",
    "            # or lone NREM is scored as the previous epoch\n",
    "            if y_pred_scores[idx]==1:\n",
    "                if y_pred_scores[idx-1]!=1 and y_pred_scores[idx+1]!=1:\n",
    "                    nrem_violations+=1\n",
    "\n",
    "                    y_pred_scores[idx]=y_pred_scores[idx-1]\n",
    "\n",
    "\n",
    "    violations[0]=wake_violations\n",
    "    violations[1]=rem_violations\n",
    "    violations[2]=nrem_violations\n",
    "    violations_total[0]=violations[0]+violations_total[0]\n",
    "    violations_total[1]=violations[1]+violations_total[1]\n",
    "    violations_total[2]=violations[2]+violations_total[2]\n",
    "\n",
    "    agreement = y_pred_scores == y_pred_scores_baseline\n",
    "    agree_pct=(len(y_pred_scores)-np.sum(violations))/len(y_pred_scores)\n",
    "\n",
    "    return agree_pct, agreement, violations_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "IYF2ZGhKkzXM"
   },
   "outputs": [],
   "source": [
    "def load_file(path_Fourier, item, scored):\n",
    "    # Handles 4 and 20 second files agnostically\n",
    "\n",
    "    # Load and standard-scale FFT arrays\n",
    "    current_file_x = StandardScaler().fit_transform(np.load(f\"{path_Fourier}{item}\"))\n",
    "\n",
    "    # Determine expansion factor of RMS EMG\n",
    "    length_factor = int(len(current_file_x)/2160)\n",
    "\n",
    "    # If RMS EMG not binned in 20 1-second bins\n",
    "    if  length_factor != 1:\n",
    "        \n",
    "        last_rms_index=20-int(20/length_factor)\n",
    "        # Repeat the RMS EMG 20/epoch_length times\n",
    "        for index in range(len(current_file_x)):\n",
    "            current_rms=current_file_x[index,-20:-last_rms_index]\n",
    "            current_file_x[index,-20:]=np.repeat(current_rms,length_factor)\n",
    "\n",
    "    x_done=1\n",
    "\n",
    "    return current_file_x, x_done, item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cSSIEeFS89D",
    "tags": []
   },
   "source": [
    "# Load Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "eZtFvvb7Q3PZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' NPM592 ', ' NPM596 ', ' NPM604 ', 'NPM644', ' NPM645 ', ' NPM646 ', ' NPM647 ', 'NPM656-659', ' NPM661 ', ' NPM662 ', ' NPM663 ', ' NPM664 ', ' NPM665 ', ' NPM666 ', ' NPM667 ', ' NPM668 ', '200316', '566', '569', ' NPM569 ', 'NPM570 ', ' NPM572 ', ' NPM574 ', ' NPM579 ', 'NPM573-576', ' NPM580 ', ' NPM614 ', 'NPM648', 'NPM652', 'NPM656']\n",
      "707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Items left: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'loaded x and y for x_ffnorm NPM642 210223 190404_058 NPM639-642.npy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Items left: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'loaded x and y for x_ffnorm NPM604 200707 190222_013 NPM604-607.npy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Items left: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'loaded x and y for x_ffnorm NPM668 210713 190351_058 NPM665-668.npy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# Datasets selected for Train/Test Split via this block\n",
    "\n",
    "old_cohort = ['566']  # not trained on animals with radically different signal\n",
    "held_cohorts = ['200316','566', '569', ' NPM569 ', \n",
    "                'NPM570 ', ' NPM572 ', ' NPM574 ', ' NPM579 ',\n",
    "                'NPM573-576', ' NPM580 ',' NPM614 '] # Wild type or control mice with lack of baseline or classifier-breaking scoring issues \n",
    "\n",
    "wild_types=['NPM648','NPM652', 'NPM656'] # WT KA mouse cohort identifiers\n",
    "wild_types_saline=['NPM644'] # WT saline mouse identifiers - no quotes ensures entire 644 cohort is loaded\n",
    "saline = [' NPM592 ', ' NPM596 ',' NPM604 '] # Saline \n",
    "\n",
    "\n",
    "recent = [' NPM645 ',' NPM646 ',' NPM647 ', 'NPM656-659',\n",
    "          ' NPM661 ',' NPM662 ', ' NPM663 ', ' NPM664 ',' NPM665 ',' NPM666 ', \n",
    "          ' NPM667 ', ' NPM668 '] # Validation set of most recent animals\n",
    "          # Mixed batch of KA and Saline, VGAT Cre and WT\n",
    "\n",
    "extra_train = [' NPM609 ',' NPM644 ']\n",
    "              # ' NPM664 '] \n",
    "# random saline animals (609,644) and recent-KA animals (664) to add to training dataset\n",
    "\n",
    "held_dates = ['200103','200104','200105','200106',\n",
    "       '200107','200108','200109',\n",
    "       '200301','200302','200303',\n",
    "       '200614','200615','200616',\n",
    "       '200719','200720','200721',\n",
    "       '200809','200810','200811',\n",
    "       '201101','200102','201103',\n",
    "       '201213','201214','201215',\n",
    "       '210131','200201','210202',\n",
    "       '210321','200322','210323',\n",
    "       '200620','200621','200622']\n",
    "       # all dates from baseline recordings of KA mice were ignored for training, \n",
    "       # due to issues sorting by condition vs. mouse vs. date\n",
    "\n",
    "\n",
    "\n",
    "path=path_Fourier_scored\n",
    "fourier_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "fourier_files = [f for f in fourier_files if \"x_ffnorm\" in f]   # find all x arrays in fourier_files\n",
    "\n",
    "non_train=saline+wild_types_saline+recent+held_cohorts+wild_types\n",
    "\n",
    "print(non_train)\n",
    "files_held=[]\n",
    "files_held=[f for d in non_train for f in fourier_files if d in f] # gather list of all non-training files\n",
    "\n",
    "train_files=[]\n",
    "train_files=[f for f in fourier_files if f not in files_held] # gather list of all training files\n",
    "np.savetxt(data_path+'training_files.txt', train_files,delimiter=\" \", fmt=\"%s\") \n",
    "\n",
    "x1,y1=load_Fourier_xy(path, train_files, standardize=True, train_recording_epoch_count=train_recording_epoch_count, viable_scores=viable_scores)\n",
    "\n",
    "\n",
    "val_files=[]\n",
    "val_files=[f for d in saline for f in fourier_files if d in f] # gather list of all saline VGAT Cre files for test dataset\n",
    "np.savetxt(data_path+'validation_files.txt', val_files,delimiter=\" \", fmt=\"%s\") \n",
    "\n",
    "x2,y2=load_Fourier_xy(path, val_files, standardize=True, train_recording_epoch_count=train_recording_epoch_count, viable_scores=viable_scores)\n",
    "\n",
    "test_files=[]\n",
    "test_files=[f for d in recent+wild_types for f in fourier_files if d in f] # gather list of all validation files marked by 'recent' variable\n",
    "np.savetxt(data_path+'testing_files.txt', test_files,delimiter=\" \", fmt=\"%s\") \n",
    "\n",
    "x3,y3=load_Fourier_xy(path, test_files, standardize=True, train_recording_epoch_count=train_recording_epoch_count, viable_scores=viable_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VhqEbEnEc795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "[' NPM592 ', ' NPM596 ', ' NPM604 ', 'NPM644', ' NPM645 ', ' NPM646 ', ' NPM647 ', 'NPM656-659', ' NPM661 ', ' NPM662 ', ' NPM663 ', ' NPM664 ', ' NPM665 ', ' NPM666 ', ' NPM667 ', ' NPM668 ', '200316', '566', '569', ' NPM569 ', 'NPM570 ', ' NPM572 ', ' NPM574 ', ' NPM579 ', 'NPM573-576', ' NPM580 ', ' NPM614 ', 'NPM648', 'NPM652', 'NPM656']\n",
      "NPM577\n",
      "NPM589\n",
      "NPM590\n",
      "NPM593\n",
      "NPM595\n",
      "NPM605\n",
      "NPM606\n",
      "NPM607\n",
      "NPM608\n",
      "NPM609\n",
      "NPM610\n",
      "NPM611\n",
      "NPM612\n",
      "NPM613\n",
      "NPM615\n",
      "NPM617\n",
      "NPM620\n",
      "NPM621\n",
      "NPM622\n",
      "NPM623\n",
      "NPM624\n",
      "NPM625\n",
      "NPM626\n",
      "NPM628\n",
      "NPM629\n",
      "NPM632\n",
      "NPM633\n",
      "NPM634\n",
      "NPM635\n",
      "NPM636\n",
      "NPM637\n",
      "NPM638\n",
      "NPM639\n",
      "NPM640\n",
      "NPM642\n",
      "35\n",
      "NPM592\n",
      "NPM596\n",
      "NPM604\n",
      "3\n",
      "NPM645\n",
      "NPM646\n",
      "NPM647\n",
      "NPM648\n",
      "NPM649\n",
      "NPM650\n",
      "NPM652\n",
      "NPM653\n",
      "NPM654\n",
      "NPM655\n",
      "NPM656\n",
      "NPM658\n",
      "NPM659\n",
      "NPM661\n",
      "NPM663\n",
      "NPM664\n",
      "NPM665\n",
      "NPM666\n",
      "NPM667\n",
      "NPM668\n",
      "20\n",
      "{'NPM667', 'NPM666', 'NPM653', 'NPM656', 'NPM658', 'NPM661', 'NPM652', 'NPM664', 'NPM650', 'NPM648', 'NPM663', 'NPM646', 'NPM668', 'NPM665', 'NPM654', 'NPM649', 'NPM655', 'NPM647', 'NPM645', 'NPM659'}\n"
     ]
    }
   ],
   "source": [
    "# List all wild-type animals\n",
    "wild_type_list=[f for d in wild_types+wild_types_saline for f in fourier_files if d in f]\n",
    "names=[]\n",
    "for i in wild_type_list:\n",
    "  names.append(i[5:11])\n",
    "print(len(unique(names)))\n",
    "\n",
    "# List all saline animals\n",
    "saline_list=[f for d in saline+wild_types_saline+[' NPM609 '] for f in fourier_files if d in f]\n",
    "names=[]\n",
    "for i in saline_list:\n",
    "  names.append(i[5:11])\n",
    "print(len(unique(names)))\n",
    "\n",
    "# Get all input fourier files \n",
    "path=path_Fourier_scored\n",
    "fourier_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "fourier_files = [f for f in fourier_files if \"x_ffnorm\" in f]\n",
    "\n",
    "non_train=saline+wild_types_saline+recent+held_cohorts+wild_types\n",
    "print(non_train)\n",
    " \n",
    "# Generate list of all non-training files\n",
    "files_held=[]\n",
    "files_held=[f for d in non_train for f in fourier_files if d in f]\n",
    "\n",
    "# Generate list of all training dataset files\n",
    "train_files=[]\n",
    "train_files=[f for f in fourier_files if f not in files_held] \n",
    "names=[]\n",
    "\n",
    "for i in train_files:\n",
    "  names.append(i[9:15])\n",
    "train_names=unique(names)\n",
    "np.savetxt(data_path+'training_animals.txt', train_names,delimiter=\" \", fmt=\"%s\") \n",
    "print(*train_names, sep = \"\\n\")\n",
    "print(len(unique(train_names)))\n",
    "\n",
    "# Generate list of all validation dataset files\n",
    "names=[]\n",
    "validation_files=[f for d in saline for f in fourier_files if d in f] # gather list of all saline VGAT Cre files for test dataset\n",
    "for i in validation_files:\n",
    "    names.append(i[9:15])\n",
    "    \n",
    "validation_names=unique(names)\n",
    "np.savetxt(data_path+'validation_animals.txt', validation_names,delimiter=\" \", fmt=\"%s\") \n",
    "print(*validation_names, sep = \"\\n\")\n",
    "print(len(unique(validation_names)))\n",
    "\n",
    "\n",
    "names=[]\n",
    "testing_files=[f for d in recent+wild_types for f in fourier_files if d in f] \n",
    "for i in testing_files:\n",
    "    names.append(i[9:15])\n",
    "\n",
    "testing_names=unique(names)\n",
    "np.savetxt(data_path+'testing_animals.txt', testing_names,delimiter=\" \", fmt=\"%s\") \n",
    "print(*testing_names, sep = \"\\n\")\n",
    "print(len(unique(testing_names)))\n",
    "\n",
    "print((set(testing_names)-set(validation_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "VhqEbEnEc795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NPM577', 'NPM589', 'NPM590', 'NPM593', 'NPM595', 'NPM605', 'NPM606', 'NPM607', 'NPM608', 'NPM609', 'NPM610', 'NPM611', 'NPM612', 'NPM613', 'NPM615', 'NPM617', 'NPM620', 'NPM621', 'NPM622', 'NPM623', 'NPM624', 'NPM625', 'NPM626', 'NPM628', 'NPM629', 'NPM632', 'NPM633', 'NPM634', 'NPM635', 'NPM636', 'NPM637', 'NPM638', 'NPM639', 'NPM640', 'NPM642']\n",
      "35\n",
      "(35, 2)\n",
      "[['NPM577' 11.0]\n",
      " ['NPM589' 46.0]\n",
      " ['NPM590' 54.0]\n",
      " ['NPM593' 40.0]\n",
      " ['NPM595' 40.0]\n",
      " ['NPM605' 17.0]\n",
      " ['NPM606' 17.0]\n",
      " ['NPM607' 17.0]\n",
      " ['NPM608' 17.0]\n",
      " ['NPM609' 54.0]\n",
      " ['NPM610' 17.0]\n",
      " ['NPM611' 17.0]\n",
      " ['NPM612' 16.0]\n",
      " ['NPM613' 16.0]\n",
      " ['NPM615' 17.0]\n",
      " ['NPM617' 18.0]\n",
      " ['NPM620' 16.0]\n",
      " ['NPM621' 16.0]\n",
      " ['NPM622' 16.0]\n",
      " ['NPM623' 4.0]\n",
      " ['NPM624' 16.0]\n",
      " ['NPM625' 16.0]\n",
      " ['NPM626' 16.0]\n",
      " ['NPM628' 16.0]\n",
      " ['NPM629' 16.0]\n",
      " ['NPM632' 16.0]\n",
      " ['NPM633' 16.0]\n",
      " ['NPM634' 16.0]\n",
      " ['NPM635' 16.0]\n",
      " ['NPM636' 16.0]\n",
      " ['NPM637' 16.0]\n",
      " ['NPM638' 16.0]\n",
      " ['NPM639' 16.0]\n",
      " ['NPM640' 17.0]\n",
      " ['NPM642' 16.0]]\n",
      "['NPM592', 'NPM596', 'NPM604']\n",
      "3\n",
      "(3, 2)\n",
      "[['NPM592' 56.0]\n",
      " ['NPM596' 56.0]\n",
      " ['NPM604' 17.0]]\n",
      "['NPM645', 'NPM646', 'NPM647', 'NPM656', 'NPM658', 'NPM659', 'NPM661', 'NPM663', 'NPM664', 'NPM665', 'NPM666', 'NPM667', 'NPM668', 'NPM648', 'NPM649', 'NPM650', 'NPM652', 'NPM653', 'NPM654', 'NPM655']\n",
      "20\n",
      "(20, 2)\n",
      "[['NPM645' 16.0]\n",
      " ['NPM646' 16.0]\n",
      " ['NPM647' 16.0]\n",
      " ['NPM656' 16.0]\n",
      " ['NPM658' 16.0]\n",
      " ['NPM659' 16.0]\n",
      " ['NPM661' 16.0]\n",
      " ['NPM663' 16.0]\n",
      " ['NPM664' 16.0]\n",
      " ['NPM665' 1.0]\n",
      " ['NPM666' 16.0]\n",
      " ['NPM667' 16.0]\n",
      " ['NPM668' 16.0]\n",
      " ['NPM648' 2.0]\n",
      " ['NPM649' 17.0]\n",
      " ['NPM650' 16.0]\n",
      " ['NPM652' 16.0]\n",
      " ['NPM653' 16.0]\n",
      " ['NPM654' 16.0]\n",
      " ['NPM655' 64.0]]\n"
     ]
    }
   ],
   "source": [
    "for datasets in [train_files,val_files,test_files]:\n",
    "\n",
    "    names=[]\n",
    "    counts=np.zeros((70))\n",
    "    for i in datasets:\n",
    "        names.append(i[9:15])\n",
    "    \n",
    "    unique_array=[]\n",
    "    ind=0\n",
    "    for name in names:\n",
    "        if name not in unique_array:\n",
    "            unique_array.append(name)\n",
    "            ind=unique_array.index(name)\n",
    "            counts[ind]+=1\n",
    "\n",
    "    \n",
    "        else:\n",
    "            counts[ind]+=1\n",
    "            \n",
    "    print(unique_array)\n",
    "    print(len(unique_array))\n",
    "    counts\n",
    "    bookkeeping=np.ndarray((len(unique_array),2),dtype='object')\n",
    "    print(bookkeeping.shape)\n",
    "    for i in range(len(unique_array)):\n",
    "        bookkeeping[i,0]=unique_array[i]\n",
    "        bookkeeping[i,1]=counts[i]\n",
    "\n",
    "    print(bookkeeping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfSm9vvWIANC",
    "tags": []
   },
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "TUeqUiqtqDzz"
   },
   "outputs": [],
   "source": [
    "# Create array of codes to link numeric labels in y variables to state labels\n",
    "\n",
    "codes=[None]*5\n",
    "codes[0]='Wake'\n",
    "codes[1]='NREM' \n",
    "codes[2]='REM'\n",
    "codes[3]='Seizure'\n",
    "codes[4]='Post-Ictal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "1vU07Qvwt5aU"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "e5aWdlhnkPZV"
   },
   "outputs": [],
   "source": [
    "# Run before re-running training\n",
    "\n",
    "X_train_seq=None\n",
    "X_val_seq=None\n",
    "X_test_seq=None\n",
    "model=None\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# Purges excess garbage data from RAM if training needs to be re-run\n",
    "for i in range(0,10):\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P549XhudPl3_",
    "tags": []
   },
   "source": [
    "# Pre-Process X and Y Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "xmgK2turA5hy"
   },
   "outputs": [],
   "source": [
    "# These arrays correspond to lists of indices in the feature vector\n",
    "# for each of our channels\n",
    "\n",
    "# Mask containing only the Delta/Theta Ratio and RMS variables\n",
    "mask_DTRMS=np.concatenate(([12,19,32,39,52,59,72,79],np.arange(80,100)))\n",
    "\n",
    "# Mask containing only the FFT variables and RMS - no statistical parameters\n",
    "mask_FFT_only = []\n",
    "for i in [0,1,2,3]:\n",
    "  ind1=(i*20)+6\n",
    "  ind2=(i*20)+13\n",
    "  mask_FFT_only=np.concatenate((mask_FFT_only,np.arange(ind1,ind2)))\n",
    "mask_FFT_only=np.concatenate((mask_FFT_only,np.arange(80,100)))\n",
    "mask_FFT_only=[int(i) for i in mask_FFT_only]\n",
    "\n",
    "# Masks for each individual channel's full feature sets\n",
    "mask_ECoG=np.arange(0,20)\n",
    "mask_EMG=np.arange(20,40)\n",
    "mask_HPCL=np.arange(40,60)\n",
    "mask_HPCR=np.arange(60,80)\n",
    "mask_RMS=np.arange(80,100)\n",
    "mask_FullFeats=np.arange(0,100)\n",
    "\n",
    "# Designate feature mask to use for preparing variables\n",
    "# This construction would send only HPCL and HPCR channels to training\n",
    "# mask = np.concatenate((mask_HPCL, mask_HPCR)) # mask_FullFeats lets all 100 features through\n",
    "\n",
    "mask=mask_FullFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "D-5UFT46j1TT"
   },
   "outputs": [],
   "source": [
    "# Convert ys to one-hot labels, store x variables to \n",
    "# counteract future in-place edits - makes it easier to re-run notebook\n",
    "\n",
    "\n",
    "X_train=x1\n",
    "y_train=to_categorical(y1, num_classes=5)\n",
    "\n",
    "X_val=x2\n",
    "y_val=to_categorical(y2, num_classes=5)\n",
    "\n",
    "X_test=x3\n",
    "y_test=to_categorical(y3, num_classes=5)\n",
    "\n",
    "X_train_masked=X_train[:,mask]\n",
    "X_val_masked=X_val[:,mask]\n",
    "X_test_masked=X_test[:,mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMX_QRQQsUMW",
    "tags": []
   },
   "source": [
    "#  Grid Search Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMX_QRQQsUMW",
    "tags": []
   },
   "source": [
    "## Full Deep Learning Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWlTA2Hunjaf"
   },
   "outputs": [],
   "source": [
    "Channel_Masks=[[mask_ECoG, 'ECoG_only'],\n",
    "               [np.concatenate((mask_HPCL,mask_HPCR)), '2xHPC'],\n",
    "               [np.concatenate((mask_EMG,mask_HPCL,mask_HPCR,mask_RMS)), 'no_ECoG'],\n",
    "               [np.concatenate((mask_EMG,mask_RMS)), 'EMG_RMS_only' ],\n",
    "               [np.concatenate((mask_ECoG,mask_EMG,mask_RMS)), 'noHPC']]\n",
    "\n",
    "Feature_Masks = [[mask_FullFeats, 'FullFeats'],\n",
    "                 [mask_FFT_only, 'FFT only'],\n",
    "                 [mask_DTRMS,'DTR+RMS only']]\n",
    "\n",
    "# Generate training variables which are common between models\n",
    "batch_size, metrics, optimizer, activity_regularizer = common_training_parameters()\n",
    "\n",
    "# Number of epochs to train \n",
    "epochs=20\n",
    "\n",
    "for window_length in [3,2,1,0]:\n",
    "  # Test all four windowing variants \n",
    "  # Grid Search Iteration Count in This Loop: 4 Combinations\n",
    "  for masks in Feature_Masks:\n",
    "    # Test all three Feature Vectors \n",
    "    # Grid Search Iteration Count in This Loop: 12 Combinations\n",
    "      \n",
    "    X_train_masked=X_train[:,masks[0]]\n",
    "    X_val_masked=X_val[:,masks[0]]\n",
    "    X_test_masked=X_test[:,masks[0]]\n",
    "\n",
    "    # Mask name for training logs\n",
    "    submodel=masks[1]\n",
    "    print(submodel)  \n",
    "\n",
    "    # Maximum number of features determined by last dimension in shape of masked array\n",
    "    max_feats=X_train_masked.shape[1]\n",
    "\n",
    "    # Generate windowed sequences from full epoch arrays\n",
    "      \n",
    "    # Number of time points on either side of scored epoch to use for input variables...\n",
    "    # determined by window length.  i.e. window_length 3 leads to an input 7 epochs long\n",
    "    X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "    X_val_seq=generate_sequences(X_val_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "    X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "\n",
    "    y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "    y_val_seq=generate_sequences(y_val, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "    y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "      \n",
    "    # Generate class weight array based on logarithmic modification of sk-learn class weighting\n",
    "    class_weight=norm_sklearn_classweight(y_train_seq, mu=False)\n",
    "    print(class_weight)\n",
    "\n",
    "    print(X_train_seq.shape)\n",
    "\n",
    "    for neuron_num in [200,100,50]:\n",
    "        # Test Three Initial-Layer Neuron Counts\n",
    "        # Grid Search Iteration Count in This Loop: 36 Combinations\n",
    "\n",
    "        \n",
    "        # Test All Above Over 4 Types of Models: 144 Total Model Variants\n",
    "        # Dense Neural Net Model training\n",
    "        model, model_history, logpath, csvpath, model_name = create_train_Dense_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=1, data_path=data_path)\n",
    "        gc.collect()\n",
    "        # Dense Neural Net Model Report\n",
    "        savepath='./Results_GPU/Dense/'\n",
    "        save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                            model_name=model_name+f'_{submodel}_win{window_length}', \n",
    "                            model_num=neuron_num, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n",
    "        gc.collect()\n",
    "        \n",
    "        # LSTM Model training\n",
    "        model, model_history, logpath, csvpath, model_name = create_train_LSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=1, data_path=data_path)\n",
    "        gc.collect()\n",
    "        # LSTM Model Report\n",
    "        savepath='./Results_GPU/LSTM/'\n",
    "        save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                            model_name=model_name+f'_{submodel}_win{window_length}_', \n",
    "                            model_num=neuron_num, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n",
    "        gc.collect()\n",
    "        \n",
    "        # BiLSTM Model Training\n",
    "        model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=1, data_path=data_path)\n",
    "        gc.collect()\n",
    "        # BiLSTM Model Report\n",
    "        savepath='./Results_GPU/BiLSTM/'\n",
    "        save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                            model_name=model_name+f'single-layer_{submodel}_win{window_length}_', \n",
    "                            model_num=neuron_num, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n",
    "      \n",
    "\n",
    "        # Stacked BiLSTM Model Training\n",
    "        model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=4, data_path=data_path)\n",
    "        gc.collect()\n",
    "        # Stacked BiLSTM Model Report\n",
    "        savepath='./Results_GPU/Quad_BiLSTM/'\n",
    "        save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                            model_name=model_name+f'quad-layer_{submodel}_win{window_length}_', \n",
    "                            model_num=neuron_num, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dg8f_hnUtyl-",
    "tags": []
   },
   "source": [
    "## Evaluate Top Models to 60 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "PVeK2QXDsaxV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FullFeats\n",
      "(1527114, 7, 100)\n",
      "[3.85603689e-01 4.84991322e-01 2.94539563e+00 1.01133377e+03\n",
      " 2.34219939e+02]\n",
      "{0: 1.0, 1: 1.229320869330697, 2: 3.033188298565099, 3: 8.871970458237705, 4: 7.409205733125976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHARVE4\\AppData\\Local\\Temp\\ipykernel_6672\\3115110011.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  j = int(np.where(classes==i)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_155\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_195 (Bidirect  (None, 7, 400)           481600    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dropout_233 (Dropout)       (None, 7, 400)            0         \n",
      "                                                                 \n",
      " flatten_155 (Flatten)       (None, 2800)              0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 5)                 14005     \n",
      "                                                                 \n",
      " activation_155 (Activation)  (None, 5)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 495,605\n",
      "Trainable params: 495,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.42619, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.8482420444488525-0.769612--val-0.9604126811027527-0.912071.h5\n",
      "707/707 - 36s - loss: 1.1328 - accuracy: 0.7696 - tp: 687970.0000 - fp: 33915.0000 - tn: 6074541.0000 - fn: 839144.0000 - categorical_accuracy: 0.7696 - precision: 0.9530 - recall: 0.4505 - auc: 0.8482 - categorical_crossentropy: 0.8682 - val_loss: 0.4262 - val_accuracy: 0.9121 - val_tp: 237051.0000 - val_fp: 12041.0000 - val_tn: 1102495.0000 - val_fn: 41583.0000 - val_categorical_accuracy: 0.9121 - val_precision: 0.9517 - val_recall: 0.8508 - val_auc: 0.9604 - val_categorical_crossentropy: 0.3854 - 36s/epoch - 51ms/step\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 2: val_loss improved from 0.42619 to 0.31655, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9492477774620056-0.897177--val-0.9696299433708191-0.918520.h5\n",
      "707/707 - 27s - loss: 0.5518 - accuracy: 0.8972 - tp: 1279803.0000 - fp: 90555.0000 - tn: 6017901.0000 - fn: 247311.0000 - categorical_accuracy: 0.8972 - precision: 0.9339 - recall: 0.8381 - auc: 0.9492 - categorical_crossentropy: 0.3891 - val_loss: 0.3166 - val_accuracy: 0.9185 - val_tp: 250194.0000 - val_fp: 16752.0000 - val_tn: 1097784.0000 - val_fn: 28440.0000 - val_categorical_accuracy: 0.9185 - val_precision: 0.9372 - val_recall: 0.8979 - val_auc: 0.9696 - val_categorical_crossentropy: 0.2726 - 27s/epoch - 38ms/step\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 3: val_loss improved from 0.31655 to 0.27260, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9657349586486816-0.914248--val-0.9760251045227051-0.925494.h5\n",
      "707/707 - 27s - loss: 0.4286 - accuracy: 0.9142 - tp: 1354409.0000 - fp: 96440.0000 - tn: 6012016.0000 - fn: 172705.0000 - categorical_accuracy: 0.9142 - precision: 0.9335 - recall: 0.8869 - auc: 0.9657 - categorical_crossentropy: 0.2854 - val_loss: 0.2726 - val_accuracy: 0.9255 - val_tp: 254356.0000 - val_fp: 16811.0000 - val_tn: 1097725.0000 - val_fn: 24278.0000 - val_categorical_accuracy: 0.9255 - val_precision: 0.9380 - val_recall: 0.9129 - val_auc: 0.9760 - val_categorical_crossentropy: 0.2274 - 27s/epoch - 38ms/step\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 4: val_loss improved from 0.27260 to 0.24801, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9743158221244812-0.924618--val-0.9800397157669067-0.930913.h5\n",
      "707/707 - 27s - loss: 0.3643 - accuracy: 0.9246 - tp: 1386584.0000 - fp: 93080.0000 - tn: 6015376.0000 - fn: 140530.0000 - categorical_accuracy: 0.9246 - precision: 0.9371 - recall: 0.9080 - auc: 0.9743 - categorical_crossentropy: 0.2344 - val_loss: 0.2480 - val_accuracy: 0.9309 - val_tp: 256785.0000 - val_fp: 16299.0000 - val_tn: 1098237.0000 - val_fn: 21849.0000 - val_categorical_accuracy: 0.9309 - val_precision: 0.9403 - val_recall: 0.9216 - val_auc: 0.9800 - val_categorical_crossentropy: 0.2019 - 27s/epoch - 38ms/step\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 5: val_loss improved from 0.24801 to 0.23273, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9789283871650696-0.931085--val-0.98243647813797-0.934828.h5\n",
      "707/707 - 27s - loss: 0.3260 - accuracy: 0.9311 - tp: 1404021.0000 - fp: 88979.0000 - tn: 6019477.0000 - fn: 123093.0000 - categorical_accuracy: 0.9311 - precision: 0.9404 - recall: 0.9194 - auc: 0.9789 - categorical_crossentropy: 0.2059 - val_loss: 0.2327 - val_accuracy: 0.9348 - val_tp: 258383.0000 - val_fp: 15747.0000 - val_tn: 1098789.0000 - val_fn: 20251.0000 - val_categorical_accuracy: 0.9348 - val_precision: 0.9426 - val_recall: 0.9273 - val_auc: 0.9824 - val_categorical_crossentropy: 0.1861 - 27s/epoch - 38ms/step\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 6: val_loss improved from 0.23273 to 0.22257, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.981708288192749-0.935454--val-0.9840094447135925-0.937667.h5\n",
      "707/707 - 28s - loss: 0.3017 - accuracy: 0.9355 - tp: 1414624.0000 - fp: 85251.0000 - tn: 6023205.0000 - fn: 112490.0000 - categorical_accuracy: 0.9355 - precision: 0.9432 - recall: 0.9263 - auc: 0.9817 - categorical_crossentropy: 0.1880 - val_loss: 0.2226 - val_accuracy: 0.9377 - val_tp: 259423.0000 - val_fp: 15291.0000 - val_tn: 1099245.0000 - val_fn: 19211.0000 - val_categorical_accuracy: 0.9377 - val_precision: 0.9443 - val_recall: 0.9311 - val_auc: 0.9840 - val_categorical_crossentropy: 0.1759 - 28s/epoch - 39ms/step\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 7: val_loss improved from 0.22257 to 0.21448, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9836238026618958-0.939092--val-0.9851704239845276-0.939831.h5\n",
      "707/707 - 27s - loss: 0.2845 - accuracy: 0.9391 - tp: 1423821.0000 - fp: 81695.0000 - tn: 6026761.0000 - fn: 103293.0000 - categorical_accuracy: 0.9391 - precision: 0.9457 - recall: 0.9324 - auc: 0.9836 - categorical_crossentropy: 0.1755 - val_loss: 0.2145 - val_accuracy: 0.9398 - val_tp: 260268.0000 - val_fp: 14808.0000 - val_tn: 1099728.0000 - val_fn: 18366.0000 - val_categorical_accuracy: 0.9398 - val_precision: 0.9462 - val_recall: 0.9341 - val_auc: 0.9852 - val_categorical_crossentropy: 0.1680 - 27s/epoch - 38ms/step\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 8: val_loss improved from 0.21448 to 0.20832, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9849640727043152-0.942026--val-0.9859639406204224-0.941482.h5\n",
      "707/707 - 27s - loss: 0.2715 - accuracy: 0.9420 - tp: 1429674.0000 - fp: 78679.0000 - tn: 6029777.0000 - fn: 97440.0000 - categorical_accuracy: 0.9420 - precision: 0.9478 - recall: 0.9362 - auc: 0.9850 - categorical_crossentropy: 0.1663 - val_loss: 0.2083 - val_accuracy: 0.9415 - val_tp: 260859.0000 - val_fp: 14478.0000 - val_tn: 1100058.0000 - val_fn: 17775.0000 - val_categorical_accuracy: 0.9415 - val_precision: 0.9474 - val_recall: 0.9362 - val_auc: 0.9860 - val_categorical_crossentropy: 0.1623 - 27s/epoch - 37ms/step\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20832 to 0.20353, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9860442876815796-0.944366--val-0.9865244030952454-0.943065.h5\n",
      "707/707 - 26s - loss: 0.2609 - accuracy: 0.9444 - tp: 1434150.0000 - fp: 76019.0000 - tn: 6032437.0000 - fn: 92964.0000 - categorical_accuracy: 0.9444 - precision: 0.9497 - recall: 0.9391 - auc: 0.9860 - categorical_crossentropy: 0.1588 - val_loss: 0.2035 - val_accuracy: 0.9431 - val_tp: 261347.0000 - val_fp: 14164.0000 - val_tn: 1100372.0000 - val_fn: 17287.0000 - val_categorical_accuracy: 0.9431 - val_precision: 0.9486 - val_recall: 0.9380 - val_auc: 0.9865 - val_categorical_crossentropy: 0.1581 - 26s/epoch - 37ms/step\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 10: val_loss improved from 0.20353 to 0.19838, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9869120121002197-0.946189--val-0.9871291518211365-0.944526.h5\n",
      "707/707 - 26s - loss: 0.2521 - accuracy: 0.9462 - tp: 1437486.0000 - fp: 73690.0000 - tn: 6034766.0000 - fn: 89628.0000 - categorical_accuracy: 0.9462 - precision: 0.9512 - recall: 0.9413 - auc: 0.9869 - categorical_crossentropy: 0.1529 - val_loss: 0.1984 - val_accuracy: 0.9445 - val_tp: 261878.0000 - val_fp: 13907.0000 - val_tn: 1100629.0000 - val_fn: 16756.0000 - val_categorical_accuracy: 0.9445 - val_precision: 0.9496 - val_recall: 0.9399 - val_auc: 0.9871 - val_categorical_crossentropy: 0.1536 - 26s/epoch - 37ms/step\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 11: val_loss improved from 0.19838 to 0.19603, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.987625002861023-0.947878--val-0.9872577786445618-0.945215.h5\n",
      "707/707 - 28s - loss: 0.2447 - accuracy: 0.9479 - tp: 1440473.0000 - fp: 71528.0000 - tn: 6036928.0000 - fn: 86641.0000 - categorical_accuracy: 0.9479 - precision: 0.9527 - recall: 0.9433 - auc: 0.9876 - categorical_crossentropy: 0.1478 - val_loss: 0.1960 - val_accuracy: 0.9452 - val_tp: 262076.0000 - val_fp: 13706.0000 - val_tn: 1100830.0000 - val_fn: 16558.0000 - val_categorical_accuracy: 0.9452 - val_precision: 0.9503 - val_recall: 0.9406 - val_auc: 0.9873 - val_categorical_crossentropy: 0.1520 - 28s/epoch - 39ms/step\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 12: val_loss improved from 0.19603 to 0.19181, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9882571697235107-0.949210--val-0.9877684712409973-0.946353.h5\n",
      "707/707 - 29s - loss: 0.2381 - accuracy: 0.9492 - tp: 1442928.0000 - fp: 69716.0000 - tn: 6038740.0000 - fn: 84186.0000 - categorical_accuracy: 0.9492 - precision: 0.9539 - recall: 0.9449 - auc: 0.9883 - categorical_crossentropy: 0.1435 - val_loss: 0.1918 - val_accuracy: 0.9464 - val_tp: 262481.0000 - val_fp: 13432.0000 - val_tn: 1101104.0000 - val_fn: 16153.0000 - val_categorical_accuracy: 0.9464 - val_precision: 0.9513 - val_recall: 0.9420 - val_auc: 0.9878 - val_categorical_crossentropy: 0.1484 - 29s/epoch - 41ms/step\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 13: val_loss improved from 0.19181 to 0.18936, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9887272119522095-0.950485--val-0.9879927039146423-0.946991.h5\n",
      "707/707 - 30s - loss: 0.2326 - accuracy: 0.9505 - tp: 1445186.0000 - fp: 68303.0000 - tn: 6040153.0000 - fn: 81928.0000 - categorical_accuracy: 0.9505 - precision: 0.9549 - recall: 0.9464 - auc: 0.9887 - categorical_crossentropy: 0.1399 - val_loss: 0.1894 - val_accuracy: 0.9470 - val_tp: 262710.0000 - val_fp: 13300.0000 - val_tn: 1101236.0000 - val_fn: 15924.0000 - val_categorical_accuracy: 0.9470 - val_precision: 0.9518 - val_recall: 0.9428 - val_auc: 0.9880 - val_categorical_crossentropy: 0.1466 - 30s/epoch - 43ms/step\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 14: val_loss improved from 0.18936 to 0.18691, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9892020225524902-0.951667--val-0.9881802797317505-0.947652.h5\n",
      "707/707 - 29s - loss: 0.2274 - accuracy: 0.9517 - tp: 1447288.0000 - fp: 66750.0000 - tn: 6041706.0000 - fn: 79826.0000 - categorical_accuracy: 0.9517 - precision: 0.9559 - recall: 0.9477 - auc: 0.9892 - categorical_crossentropy: 0.1365 - val_loss: 0.1869 - val_accuracy: 0.9477 - val_tp: 262940.0000 - val_fp: 13161.0000 - val_tn: 1101375.0000 - val_fn: 15694.0000 - val_categorical_accuracy: 0.9477 - val_precision: 0.9523 - val_recall: 0.9437 - val_auc: 0.9882 - val_categorical_crossentropy: 0.1447 - 29s/epoch - 41ms/step\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 15: val_loss improved from 0.18691 to 0.18388, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9895872473716736-0.952717--val-0.9885575771331787-0.948556.h5\n",
      "707/707 - 29s - loss: 0.2227 - accuracy: 0.9527 - tp: 1449190.0000 - fp: 65320.0000 - tn: 6043136.0000 - fn: 77924.0000 - categorical_accuracy: 0.9527 - precision: 0.9569 - recall: 0.9490 - auc: 0.9896 - categorical_crossentropy: 0.1335 - val_loss: 0.1839 - val_accuracy: 0.9486 - val_tp: 263210.0000 - val_fp: 13011.0000 - val_tn: 1101525.0000 - val_fn: 15424.0000 - val_categorical_accuracy: 0.9486 - val_precision: 0.9529 - val_recall: 0.9446 - val_auc: 0.9886 - val_categorical_crossentropy: 0.1422 - 29s/epoch - 42ms/step\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 16: val_loss improved from 0.18388 to 0.18136, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9899402260780334-0.953738--val-0.9888339042663574-0.949328.h5\n",
      "707/707 - 30s - loss: 0.2187 - accuracy: 0.9537 - tp: 1450742.0000 - fp: 64121.0000 - tn: 6044335.0000 - fn: 76372.0000 - categorical_accuracy: 0.9537 - precision: 0.9577 - recall: 0.9500 - auc: 0.9899 - categorical_crossentropy: 0.1309 - val_loss: 0.1814 - val_accuracy: 0.9493 - val_tp: 263449.0000 - val_fp: 12831.0000 - val_tn: 1101705.0000 - val_fn: 15185.0000 - val_categorical_accuracy: 0.9493 - val_precision: 0.9536 - val_recall: 0.9455 - val_auc: 0.9888 - val_categorical_crossentropy: 0.1403 - 30s/epoch - 42ms/step\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 17: val_loss improved from 0.18136 to 0.17949, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9902598261833191-0.954431--val-0.9889631867408752-0.949870.h5\n",
      "707/707 - 30s - loss: 0.2148 - accuracy: 0.9544 - tp: 1452071.0000 - fp: 63132.0000 - tn: 6045324.0000 - fn: 75043.0000 - categorical_accuracy: 0.9544 - precision: 0.9583 - recall: 0.9509 - auc: 0.9903 - categorical_crossentropy: 0.1284 - val_loss: 0.1795 - val_accuracy: 0.9499 - val_tp: 263654.0000 - val_fp: 12714.0000 - val_tn: 1101822.0000 - val_fn: 14980.0000 - val_categorical_accuracy: 0.9499 - val_precision: 0.9540 - val_recall: 0.9462 - val_auc: 0.9890 - val_categorical_crossentropy: 0.1389 - 30s/epoch - 42ms/step\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 18: val_loss improved from 0.17949 to 0.17873, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9905667304992676-0.955286--val-0.9890041947364807-0.949855.h5\n",
      "707/707 - 30s - loss: 0.2113 - accuracy: 0.9553 - tp: 1453450.0000 - fp: 62121.0000 - tn: 6046335.0000 - fn: 73664.0000 - categorical_accuracy: 0.9553 - precision: 0.9590 - recall: 0.9518 - auc: 0.9906 - categorical_crossentropy: 0.1261 - val_loss: 0.1787 - val_accuracy: 0.9499 - val_tp: 263672.0000 - val_fp: 12706.0000 - val_tn: 1101830.0000 - val_fn: 14962.0000 - val_categorical_accuracy: 0.9499 - val_precision: 0.9540 - val_recall: 0.9463 - val_auc: 0.9890 - val_categorical_crossentropy: 0.1387 - 30s/epoch - 43ms/step\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 19: val_loss improved from 0.17873 to 0.17655, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9908028841018677-0.955932--val-0.9892177581787109-0.950580.h5\n",
      "707/707 - 31s - loss: 0.2082 - accuracy: 0.9559 - tp: 1454778.0000 - fp: 61199.0000 - tn: 6047257.0000 - fn: 72336.0000 - categorical_accuracy: 0.9559 - precision: 0.9596 - recall: 0.9526 - auc: 0.9908 - categorical_crossentropy: 0.1242 - val_loss: 0.1765 - val_accuracy: 0.9506 - val_tp: 263874.0000 - val_fp: 12572.0000 - val_tn: 1101964.0000 - val_fn: 14760.0000 - val_categorical_accuracy: 0.9506 - val_precision: 0.9545 - val_recall: 0.9470 - val_auc: 0.9892 - val_categorical_crossentropy: 0.1369 - 31s/epoch - 43ms/step\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 20: val_loss improved from 0.17655 to 0.17497, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9909999370574951-0.956631--val-0.9894100427627563-0.951000.h5\n",
      "707/707 - 30s - loss: 0.2053 - accuracy: 0.9566 - tp: 1455809.0000 - fp: 60252.0000 - tn: 6048204.0000 - fn: 71305.0000 - categorical_accuracy: 0.9566 - precision: 0.9603 - recall: 0.9533 - auc: 0.9910 - categorical_crossentropy: 0.1224 - val_loss: 0.1750 - val_accuracy: 0.9510 - val_tp: 264029.0000 - val_fp: 12489.0000 - val_tn: 1102047.0000 - val_fn: 14605.0000 - val_categorical_accuracy: 0.9510 - val_precision: 0.9548 - val_recall: 0.9476 - val_auc: 0.9894 - val_categorical_crossentropy: 0.1358 - 30s/epoch - 43ms/step\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 21: val_loss improved from 0.17497 to 0.17287, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9912703037261963-0.957120--val-0.989637017250061-0.951578.h5\n",
      "707/707 - 29s - loss: 0.2026 - accuracy: 0.9571 - tp: 1456774.0000 - fp: 59713.0000 - tn: 6048743.0000 - fn: 70340.0000 - categorical_accuracy: 0.9571 - precision: 0.9606 - recall: 0.9539 - auc: 0.9913 - categorical_crossentropy: 0.1206 - val_loss: 0.1729 - val_accuracy: 0.9516 - val_tp: 264225.0000 - val_fp: 12310.0000 - val_tn: 1102226.0000 - val_fn: 14409.0000 - val_categorical_accuracy: 0.9516 - val_precision: 0.9555 - val_recall: 0.9483 - val_auc: 0.9896 - val_categorical_crossentropy: 0.1341 - 29s/epoch - 41ms/step\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 22: val_loss improved from 0.17287 to 0.17182, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9914249181747437-0.957656--val-0.9896904826164246-0.951808.h5\n",
      "707/707 - 29s - loss: 0.2002 - accuracy: 0.9577 - tp: 1457589.0000 - fp: 59064.0000 - tn: 6049392.0000 - fn: 69525.0000 - categorical_accuracy: 0.9577 - precision: 0.9611 - recall: 0.9545 - auc: 0.9914 - categorical_crossentropy: 0.1192 - val_loss: 0.1718 - val_accuracy: 0.9518 - val_tp: 264374.0000 - val_fp: 12269.0000 - val_tn: 1102267.0000 - val_fn: 14260.0000 - val_categorical_accuracy: 0.9518 - val_precision: 0.9557 - val_recall: 0.9488 - val_auc: 0.9897 - val_categorical_crossentropy: 0.1334 - 29s/epoch - 41ms/step\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 23: val_loss improved from 0.17182 to 0.17068, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9916044473648071-0.958276--val-0.9898571968078613-0.952124.h5\n",
      "707/707 - 29s - loss: 0.1979 - accuracy: 0.9583 - tp: 1458689.0000 - fp: 58304.0000 - tn: 6050152.0000 - fn: 68425.0000 - categorical_accuracy: 0.9583 - precision: 0.9616 - recall: 0.9552 - auc: 0.9916 - categorical_crossentropy: 0.1178 - val_loss: 0.1707 - val_accuracy: 0.9521 - val_tp: 264418.0000 - val_fp: 12228.0000 - val_tn: 1102308.0000 - val_fn: 14216.0000 - val_categorical_accuracy: 0.9521 - val_precision: 0.9558 - val_recall: 0.9490 - val_auc: 0.9899 - val_categorical_crossentropy: 0.1327 - 29s/epoch - 41ms/step\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 24: val_loss improved from 0.17068 to 0.16876, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9918082356452942-0.958665--val-0.9900415539741516-0.952673.h5\n",
      "707/707 - 30s - loss: 0.1955 - accuracy: 0.9587 - tp: 1459337.0000 - fp: 57770.0000 - tn: 6050686.0000 - fn: 67777.0000 - categorical_accuracy: 0.9587 - precision: 0.9619 - recall: 0.9556 - auc: 0.9918 - categorical_crossentropy: 0.1163 - val_loss: 0.1688 - val_accuracy: 0.9527 - val_tp: 264566.0000 - val_fp: 12103.0000 - val_tn: 1102433.0000 - val_fn: 14068.0000 - val_categorical_accuracy: 0.9527 - val_precision: 0.9563 - val_recall: 0.9495 - val_auc: 0.9900 - val_categorical_crossentropy: 0.1311 - 30s/epoch - 42ms/step\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16876\n",
      "707/707 - 30s - loss: 0.1937 - accuracy: 0.9591 - tp: 1460170.0000 - fp: 57260.0000 - tn: 6051196.0000 - fn: 66944.0000 - categorical_accuracy: 0.9591 - precision: 0.9623 - recall: 0.9562 - auc: 0.9919 - categorical_crossentropy: 0.1151 - val_loss: 0.1693 - val_accuracy: 0.9525 - val_tp: 264542.0000 - val_fp: 12166.0000 - val_tn: 1102370.0000 - val_fn: 14092.0000 - val_categorical_accuracy: 0.9525 - val_precision: 0.9560 - val_recall: 0.9494 - val_auc: 0.9899 - val_categorical_crossentropy: 0.1319 - 30s/epoch - 43ms/step\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 26: val_loss improved from 0.16876 to 0.16801, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9920910000801086-0.959438--val-0.9899947643280029-0.952831.h5\n",
      "707/707 - 30s - loss: 0.1917 - accuracy: 0.9594 - tp: 1460784.0000 - fp: 56788.0000 - tn: 6051668.0000 - fn: 66330.0000 - categorical_accuracy: 0.9594 - precision: 0.9626 - recall: 0.9566 - auc: 0.9921 - categorical_crossentropy: 0.1139 - val_loss: 0.1680 - val_accuracy: 0.9528 - val_tp: 264640.0000 - val_fp: 12091.0000 - val_tn: 1102445.0000 - val_fn: 13994.0000 - val_categorical_accuracy: 0.9528 - val_precision: 0.9563 - val_recall: 0.9498 - val_auc: 0.9900 - val_categorical_crossentropy: 0.1310 - 30s/epoch - 43ms/step\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 27: val_loss improved from 0.16801 to 0.16738, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9922328591346741-0.959725--val-0.9900753498077393-0.953032.h5\n",
      "707/707 - 29s - loss: 0.1900 - accuracy: 0.9597 - tp: 1461171.0000 - fp: 56473.0000 - tn: 6051983.0000 - fn: 65943.0000 - categorical_accuracy: 0.9597 - precision: 0.9628 - recall: 0.9568 - auc: 0.9922 - categorical_crossentropy: 0.1129 - val_loss: 0.1674 - val_accuracy: 0.9530 - val_tp: 264755.0000 - val_fp: 12061.0000 - val_tn: 1102475.0000 - val_fn: 13879.0000 - val_categorical_accuracy: 0.9530 - val_precision: 0.9564 - val_recall: 0.9502 - val_auc: 0.9901 - val_categorical_crossentropy: 0.1307 - 29s/epoch - 42ms/step\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 28: val_loss improved from 0.16738 to 0.16676, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9923798441886902-0.960173--val-0.9901208281517029-0.953229.h5\n",
      "707/707 - 29s - loss: 0.1881 - accuracy: 0.9602 - tp: 1461949.0000 - fp: 55775.0000 - tn: 6052681.0000 - fn: 65165.0000 - categorical_accuracy: 0.9602 - precision: 0.9633 - recall: 0.9573 - auc: 0.9924 - categorical_crossentropy: 0.1117 - val_loss: 0.1668 - val_accuracy: 0.9532 - val_tp: 264785.0000 - val_fp: 12043.0000 - val_tn: 1102493.0000 - val_fn: 13849.0000 - val_categorical_accuracy: 0.9532 - val_precision: 0.9565 - val_recall: 0.9503 - val_auc: 0.9901 - val_categorical_crossentropy: 0.1304 - 29s/epoch - 41ms/step\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 29: val_loss improved from 0.16676 to 0.16638, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9925014972686768-0.960465--val-0.9900913238525391-0.953265.h5\n",
      "707/707 - 28s - loss: 0.1865 - accuracy: 0.9605 - tp: 1462568.0000 - fp: 55467.0000 - tn: 6052989.0000 - fn: 64546.0000 - categorical_accuracy: 0.9605 - precision: 0.9635 - recall: 0.9577 - auc: 0.9925 - categorical_crossentropy: 0.1108 - val_loss: 0.1664 - val_accuracy: 0.9533 - val_tp: 264824.0000 - val_fp: 11989.0000 - val_tn: 1102547.0000 - val_fn: 13810.0000 - val_categorical_accuracy: 0.9533 - val_precision: 0.9567 - val_recall: 0.9504 - val_auc: 0.9901 - val_categorical_crossentropy: 0.1303 - 28s/epoch - 40ms/step\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 30: val_loss improved from 0.16638 to 0.16570, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9926083087921143-0.960823--val-0.9901555776596069-0.953498.h5\n",
      "707/707 - 29s - loss: 0.1850 - accuracy: 0.9608 - tp: 1463118.0000 - fp: 55061.0000 - tn: 6053395.0000 - fn: 63996.0000 - categorical_accuracy: 0.9608 - precision: 0.9637 - recall: 0.9581 - auc: 0.9926 - categorical_crossentropy: 0.1098 - val_loss: 0.1657 - val_accuracy: 0.9535 - val_tp: 264905.0000 - val_fp: 11973.0000 - val_tn: 1102563.0000 - val_fn: 13729.0000 - val_categorical_accuracy: 0.9535 - val_precision: 0.9568 - val_recall: 0.9507 - val_auc: 0.9902 - val_categorical_crossentropy: 0.1299 - 29s/epoch - 41ms/step\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 31: val_loss improved from 0.16570 to 0.16449, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9927321076393127-0.961207--val-0.9902842044830322-0.953803.h5\n",
      "707/707 - 30s - loss: 0.1835 - accuracy: 0.9612 - tp: 1463790.0000 - fp: 54657.0000 - tn: 6053799.0000 - fn: 63324.0000 - categorical_accuracy: 0.9612 - precision: 0.9640 - recall: 0.9585 - auc: 0.9927 - categorical_crossentropy: 0.1089 - val_loss: 0.1645 - val_accuracy: 0.9538 - val_tp: 264972.0000 - val_fp: 11881.0000 - val_tn: 1102655.0000 - val_fn: 13662.0000 - val_categorical_accuracy: 0.9538 - val_precision: 0.9571 - val_recall: 0.9510 - val_auc: 0.9903 - val_categorical_crossentropy: 0.1290 - 30s/epoch - 42ms/step\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 32: val_loss improved from 0.16449 to 0.16382, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9928073287010193-0.961294--val-0.9903212785720825-0.954022.h5\n",
      "707/707 - 30s - loss: 0.1821 - accuracy: 0.9613 - tp: 1464018.0000 - fp: 54351.0000 - tn: 6054105.0000 - fn: 63096.0000 - categorical_accuracy: 0.9613 - precision: 0.9642 - recall: 0.9587 - auc: 0.9928 - categorical_crossentropy: 0.1081 - val_loss: 0.1638 - val_accuracy: 0.9540 - val_tp: 265032.0000 - val_fp: 11863.0000 - val_tn: 1102673.0000 - val_fn: 13602.0000 - val_categorical_accuracy: 0.9540 - val_precision: 0.9572 - val_recall: 0.9512 - val_auc: 0.9903 - val_categorical_crossentropy: 0.1286 - 30s/epoch - 43ms/step\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16382\n",
      "707/707 - 30s - loss: 0.1808 - accuracy: 0.9615 - tp: 1464441.0000 - fp: 54168.0000 - tn: 6054288.0000 - fn: 62673.0000 - categorical_accuracy: 0.9615 - precision: 0.9643 - recall: 0.9590 - auc: 0.9929 - categorical_crossentropy: 0.1074 - val_loss: 0.1646 - val_accuracy: 0.9536 - val_tp: 264974.0000 - val_fp: 11959.0000 - val_tn: 1102577.0000 - val_fn: 13660.0000 - val_categorical_accuracy: 0.9536 - val_precision: 0.9568 - val_recall: 0.9510 - val_auc: 0.9902 - val_categorical_crossentropy: 0.1296 - 30s/epoch - 42ms/step\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 34: val_loss improved from 0.16382 to 0.16370, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9929944276809692-0.961850--val-0.9902653098106384-0.953861.h5\n",
      "707/707 - 30s - loss: 0.1797 - accuracy: 0.9618 - tp: 1465015.0000 - fp: 53753.0000 - tn: 6054703.0000 - fn: 62099.0000 - categorical_accuracy: 0.9618 - precision: 0.9646 - recall: 0.9593 - auc: 0.9930 - categorical_crossentropy: 0.1067 - val_loss: 0.1637 - val_accuracy: 0.9539 - val_tp: 265048.0000 - val_fp: 11902.0000 - val_tn: 1102634.0000 - val_fn: 13586.0000 - val_categorical_accuracy: 0.9539 - val_precision: 0.9570 - val_recall: 0.9512 - val_auc: 0.9903 - val_categorical_crossentropy: 0.1289 - 30s/epoch - 42ms/step\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 35: val_loss improved from 0.16370 to 0.16210, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9931133389472961-0.962089--val-0.9904351234436035-0.954421.h5\n",
      "707/707 - 29s - loss: 0.1782 - accuracy: 0.9621 - tp: 1465328.0000 - fp: 53433.0000 - tn: 6055023.0000 - fn: 61786.0000 - categorical_accuracy: 0.9621 - precision: 0.9648 - recall: 0.9595 - auc: 0.9931 - categorical_crossentropy: 0.1058 - val_loss: 0.1621 - val_accuracy: 0.9544 - val_tp: 265218.0000 - val_fp: 11788.0000 - val_tn: 1102748.0000 - val_fn: 13416.0000 - val_categorical_accuracy: 0.9544 - val_precision: 0.9574 - val_recall: 0.9519 - val_auc: 0.9904 - val_categorical_crossentropy: 0.1276 - 29s/epoch - 41ms/step\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 36: val_loss improved from 0.16210 to 0.16102, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9931340217590332-0.962261--val-0.990523099899292-0.954614.h5\n",
      "707/707 - 30s - loss: 0.1772 - accuracy: 0.9623 - tp: 1465656.0000 - fp: 53255.0000 - tn: 6055201.0000 - fn: 61458.0000 - categorical_accuracy: 0.9623 - precision: 0.9649 - recall: 0.9598 - auc: 0.9931 - categorical_crossentropy: 0.1053 - val_loss: 0.1610 - val_accuracy: 0.9546 - val_tp: 265287.0000 - val_fp: 11704.0000 - val_tn: 1102832.0000 - val_fn: 13347.0000 - val_categorical_accuracy: 0.9546 - val_precision: 0.9577 - val_recall: 0.9521 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1267 - 30s/epoch - 42ms/step\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16102\n",
      "707/707 - 29s - loss: 0.1760 - accuracy: 0.9625 - tp: 1466080.0000 - fp: 52901.0000 - tn: 6055555.0000 - fn: 61034.0000 - categorical_accuracy: 0.9625 - precision: 0.9652 - recall: 0.9600 - auc: 0.9933 - categorical_crossentropy: 0.1045 - val_loss: 0.1629 - val_accuracy: 0.9541 - val_tp: 265102.0000 - val_fp: 11870.0000 - val_tn: 1102666.0000 - val_fn: 13532.0000 - val_categorical_accuracy: 0.9541 - val_precision: 0.9571 - val_recall: 0.9514 - val_auc: 0.9902 - val_categorical_crossentropy: 0.1288 - 29s/epoch - 41ms/step\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16102\n",
      "707/707 - 30s - loss: 0.1750 - accuracy: 0.9628 - tp: 1466561.0000 - fp: 52598.0000 - tn: 6055858.0000 - fn: 60553.0000 - categorical_accuracy: 0.9628 - precision: 0.9654 - recall: 0.9603 - auc: 0.9933 - categorical_crossentropy: 0.1039 - val_loss: 0.1612 - val_accuracy: 0.9546 - val_tp: 265248.0000 - val_fp: 11759.0000 - val_tn: 1102777.0000 - val_fn: 13386.0000 - val_categorical_accuracy: 0.9546 - val_precision: 0.9575 - val_recall: 0.9520 - val_auc: 0.9904 - val_categorical_crossentropy: 0.1273 - 30s/epoch - 42ms/step\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 39: val_loss improved from 0.16102 to 0.16055, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9933763742446899-0.962901--val-0.9904805421829224-0.954729.h5\n",
      "707/707 - 30s - loss: 0.1740 - accuracy: 0.9629 - tp: 1466737.0000 - fp: 52397.0000 - tn: 6056059.0000 - fn: 60377.0000 - categorical_accuracy: 0.9629 - precision: 0.9655 - recall: 0.9605 - auc: 0.9934 - categorical_crossentropy: 0.1033 - val_loss: 0.1606 - val_accuracy: 0.9547 - val_tp: 265296.0000 - val_fp: 11723.0000 - val_tn: 1102813.0000 - val_fn: 13338.0000 - val_categorical_accuracy: 0.9547 - val_precision: 0.9577 - val_recall: 0.9521 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1269 - 30s/epoch - 42ms/step\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16055\n",
      "707/707 - 30s - loss: 0.1730 - accuracy: 0.9630 - tp: 1467165.0000 - fp: 52277.0000 - tn: 6056179.0000 - fn: 59949.0000 - categorical_accuracy: 0.9630 - precision: 0.9656 - recall: 0.9607 - auc: 0.9935 - categorical_crossentropy: 0.1028 - val_loss: 0.1607 - val_accuracy: 0.9547 - val_tp: 265282.0000 - val_fp: 11729.0000 - val_tn: 1102807.0000 - val_fn: 13352.0000 - val_categorical_accuracy: 0.9547 - val_precision: 0.9577 - val_recall: 0.9521 - val_auc: 0.9904 - val_categorical_crossentropy: 0.1272 - 30s/epoch - 42ms/step\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 41: val_loss improved from 0.16055 to 0.15931, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9935097694396973-0.963235--val-0.9905308485031128-0.955095.h5\n",
      "707/707 - 29s - loss: 0.1719 - accuracy: 0.9632 - tp: 1467448.0000 - fp: 51949.0000 - tn: 6056507.0000 - fn: 59666.0000 - categorical_accuracy: 0.9632 - precision: 0.9658 - recall: 0.9609 - auc: 0.9935 - categorical_crossentropy: 0.1021 - val_loss: 0.1593 - val_accuracy: 0.9551 - val_tp: 265419.0000 - val_fp: 11615.0000 - val_tn: 1102921.0000 - val_fn: 13215.0000 - val_categorical_accuracy: 0.9551 - val_precision: 0.9581 - val_recall: 0.9526 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1261 - 29s/epoch - 41ms/step\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 42: val_loss improved from 0.15931 to 0.15909, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9935938119888306-0.963388--val-0.9905188083648682-0.954955.h5\n",
      "707/707 - 30s - loss: 0.1709 - accuracy: 0.9634 - tp: 1467679.0000 - fp: 51709.0000 - tn: 6056747.0000 - fn: 59435.0000 - categorical_accuracy: 0.9634 - precision: 0.9660 - recall: 0.9611 - auc: 0.9936 - categorical_crossentropy: 0.1015 - val_loss: 0.1591 - val_accuracy: 0.9550 - val_tp: 265381.0000 - val_fp: 11626.0000 - val_tn: 1102910.0000 - val_fn: 13253.0000 - val_categorical_accuracy: 0.9550 - val_precision: 0.9580 - val_recall: 0.9524 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1261 - 30s/epoch - 43ms/step\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 43: val_loss improved from 0.15909 to 0.15838, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9936420321464539-0.963553--val-0.9906232953071594-0.955250.h5\n",
      "707/707 - 30s - loss: 0.1701 - accuracy: 0.9636 - tp: 1468015.0000 - fp: 51537.0000 - tn: 6056919.0000 - fn: 59099.0000 - categorical_accuracy: 0.9636 - precision: 0.9661 - recall: 0.9613 - auc: 0.9936 - categorical_crossentropy: 0.1012 - val_loss: 0.1584 - val_accuracy: 0.9552 - val_tp: 265476.0000 - val_fp: 11579.0000 - val_tn: 1102957.0000 - val_fn: 13158.0000 - val_categorical_accuracy: 0.9552 - val_precision: 0.9582 - val_recall: 0.9528 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1255 - 30s/epoch - 43ms/step\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.15838\n",
      "707/707 - 28s - loss: 0.1692 - accuracy: 0.9638 - tp: 1468430.0000 - fp: 51280.0000 - tn: 6057176.0000 - fn: 58684.0000 - categorical_accuracy: 0.9638 - precision: 0.9663 - recall: 0.9616 - auc: 0.9937 - categorical_crossentropy: 0.1006 - val_loss: 0.1584 - val_accuracy: 0.9552 - val_tp: 265464.0000 - val_fp: 11603.0000 - val_tn: 1102933.0000 - val_fn: 13170.0000 - val_categorical_accuracy: 0.9552 - val_precision: 0.9581 - val_recall: 0.9527 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1257 - 28s/epoch - 40ms/step\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 45: val_loss improved from 0.15838 to 0.15809, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.993775486946106-0.963867--val-0.9906048774719238-0.955354.h5\n",
      "707/707 - 29s - loss: 0.1684 - accuracy: 0.9639 - tp: 1468430.0000 - fp: 51187.0000 - tn: 6057269.0000 - fn: 58684.0000 - categorical_accuracy: 0.9639 - precision: 0.9663 - recall: 0.9616 - auc: 0.9938 - categorical_crossentropy: 0.1002 - val_loss: 0.1581 - val_accuracy: 0.9554 - val_tp: 265502.0000 - val_fp: 11586.0000 - val_tn: 1102950.0000 - val_fn: 13132.0000 - val_categorical_accuracy: 0.9554 - val_precision: 0.9582 - val_recall: 0.9529 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1255 - 29s/epoch - 41ms/step\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 46: val_loss improved from 0.15809 to 0.15782, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9937930703163147-0.964065--val-0.9905959963798523-0.955303.h5\n",
      "707/707 - 28s - loss: 0.1677 - accuracy: 0.9641 - tp: 1468811.0000 - fp: 50951.0000 - tn: 6057505.0000 - fn: 58303.0000 - categorical_accuracy: 0.9641 - precision: 0.9665 - recall: 0.9618 - auc: 0.9938 - categorical_crossentropy: 0.0997 - val_loss: 0.1578 - val_accuracy: 0.9553 - val_tp: 265486.0000 - val_fp: 11597.0000 - val_tn: 1102939.0000 - val_fn: 13148.0000 - val_categorical_accuracy: 0.9553 - val_precision: 0.9581 - val_recall: 0.9528 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1255 - 28s/epoch - 39ms/step\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 47: val_loss improved from 0.15782 to 0.15753, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9939174652099609-0.964148--val-0.9905805587768555-0.955336.h5\n",
      "707/707 - 28s - loss: 0.1667 - accuracy: 0.9641 - tp: 1469046.0000 - fp: 50750.0000 - tn: 6057706.0000 - fn: 58068.0000 - categorical_accuracy: 0.9641 - precision: 0.9666 - recall: 0.9620 - auc: 0.9939 - categorical_crossentropy: 0.0991 - val_loss: 0.1575 - val_accuracy: 0.9553 - val_tp: 265507.0000 - val_fp: 11571.0000 - val_tn: 1102965.0000 - val_fn: 13127.0000 - val_categorical_accuracy: 0.9553 - val_precision: 0.9582 - val_recall: 0.9529 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1254 - 28s/epoch - 40ms/step\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.15753\n",
      "707/707 - 27s - loss: 0.1658 - accuracy: 0.9645 - tp: 1469488.0000 - fp: 50289.0000 - tn: 6058167.0000 - fn: 57626.0000 - categorical_accuracy: 0.9645 - precision: 0.9669 - recall: 0.9623 - auc: 0.9939 - categorical_crossentropy: 0.0986 - val_loss: 0.1580 - val_accuracy: 0.9552 - val_tp: 265477.0000 - val_fp: 11595.0000 - val_tn: 1102941.0000 - val_fn: 13157.0000 - val_categorical_accuracy: 0.9552 - val_precision: 0.9582 - val_recall: 0.9528 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1260 - 27s/epoch - 39ms/step\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.15753\n",
      "707/707 - 27s - loss: 0.1652 - accuracy: 0.9645 - tp: 1469458.0000 - fp: 50320.0000 - tn: 6058136.0000 - fn: 57656.0000 - categorical_accuracy: 0.9645 - precision: 0.9669 - recall: 0.9622 - auc: 0.9940 - categorical_crossentropy: 0.0982 - val_loss: 0.1577 - val_accuracy: 0.9553 - val_tp: 265499.0000 - val_fp: 11588.0000 - val_tn: 1102948.0000 - val_fn: 13135.0000 - val_categorical_accuracy: 0.9553 - val_precision: 0.9582 - val_recall: 0.9529 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1258 - 27s/epoch - 38ms/step\n",
      "Epoch 50/60\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      "Epoch 50: val_loss improved from 0.15753 to 0.15746, saving model to ./Checkpoints\\BiLSTM_size_200___weights-auc-0.9940172433853149-0.964594--val-0.9904949069023132-0.955217.h5\n",
      "707/707 - 28s - loss: 0.1644 - accuracy: 0.9646 - tp: 1469770.0000 - fp: 50222.0000 - tn: 6058234.0000 - fn: 57344.0000 - categorical_accuracy: 0.9646 - precision: 0.9670 - recall: 0.9624 - auc: 0.9940 - categorical_crossentropy: 0.0979 - val_loss: 0.1575 - val_accuracy: 0.9552 - val_tp: 265488.0000 - val_fp: 11598.0000 - val_tn: 1102938.0000 - val_fn: 13146.0000 - val_categorical_accuracy: 0.9552 - val_precision: 0.9581 - val_recall: 0.9528 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1258 - 28s/epoch - 39ms/step\n",
      "Epoch 50: early stopping\n",
      "BiLSTM_size_200__single-layer_FullFeats_win3_\n",
      "340/340 [==============================] - 8s 21ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.985964  0.968201  0.977002  421748.000000\n",
      "1              0.946782  0.969141  0.957831  268545.000000\n",
      "2              0.877756  0.901833  0.889632   43528.000000\n",
      "3              0.801527  0.937500  0.864198     112.000000\n",
      "4              0.708333  0.774403  0.739896     461.000000\n",
      "accuracy       0.964485  0.964485  0.964485       0.964485\n",
      "macro avg      0.864072  0.910216  0.885712  734394.000000\n",
      "weighted avg   0.965020  0.964485  0.964647  734394.000000\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Saline-KA Holdout Testing Dataset\n",
      "129/129 [==============================] - 3s 21ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.975412  0.966886  0.971130  132934.000000\n",
      "1              0.961718  0.950952  0.956305  125591.000000\n",
      "2              0.803942  0.906609  0.852195   20109.000000\n",
      "accuracy       0.955354  0.955354  0.955354       0.955354\n",
      "macro avg      0.913691  0.941482  0.926543  278634.000000\n",
      "weighted avg   0.956865  0.955354  0.955864  278634.000000\n",
      "unique ground truth labels: [0, 1, 2]\n",
      "unique predicted labels: [0, 1, 2]\n",
      "Saline Control Validation Dataset\n",
      "707/707 [==============================] - 16s 23ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.983728  0.969370  0.976496  7.920640e+05\n",
      "1              0.960403  0.964287  0.962341  6.297490e+05\n",
      "2              0.865117  0.937933  0.900055  1.036950e+05\n",
      "3              0.815029  0.933775  0.870370  3.020000e+02\n",
      "4              0.826649  0.980061  0.896842  1.304000e+03\n",
      "accuracy       0.965141  0.965141  0.965141  9.651414e-01\n",
      "macro avg      0.890185  0.957085  0.921221  1.527114e+06\n",
      "weighted avg   0.965888  0.965141  0.965380  1.527114e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training Dataset\n",
      "1176/1176 [==============================] - 27s 23ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.983600  0.968759  0.976123  1.346746e+06\n",
      "1              0.956931  0.963925  0.960415  1.023885e+06\n",
      "2              0.860545  0.924778  0.891506  1.673320e+05\n",
      "3              0.811321  0.934783  0.868687  4.140000e+02\n",
      "4              0.797561  0.926346  0.857143  1.765000e+03\n",
      "accuracy       0.963878  0.963878  0.963878  9.638780e-01\n",
      "macro avg      0.881992  0.943718  0.910775  2.540142e+06\n",
      "weighted avg   0.964587  0.963878  0.964117  2.540142e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training, Validation, and Testing Overall\n",
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_196 (Bidirect  (None, 7, 400)           481600    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dropout_234 (Dropout)       (None, 7, 400)            0         \n",
      "                                                                 \n",
      " bidirectional_197 (Bidirect  (None, 7, 200)           400800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dropout_235 (Dropout)       (None, 7, 200)            0         \n",
      "                                                                 \n",
      " bidirectional_198 (Bidirect  (None, 7, 100)           100400    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dropout_236 (Dropout)       (None, 7, 100)            0         \n",
      "                                                                 \n",
      " bidirectional_199 (Bidirect  (None, 7, 50)            25200     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dropout_237 (Dropout)       (None, 7, 50)             0         \n",
      "                                                                 \n",
      " flatten_156 (Flatten)       (None, 350)               0         \n",
      "                                                                 \n",
      " dense_195 (Dense)           (None, 5)                 1755      \n",
      "                                                                 \n",
      " activation_156 (Activation)  (None, 5)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,009,755\n",
      "Trainable params: 1,009,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.39718, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.85655677318573-0.804070--val-0.9605271816253662-0.912950.h5\n",
      "707/707 - 78s - loss: 1.2581 - accuracy: 0.8041 - tp: 648020.0000 - fp: 40157.0000 - tn: 6068299.0000 - fn: 879094.0000 - categorical_accuracy: 0.8041 - precision: 0.9416 - recall: 0.4243 - auc: 0.8566 - categorical_crossentropy: 0.9230 - val_loss: 0.3972 - val_accuracy: 0.9130 - val_tp: 239997.0000 - val_fp: 13298.0000 - val_tn: 1101238.0000 - val_fn: 38637.0000 - val_categorical_accuracy: 0.9130 - val_precision: 0.9475 - val_recall: 0.8613 - val_auc: 0.9605 - val_categorical_crossentropy: 0.3532 - 78s/epoch - 110ms/step\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 2: val_loss improved from 0.39718 to 0.30765, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9489096403121948-0.899614--val-0.9689831733703613-0.914752.h5\n",
      "707/707 - 59s - loss: 0.5639 - accuracy: 0.8996 - tp: 1316047.0000 - fp: 104841.0000 - tn: 6003615.0000 - fn: 211067.0000 - categorical_accuracy: 0.8996 - precision: 0.9262 - recall: 0.8618 - auc: 0.9489 - categorical_crossentropy: 0.3645 - val_loss: 0.3076 - val_accuracy: 0.9148 - val_tp: 252368.0000 - val_fp: 20589.0000 - val_tn: 1093947.0000 - val_fn: 26266.0000 - val_categorical_accuracy: 0.9148 - val_precision: 0.9246 - val_recall: 0.9057 - val_auc: 0.9690 - val_categorical_crossentropy: 0.2647 - 59s/epoch - 84ms/step\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 3: val_loss improved from 0.30765 to 0.27200, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9638084173202515-0.911804--val-0.975104570388794-0.920702.h5\n",
      "707/707 - 58s - loss: 0.4345 - accuracy: 0.9118 - tp: 1370007.0000 - fp: 113543.0000 - tn: 5994913.0000 - fn: 157107.0000 - categorical_accuracy: 0.9118 - precision: 0.9235 - recall: 0.8971 - auc: 0.9638 - categorical_crossentropy: 0.2785 - val_loss: 0.2720 - val_accuracy: 0.9207 - val_tp: 254954.0000 - val_fp: 20064.0000 - val_tn: 1094472.0000 - val_fn: 23680.0000 - val_categorical_accuracy: 0.9207 - val_precision: 0.9270 - val_recall: 0.9150 - val_auc: 0.9751 - val_categorical_crossentropy: 0.2310 - 58s/epoch - 83ms/step\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 4: val_loss improved from 0.27200 to 0.24887, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.971839964389801-0.920949--val-0.9790976047515869-0.927116.h5\n",
      "707/707 - 59s - loss: 0.3723 - accuracy: 0.9209 - tp: 1392220.0000 - fp: 105763.0000 - tn: 6002693.0000 - fn: 134894.0000 - categorical_accuracy: 0.9209 - precision: 0.9294 - recall: 0.9117 - auc: 0.9718 - categorical_crossentropy: 0.2378 - val_loss: 0.2489 - val_accuracy: 0.9271 - val_tp: 257020.0000 - val_fp: 18669.0000 - val_tn: 1095867.0000 - val_fn: 21614.0000 - val_categorical_accuracy: 0.9271 - val_precision: 0.9323 - val_recall: 0.9224 - val_auc: 0.9791 - val_categorical_crossentropy: 0.2087 - 59s/epoch - 84ms/step\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 5: val_loss improved from 0.24887 to 0.23089, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9766844511032104-0.927408--val-0.9818243980407715-0.932122.h5\n",
      "707/707 - 60s - loss: 0.3348 - accuracy: 0.9274 - tp: 1404903.0000 - fp: 98208.0000 - tn: 6010248.0000 - fn: 122211.0000 - categorical_accuracy: 0.9274 - precision: 0.9347 - recall: 0.9200 - auc: 0.9767 - categorical_crossentropy: 0.2129 - val_loss: 0.2309 - val_accuracy: 0.9321 - val_tp: 258531.0000 - val_fp: 17375.0000 - val_tn: 1097161.0000 - val_fn: 20103.0000 - val_categorical_accuracy: 0.9321 - val_precision: 0.9370 - val_recall: 0.9279 - val_auc: 0.9818 - val_categorical_crossentropy: 0.1912 - 60s/epoch - 85ms/step\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 6: val_loss improved from 0.23089 to 0.22073, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9797120690345764-0.931643--val-0.9834297299385071-0.935503.h5\n",
      "707/707 - 61s - loss: 0.3116 - accuracy: 0.9316 - tp: 1412344.0000 - fp: 92651.0000 - tn: 6015805.0000 - fn: 114770.0000 - categorical_accuracy: 0.9316 - precision: 0.9384 - recall: 0.9248 - auc: 0.9797 - categorical_crossentropy: 0.1970 - val_loss: 0.2207 - val_accuracy: 0.9355 - val_tp: 259560.0000 - val_fp: 16510.0000 - val_tn: 1098026.0000 - val_fn: 19074.0000 - val_categorical_accuracy: 0.9355 - val_precision: 0.9402 - val_recall: 0.9315 - val_auc: 0.9834 - val_categorical_crossentropy: 0.1816 - 61s/epoch - 86ms/step\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 7: val_loss improved from 0.22073 to 0.21312, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9815722107887268-0.934757--val-0.9843980073928833-0.937879.h5\n",
      "707/707 - 62s - loss: 0.2960 - accuracy: 0.9348 - tp: 1417766.0000 - fp: 88933.0000 - tn: 6019523.0000 - fn: 109348.0000 - categorical_accuracy: 0.9348 - precision: 0.9410 - recall: 0.9284 - auc: 0.9816 - categorical_crossentropy: 0.1866 - val_loss: 0.2131 - val_accuracy: 0.9379 - val_tp: 260241.0000 - val_fp: 15909.0000 - val_tn: 1098627.0000 - val_fn: 18393.0000 - val_categorical_accuracy: 0.9379 - val_precision: 0.9424 - val_recall: 0.9340 - val_auc: 0.9844 - val_categorical_crossentropy: 0.1745 - 62s/epoch - 88ms/step\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 8: val_loss improved from 0.21312 to 0.20865, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9830104112625122-0.936977--val-0.9848552942276001-0.939340.h5\n",
      "707/707 - 61s - loss: 0.2841 - accuracy: 0.9370 - tp: 1422017.0000 - fp: 85918.0000 - tn: 6022538.0000 - fn: 105097.0000 - categorical_accuracy: 0.9370 - precision: 0.9430 - recall: 0.9312 - auc: 0.9830 - categorical_crossentropy: 0.1785 - val_loss: 0.2086 - val_accuracy: 0.9393 - val_tp: 260710.0000 - val_fp: 15554.0000 - val_tn: 1098982.0000 - val_fn: 17924.0000 - val_categorical_accuracy: 0.9393 - val_precision: 0.9437 - val_recall: 0.9357 - val_auc: 0.9849 - val_categorical_crossentropy: 0.1706 - 61s/epoch - 86ms/step\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 9: val_loss improved from 0.20865 to 0.20356, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9839591383934021-0.938719--val-0.9852918386459351-0.940851.h5\n",
      "707/707 - 61s - loss: 0.2749 - accuracy: 0.9387 - tp: 1425299.0000 - fp: 83825.0000 - tn: 6024631.0000 - fn: 101815.0000 - categorical_accuracy: 0.9387 - precision: 0.9445 - recall: 0.9333 - auc: 0.9840 - categorical_crossentropy: 0.1727 - val_loss: 0.2036 - val_accuracy: 0.9409 - val_tp: 261110.0000 - val_fp: 15140.0000 - val_tn: 1099396.0000 - val_fn: 17524.0000 - val_categorical_accuracy: 0.9409 - val_precision: 0.9452 - val_recall: 0.9371 - val_auc: 0.9853 - val_categorical_crossentropy: 0.1660 - 61s/epoch - 86ms/step\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 10: val_loss improved from 0.20356 to 0.19987, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9848219752311707-0.940283--val-0.9857233166694641-0.942103.h5\n",
      "707/707 - 63s - loss: 0.2668 - accuracy: 0.9403 - tp: 1428377.0000 - fp: 81961.0000 - tn: 6026495.0000 - fn: 98737.0000 - categorical_accuracy: 0.9403 - precision: 0.9457 - recall: 0.9353 - auc: 0.9848 - categorical_crossentropy: 0.1674 - val_loss: 0.1999 - val_accuracy: 0.9421 - val_tp: 261549.0000 - val_fp: 14888.0000 - val_tn: 1099648.0000 - val_fn: 17085.0000 - val_categorical_accuracy: 0.9421 - val_precision: 0.9461 - val_recall: 0.9387 - val_auc: 0.9857 - val_categorical_crossentropy: 0.1627 - 63s/epoch - 89ms/step\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 11: val_loss improved from 0.19987 to 0.19854, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9855560064315796-0.941469--val-0.985797643661499-0.942326.h5\n",
      "707/707 - 60s - loss: 0.2601 - accuracy: 0.9415 - tp: 1430355.0000 - fp: 80454.0000 - tn: 6028002.0000 - fn: 96759.0000 - categorical_accuracy: 0.9415 - precision: 0.9467 - recall: 0.9366 - auc: 0.9856 - categorical_crossentropy: 0.1629 - val_loss: 0.1985 - val_accuracy: 0.9423 - val_tp: 261613.0000 - val_fp: 14777.0000 - val_tn: 1099759.0000 - val_fn: 17021.0000 - val_categorical_accuracy: 0.9423 - val_precision: 0.9465 - val_recall: 0.9389 - val_auc: 0.9858 - val_categorical_crossentropy: 0.1617 - 60s/epoch - 85ms/step\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 12: val_loss improved from 0.19854 to 0.19540, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9861454963684082-0.942837--val-0.9861084222793579-0.943499.h5\n",
      "707/707 - 62s - loss: 0.2541 - accuracy: 0.9428 - tp: 1432826.0000 - fp: 78705.0000 - tn: 6029751.0000 - fn: 94288.0000 - categorical_accuracy: 0.9428 - precision: 0.9479 - recall: 0.9383 - auc: 0.9861 - categorical_crossentropy: 0.1591 - val_loss: 0.1954 - val_accuracy: 0.9435 - val_tp: 261963.0000 - val_fp: 14518.0000 - val_tn: 1100018.0000 - val_fn: 16671.0000 - val_categorical_accuracy: 0.9435 - val_precision: 0.9475 - val_recall: 0.9402 - val_auc: 0.9861 - val_categorical_crossentropy: 0.1589 - 62s/epoch - 88ms/step\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 13: val_loss improved from 0.19540 to 0.19347, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9866601228713989-0.943827--val-0.986265242099762-0.944124.h5\n",
      "707/707 - 54s - loss: 0.2487 - accuracy: 0.9438 - tp: 1434576.0000 - fp: 77628.0000 - tn: 6030828.0000 - fn: 92538.0000 - categorical_accuracy: 0.9438 - precision: 0.9487 - recall: 0.9394 - auc: 0.9867 - categorical_crossentropy: 0.1555 - val_loss: 0.1935 - val_accuracy: 0.9441 - val_tp: 262167.0000 - val_fp: 14385.0000 - val_tn: 1100151.0000 - val_fn: 16467.0000 - val_categorical_accuracy: 0.9441 - val_precision: 0.9480 - val_recall: 0.9409 - val_auc: 0.9863 - val_categorical_crossentropy: 0.1573 - 54s/epoch - 77ms/step\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 14: val_loss improved from 0.19347 to 0.19174, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9871029257774353-0.944794--val-0.9864222407341003-0.944533.h5\n",
      "707/707 - 60s - loss: 0.2442 - accuracy: 0.9448 - tp: 1436432.0000 - fp: 76245.0000 - tn: 6032211.0000 - fn: 90682.0000 - categorical_accuracy: 0.9448 - precision: 0.9496 - recall: 0.9406 - auc: 0.9871 - categorical_crossentropy: 0.1526 - val_loss: 0.1917 - val_accuracy: 0.9445 - val_tp: 262324.0000 - val_fp: 14255.0000 - val_tn: 1100281.0000 - val_fn: 16310.0000 - val_categorical_accuracy: 0.9445 - val_precision: 0.9485 - val_recall: 0.9415 - val_auc: 0.9864 - val_categorical_crossentropy: 0.1559 - 60s/epoch - 85ms/step\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 15: val_loss improved from 0.19174 to 0.19024, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9875161647796631-0.945679--val-0.986524224281311-0.945114.h5\n",
      "707/707 - 61s - loss: 0.2398 - accuracy: 0.9457 - tp: 1437779.0000 - fp: 75168.0000 - tn: 6033288.0000 - fn: 89335.0000 - categorical_accuracy: 0.9457 - precision: 0.9503 - recall: 0.9415 - auc: 0.9875 - categorical_crossentropy: 0.1497 - val_loss: 0.1902 - val_accuracy: 0.9451 - val_tp: 262504.0000 - val_fp: 14146.0000 - val_tn: 1100390.0000 - val_fn: 16130.0000 - val_categorical_accuracy: 0.9451 - val_precision: 0.9489 - val_recall: 0.9421 - val_auc: 0.9865 - val_categorical_crossentropy: 0.1546 - 61s/epoch - 86ms/step\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 16: val_loss improved from 0.19024 to 0.18796, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9878075122833252-0.946464--val-0.9868249893188477-0.945681.h5\n",
      "707/707 - 61s - loss: 0.2363 - accuracy: 0.9465 - tp: 1439227.0000 - fp: 74240.0000 - tn: 6034216.0000 - fn: 87887.0000 - categorical_accuracy: 0.9465 - precision: 0.9509 - recall: 0.9424 - auc: 0.9878 - categorical_crossentropy: 0.1474 - val_loss: 0.1880 - val_accuracy: 0.9457 - val_tp: 262693.0000 - val_fp: 14036.0000 - val_tn: 1100500.0000 - val_fn: 15941.0000 - val_categorical_accuracy: 0.9457 - val_precision: 0.9493 - val_recall: 0.9428 - val_auc: 0.9868 - val_categorical_crossentropy: 0.1526 - 61s/epoch - 86ms/step\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 17: val_loss improved from 0.18796 to 0.18719, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.988102376461029-0.947277--val-0.9867459535598755-0.945965.h5\n",
      "707/707 - 61s - loss: 0.2327 - accuracy: 0.9473 - tp: 1440473.0000 - fp: 73184.0000 - tn: 6035272.0000 - fn: 86641.0000 - categorical_accuracy: 0.9473 - precision: 0.9517 - recall: 0.9433 - auc: 0.9881 - categorical_crossentropy: 0.1450 - val_loss: 0.1872 - val_accuracy: 0.9460 - val_tp: 262737.0000 - val_fp: 13943.0000 - val_tn: 1100593.0000 - val_fn: 15897.0000 - val_categorical_accuracy: 0.9460 - val_precision: 0.9496 - val_recall: 0.9429 - val_auc: 0.9867 - val_categorical_crossentropy: 0.1521 - 61s/epoch - 86ms/step\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.18719\n",
      "707/707 - 62s - loss: 0.2299 - accuracy: 0.9478 - tp: 1441687.0000 - fp: 72521.0000 - tn: 6035935.0000 - fn: 85427.0000 - categorical_accuracy: 0.9478 - precision: 0.9521 - recall: 0.9441 - auc: 0.9884 - categorical_crossentropy: 0.1431 - val_loss: 0.1873 - val_accuracy: 0.9458 - val_tp: 262711.0000 - val_fp: 13960.0000 - val_tn: 1100576.0000 - val_fn: 15923.0000 - val_categorical_accuracy: 0.9458 - val_precision: 0.9495 - val_recall: 0.9429 - val_auc: 0.9867 - val_categorical_crossentropy: 0.1525 - 62s/epoch - 88ms/step\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 19: val_loss improved from 0.18719 to 0.18413, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9887049198150635-0.948523--val-0.9870659708976746-0.946948.h5\n",
      "707/707 - 62s - loss: 0.2263 - accuracy: 0.9485 - tp: 1442836.0000 - fp: 71537.0000 - tn: 6036919.0000 - fn: 84278.0000 - categorical_accuracy: 0.9485 - precision: 0.9528 - recall: 0.9448 - auc: 0.9887 - categorical_crossentropy: 0.1409 - val_loss: 0.1841 - val_accuracy: 0.9469 - val_tp: 263032.0000 - val_fp: 13708.0000 - val_tn: 1100828.0000 - val_fn: 15602.0000 - val_categorical_accuracy: 0.9469 - val_precision: 0.9505 - val_recall: 0.9440 - val_auc: 0.9871 - val_categorical_crossentropy: 0.1495 - 62s/epoch - 88ms/step\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 20: val_loss improved from 0.18413 to 0.18328, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9889450073242188-0.949245--val-0.9871631860733032-0.947210.h5\n",
      "707/707 - 62s - loss: 0.2236 - accuracy: 0.9492 - tp: 1443942.0000 - fp: 70779.0000 - tn: 6037677.0000 - fn: 83172.0000 - categorical_accuracy: 0.9492 - precision: 0.9533 - recall: 0.9455 - auc: 0.9889 - categorical_crossentropy: 0.1390 - val_loss: 0.1833 - val_accuracy: 0.9472 - val_tp: 263119.0000 - val_fp: 13672.0000 - val_tn: 1100864.0000 - val_fn: 15515.0000 - val_categorical_accuracy: 0.9472 - val_precision: 0.9506 - val_recall: 0.9443 - val_auc: 0.9872 - val_categorical_crossentropy: 0.1489 - 62s/epoch - 88ms/step\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 21: val_loss improved from 0.18328 to 0.18055, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9891177415847778-0.949828--val-0.9874197244644165-0.947950.h5\n",
      "707/707 - 61s - loss: 0.2206 - accuracy: 0.9498 - tp: 1445084.0000 - fp: 69944.0000 - tn: 6038512.0000 - fn: 82030.0000 - categorical_accuracy: 0.9498 - precision: 0.9538 - recall: 0.9463 - auc: 0.9891 - categorical_crossentropy: 0.1372 - val_loss: 0.1805 - val_accuracy: 0.9479 - val_tp: 263341.0000 - val_fp: 13422.0000 - val_tn: 1101114.0000 - val_fn: 15293.0000 - val_categorical_accuracy: 0.9479 - val_precision: 0.9515 - val_recall: 0.9451 - val_auc: 0.9874 - val_categorical_crossentropy: 0.1464 - 61s/epoch - 86ms/step\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.18055\n",
      "707/707 - 61s - loss: 0.2185 - accuracy: 0.9505 - tp: 1446192.0000 - fp: 69013.0000 - tn: 6039443.0000 - fn: 80922.0000 - categorical_accuracy: 0.9505 - precision: 0.9545 - recall: 0.9470 - auc: 0.9894 - categorical_crossentropy: 0.1357 - val_loss: 0.1811 - val_accuracy: 0.9480 - val_tp: 263375.0000 - val_fp: 13491.0000 - val_tn: 1101045.0000 - val_fn: 15259.0000 - val_categorical_accuracy: 0.9480 - val_precision: 0.9513 - val_recall: 0.9452 - val_auc: 0.9873 - val_categorical_crossentropy: 0.1471 - 61s/epoch - 86ms/step\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 23: val_loss improved from 0.18055 to 0.17878, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9896044731140137-0.950987--val-0.9875986576080322-0.948542.h5\n",
      "707/707 - 61s - loss: 0.2159 - accuracy: 0.9510 - tp: 1447005.0000 - fp: 68506.0000 - tn: 6039950.0000 - fn: 80109.0000 - categorical_accuracy: 0.9510 - precision: 0.9548 - recall: 0.9475 - auc: 0.9896 - categorical_crossentropy: 0.1340 - val_loss: 0.1788 - val_accuracy: 0.9485 - val_tp: 263509.0000 - val_fp: 13293.0000 - val_tn: 1101243.0000 - val_fn: 15125.0000 - val_categorical_accuracy: 0.9485 - val_precision: 0.9520 - val_recall: 0.9457 - val_auc: 0.9876 - val_categorical_crossentropy: 0.1450 - 61s/epoch - 86ms/step\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 24: val_loss improved from 0.17878 to 0.17852, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9897596836090088-0.951552--val-0.9875450134277344-0.948649.h5\n",
      "707/707 - 59s - loss: 0.2138 - accuracy: 0.9516 - tp: 1447949.0000 - fp: 67632.0000 - tn: 6040824.0000 - fn: 79165.0000 - categorical_accuracy: 0.9516 - precision: 0.9554 - recall: 0.9482 - auc: 0.9898 - categorical_crossentropy: 0.1327 - val_loss: 0.1785 - val_accuracy: 0.9486 - val_tp: 263559.0000 - val_fp: 13277.0000 - val_tn: 1101259.0000 - val_fn: 15075.0000 - val_categorical_accuracy: 0.9486 - val_precision: 0.9520 - val_recall: 0.9459 - val_auc: 0.9875 - val_categorical_crossentropy: 0.1449 - 59s/epoch - 83ms/step\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.17852\n",
      "707/707 - 59s - loss: 0.2119 - accuracy: 0.9518 - tp: 1448483.0000 - fp: 67281.0000 - tn: 6041175.0000 - fn: 78631.0000 - categorical_accuracy: 0.9518 - precision: 0.9556 - recall: 0.9485 - auc: 0.9900 - categorical_crossentropy: 0.1314 - val_loss: 0.1789 - val_accuracy: 0.9486 - val_tp: 263560.0000 - val_fp: 13316.0000 - val_tn: 1101220.0000 - val_fn: 15074.0000 - val_categorical_accuracy: 0.9486 - val_precision: 0.9519 - val_recall: 0.9459 - val_auc: 0.9874 - val_categorical_crossentropy: 0.1455 - 59s/epoch - 83ms/step\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 26: val_loss improved from 0.17852 to 0.17694, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9901027083396912-0.952246--val-0.9876141548156738-0.949371.h5\n",
      "707/707 - 59s - loss: 0.2096 - accuracy: 0.9522 - tp: 1449251.0000 - fp: 66819.0000 - tn: 6041637.0000 - fn: 77863.0000 - categorical_accuracy: 0.9522 - precision: 0.9559 - recall: 0.9490 - auc: 0.9901 - categorical_crossentropy: 0.1299 - val_loss: 0.1769 - val_accuracy: 0.9494 - val_tp: 263821.0000 - val_fp: 13146.0000 - val_tn: 1101390.0000 - val_fn: 14813.0000 - val_categorical_accuracy: 0.9494 - val_precision: 0.9525 - val_recall: 0.9468 - val_auc: 0.9876 - val_categorical_crossentropy: 0.1437 - 59s/epoch - 83ms/step\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 27: val_loss improved from 0.17694 to 0.17677, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9903297424316406-0.952898--val-0.9876421689987183-0.949418.h5\n",
      "707/707 - 59s - loss: 0.2074 - accuracy: 0.9529 - tp: 1450261.0000 - fp: 65906.0000 - tn: 6042550.0000 - fn: 76853.0000 - categorical_accuracy: 0.9529 - precision: 0.9565 - recall: 0.9497 - auc: 0.9903 - categorical_crossentropy: 0.1285 - val_loss: 0.1768 - val_accuracy: 0.9494 - val_tp: 263880.0000 - val_fp: 13167.0000 - val_tn: 1101369.0000 - val_fn: 14754.0000 - val_categorical_accuracy: 0.9494 - val_precision: 0.9525 - val_recall: 0.9470 - val_auc: 0.9876 - val_categorical_crossentropy: 0.1437 - 59s/epoch - 84ms/step\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 28: val_loss improved from 0.17677 to 0.17664, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.99042809009552-0.953152--val-0.987633228302002-0.949202.h5\n",
      "707/707 - 61s - loss: 0.2062 - accuracy: 0.9532 - tp: 1450740.0000 - fp: 65603.0000 - tn: 6042853.0000 - fn: 76374.0000 - categorical_accuracy: 0.9532 - precision: 0.9567 - recall: 0.9500 - auc: 0.9904 - categorical_crossentropy: 0.1276 - val_loss: 0.1766 - val_accuracy: 0.9492 - val_tp: 263794.0000 - val_fp: 13194.0000 - val_tn: 1101342.0000 - val_fn: 14840.0000 - val_categorical_accuracy: 0.9492 - val_precision: 0.9524 - val_recall: 0.9467 - val_auc: 0.9876 - val_categorical_crossentropy: 0.1438 - 61s/epoch - 87ms/step\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 29: val_loss improved from 0.17664 to 0.17611, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9906183481216431-0.953881--val-0.9875785112380981-0.949697.h5\n",
      "707/707 - 52s - loss: 0.2040 - accuracy: 0.9539 - tp: 1451938.0000 - fp: 64691.0000 - tn: 6043765.0000 - fn: 75176.0000 - categorical_accuracy: 0.9539 - precision: 0.9573 - recall: 0.9508 - auc: 0.9906 - categorical_crossentropy: 0.1263 - val_loss: 0.1761 - val_accuracy: 0.9497 - val_tp: 263926.0000 - val_fp: 13130.0000 - val_tn: 1101406.0000 - val_fn: 14708.0000 - val_categorical_accuracy: 0.9497 - val_precision: 0.9526 - val_recall: 0.9472 - val_auc: 0.9876 - val_categorical_crossentropy: 0.1434 - 52s/epoch - 73ms/step\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 30: val_loss improved from 0.17611 to 0.17457, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9907423257827759-0.954009--val-0.9878141283988953-0.950218.h5\n",
      "707/707 - 60s - loss: 0.2023 - accuracy: 0.9540 - tp: 1452159.0000 - fp: 64572.0000 - tn: 6043884.0000 - fn: 74955.0000 - categorical_accuracy: 0.9540 - precision: 0.9574 - recall: 0.9509 - auc: 0.9907 - categorical_crossentropy: 0.1252 - val_loss: 0.1746 - val_accuracy: 0.9502 - val_tp: 264083.0000 - val_fp: 12989.0000 - val_tn: 1101547.0000 - val_fn: 14551.0000 - val_categorical_accuracy: 0.9502 - val_precision: 0.9531 - val_recall: 0.9478 - val_auc: 0.9878 - val_categorical_crossentropy: 0.1420 - 60s/epoch - 84ms/step\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 31: val_loss improved from 0.17457 to 0.17427, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9908756017684937-0.954512--val-0.9877708554267883-0.950175.h5\n",
      "707/707 - 60s - loss: 0.2007 - accuracy: 0.9545 - tp: 1452932.0000 - fp: 63945.0000 - tn: 6044511.0000 - fn: 74182.0000 - categorical_accuracy: 0.9545 - precision: 0.9578 - recall: 0.9514 - auc: 0.9909 - categorical_crossentropy: 0.1241 - val_loss: 0.1743 - val_accuracy: 0.9502 - val_tp: 264076.0000 - val_fp: 12983.0000 - val_tn: 1101553.0000 - val_fn: 14558.0000 - val_categorical_accuracy: 0.9502 - val_precision: 0.9531 - val_recall: 0.9478 - val_auc: 0.9878 - val_categorical_crossentropy: 0.1419 - 60s/epoch - 85ms/step\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.17427\n",
      "707/707 - 59s - loss: 0.1990 - accuracy: 0.9549 - tp: 1453654.0000 - fp: 63452.0000 - tn: 6045004.0000 - fn: 73460.0000 - categorical_accuracy: 0.9549 - precision: 0.9582 - recall: 0.9519 - auc: 0.9910 - categorical_crossentropy: 0.1230 - val_loss: 0.1746 - val_accuracy: 0.9500 - val_tp: 263987.0000 - val_fp: 13039.0000 - val_tn: 1101497.0000 - val_fn: 14647.0000 - val_categorical_accuracy: 0.9500 - val_precision: 0.9529 - val_recall: 0.9474 - val_auc: 0.9877 - val_categorical_crossentropy: 0.1423 - 59s/epoch - 84ms/step\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.17427\n",
      "707/707 - 60s - loss: 0.1975 - accuracy: 0.9552 - tp: 1454143.0000 - fp: 62995.0000 - tn: 6045461.0000 - fn: 72971.0000 - categorical_accuracy: 0.9552 - precision: 0.9585 - recall: 0.9522 - auc: 0.9911 - categorical_crossentropy: 0.1220 - val_loss: 0.1753 - val_accuracy: 0.9498 - val_tp: 263976.0000 - val_fp: 13091.0000 - val_tn: 1101445.0000 - val_fn: 14658.0000 - val_categorical_accuracy: 0.9498 - val_precision: 0.9528 - val_recall: 0.9474 - val_auc: 0.9876 - val_categorical_crossentropy: 0.1432 - 60s/epoch - 85ms/step\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 34: val_loss improved from 0.17427 to 0.17333, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9912222027778625-0.955308--val-0.9878520965576172-0.950379.h5\n",
      "707/707 - 60s - loss: 0.1963 - accuracy: 0.9553 - tp: 1454554.0000 - fp: 62812.0000 - tn: 6045644.0000 - fn: 72560.0000 - categorical_accuracy: 0.9553 - precision: 0.9586 - recall: 0.9525 - auc: 0.9912 - categorical_crossentropy: 0.1212 - val_loss: 0.1733 - val_accuracy: 0.9504 - val_tp: 264120.0000 - val_fp: 12927.0000 - val_tn: 1101609.0000 - val_fn: 14514.0000 - val_categorical_accuracy: 0.9504 - val_precision: 0.9533 - val_recall: 0.9479 - val_auc: 0.9879 - val_categorical_crossentropy: 0.1414 - 60s/epoch - 85ms/step\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 35: val_loss improved from 0.17333 to 0.17213, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9913002848625183-0.955671--val-0.9879204034805298-0.951033.h5\n",
      "707/707 - 61s - loss: 0.1950 - accuracy: 0.9557 - tp: 1455098.0000 - fp: 62490.0000 - tn: 6045966.0000 - fn: 72016.0000 - categorical_accuracy: 0.9557 - precision: 0.9588 - recall: 0.9528 - auc: 0.9913 - categorical_crossentropy: 0.1204 - val_loss: 0.1721 - val_accuracy: 0.9510 - val_tp: 264367.0000 - val_fp: 12789.0000 - val_tn: 1101747.0000 - val_fn: 14267.0000 - val_categorical_accuracy: 0.9510 - val_precision: 0.9539 - val_recall: 0.9488 - val_auc: 0.9879 - val_categorical_crossentropy: 0.1403 - 61s/epoch - 86ms/step\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 36: val_loss improved from 0.17213 to 0.17121, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9914777874946594-0.956088--val-0.9880255460739136-0.951262.h5\n",
      "707/707 - 61s - loss: 0.1933 - accuracy: 0.9561 - tp: 1455829.0000 - fp: 61871.0000 - tn: 6046585.0000 - fn: 71285.0000 - categorical_accuracy: 0.9561 - precision: 0.9592 - recall: 0.9533 - auc: 0.9915 - categorical_crossentropy: 0.1194 - val_loss: 0.1712 - val_accuracy: 0.9513 - val_tp: 264361.0000 - val_fp: 12736.0000 - val_tn: 1101800.0000 - val_fn: 14273.0000 - val_categorical_accuracy: 0.9513 - val_precision: 0.9540 - val_recall: 0.9488 - val_auc: 0.9880 - val_categorical_crossentropy: 0.1395 - 61s/epoch - 86ms/step\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.17121\n",
      "707/707 - 60s - loss: 0.1917 - accuracy: 0.9564 - tp: 1456284.0000 - fp: 61526.0000 - tn: 6046930.0000 - fn: 70830.0000 - categorical_accuracy: 0.9564 - precision: 0.9595 - recall: 0.9536 - auc: 0.9916 - categorical_crossentropy: 0.1183 - val_loss: 0.1733 - val_accuracy: 0.9505 - val_tp: 264208.0000 - val_fp: 12958.0000 - val_tn: 1101578.0000 - val_fn: 14426.0000 - val_categorical_accuracy: 0.9505 - val_precision: 0.9532 - val_recall: 0.9482 - val_auc: 0.9877 - val_categorical_crossentropy: 0.1418 - 60s/epoch - 85ms/step\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 38: val_loss improved from 0.17121 to 0.17074, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9916238784790039-0.956745--val-0.9879992008209229-0.951406.h5\n",
      "707/707 - 59s - loss: 0.1909 - accuracy: 0.9567 - tp: 1456929.0000 - fp: 60977.0000 - tn: 6047479.0000 - fn: 70185.0000 - categorical_accuracy: 0.9567 - precision: 0.9598 - recall: 0.9540 - auc: 0.9916 - categorical_crossentropy: 0.1178 - val_loss: 0.1707 - val_accuracy: 0.9514 - val_tp: 264446.0000 - val_fp: 12706.0000 - val_tn: 1101830.0000 - val_fn: 14188.0000 - val_categorical_accuracy: 0.9514 - val_precision: 0.9542 - val_recall: 0.9491 - val_auc: 0.9880 - val_categorical_crossentropy: 0.1394 - 59s/epoch - 83ms/step\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 39: val_loss improved from 0.17074 to 0.17059, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9917557239532471-0.956942--val-0.987987756729126-0.951499.h5\n",
      "707/707 - 58s - loss: 0.1893 - accuracy: 0.9569 - tp: 1457176.0000 - fp: 60702.0000 - tn: 6047754.0000 - fn: 69938.0000 - categorical_accuracy: 0.9569 - precision: 0.9600 - recall: 0.9542 - auc: 0.9918 - categorical_crossentropy: 0.1168 - val_loss: 0.1706 - val_accuracy: 0.9515 - val_tp: 264500.0000 - val_fp: 12713.0000 - val_tn: 1101823.0000 - val_fn: 14134.0000 - val_categorical_accuracy: 0.9515 - val_precision: 0.9541 - val_recall: 0.9493 - val_auc: 0.9880 - val_categorical_crossentropy: 0.1393 - 58s/epoch - 82ms/step\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 40: val_loss improved from 0.17059 to 0.16998, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9918561577796936-0.957285--val-0.9880532026290894-0.951668.h5\n",
      "707/707 - 60s - loss: 0.1881 - accuracy: 0.9573 - tp: 1457803.0000 - fp: 60297.0000 - tn: 6048159.0000 - fn: 69311.0000 - categorical_accuracy: 0.9573 - precision: 0.9603 - recall: 0.9546 - auc: 0.9919 - categorical_crossentropy: 0.1160 - val_loss: 0.1700 - val_accuracy: 0.9517 - val_tp: 264542.0000 - val_fp: 12674.0000 - val_tn: 1101862.0000 - val_fn: 14092.0000 - val_categorical_accuracy: 0.9517 - val_precision: 0.9543 - val_recall: 0.9494 - val_auc: 0.9881 - val_categorical_crossentropy: 0.1389 - 60s/epoch - 84ms/step\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 41: val_loss improved from 0.16998 to 0.16912, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9919817447662354-0.957478--val-0.9881108999252319-0.951901.h5\n",
      "707/707 - 60s - loss: 0.1871 - accuracy: 0.9575 - tp: 1458217.0000 - fp: 60026.0000 - tn: 6048430.0000 - fn: 68897.0000 - categorical_accuracy: 0.9575 - precision: 0.9605 - recall: 0.9549 - auc: 0.9920 - categorical_crossentropy: 0.1153 - val_loss: 0.1691 - val_accuracy: 0.9519 - val_tp: 264647.0000 - val_fp: 12605.0000 - val_tn: 1101931.0000 - val_fn: 13987.0000 - val_categorical_accuracy: 0.9519 - val_precision: 0.9545 - val_recall: 0.9498 - val_auc: 0.9881 - val_categorical_crossentropy: 0.1381 - 60s/epoch - 85ms/step\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 42: val_loss improved from 0.16912 to 0.16810, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9920556545257568-0.957669--val-0.9883424639701843-0.951883.h5\n",
      "707/707 - 61s - loss: 0.1858 - accuracy: 0.9577 - tp: 1458521.0000 - fp: 59850.0000 - tn: 6048606.0000 - fn: 68593.0000 - categorical_accuracy: 0.9577 - precision: 0.9606 - recall: 0.9551 - auc: 0.9921 - categorical_crossentropy: 0.1145 - val_loss: 0.1681 - val_accuracy: 0.9519 - val_tp: 264613.0000 - val_fp: 12577.0000 - val_tn: 1101959.0000 - val_fn: 14021.0000 - val_categorical_accuracy: 0.9519 - val_precision: 0.9546 - val_recall: 0.9497 - val_auc: 0.9883 - val_categorical_crossentropy: 0.1373 - 61s/epoch - 86ms/step\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16810\n",
      "707/707 - 59s - loss: 0.1848 - accuracy: 0.9580 - tp: 1459024.0000 - fp: 59376.0000 - tn: 6049080.0000 - fn: 68090.0000 - categorical_accuracy: 0.9580 - precision: 0.9609 - recall: 0.9554 - auc: 0.9921 - categorical_crossentropy: 0.1139 - val_loss: 0.1684 - val_accuracy: 0.9522 - val_tp: 264717.0000 - val_fp: 12570.0000 - val_tn: 1101966.0000 - val_fn: 13917.0000 - val_categorical_accuracy: 0.9522 - val_precision: 0.9547 - val_recall: 0.9501 - val_auc: 0.9882 - val_categorical_crossentropy: 0.1377 - 59s/epoch - 84ms/step\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 44: val_loss improved from 0.16810 to 0.16764, saving model to ./Checkpoints\\QBiLSTM_size_200___weights-auc-0.9921770691871643-0.958143--val-0.9882562160491943-0.952339.h5\n",
      "707/707 - 59s - loss: 0.1840 - accuracy: 0.9581 - tp: 1459295.0000 - fp: 59112.0000 - tn: 6049344.0000 - fn: 67819.0000 - categorical_accuracy: 0.9581 - precision: 0.9611 - recall: 0.9556 - auc: 0.9922 - categorical_crossentropy: 0.1134 - val_loss: 0.1676 - val_accuracy: 0.9523 - val_tp: 264795.0000 - val_fp: 12494.0000 - val_tn: 1102042.0000 - val_fn: 13839.0000 - val_categorical_accuracy: 0.9523 - val_precision: 0.9549 - val_recall: 0.9503 - val_auc: 0.9883 - val_categorical_crossentropy: 0.1370 - 59s/epoch - 84ms/step\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16764\n",
      "707/707 - 60s - loss: 0.1827 - accuracy: 0.9584 - tp: 1459803.0000 - fp: 58803.0000 - tn: 6049653.0000 - fn: 67311.0000 - categorical_accuracy: 0.9584 - precision: 0.9613 - recall: 0.9559 - auc: 0.9923 - categorical_crossentropy: 0.1126 - val_loss: 0.1682 - val_accuracy: 0.9524 - val_tp: 264792.0000 - val_fp: 12485.0000 - val_tn: 1102051.0000 - val_fn: 13842.0000 - val_categorical_accuracy: 0.9524 - val_precision: 0.9550 - val_recall: 0.9503 - val_auc: 0.9881 - val_categorical_crossentropy: 0.1377 - 60s/epoch - 85ms/step\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16764\n",
      "707/707 - 60s - loss: 0.1817 - accuracy: 0.9587 - tp: 1460298.0000 - fp: 58482.0000 - tn: 6049974.0000 - fn: 66816.0000 - categorical_accuracy: 0.9587 - precision: 0.9615 - recall: 0.9562 - auc: 0.9923 - categorical_crossentropy: 0.1119 - val_loss: 0.1681 - val_accuracy: 0.9522 - val_tp: 264765.0000 - val_fp: 12552.0000 - val_tn: 1101984.0000 - val_fn: 13869.0000 - val_categorical_accuracy: 0.9522 - val_precision: 0.9547 - val_recall: 0.9502 - val_auc: 0.9881 - val_categorical_crossentropy: 0.1378 - 60s/epoch - 85ms/step\n",
      "Epoch 47/60\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16764\n",
      "707/707 - 60s - loss: 0.1808 - accuracy: 0.9589 - tp: 1460514.0000 - fp: 58228.0000 - tn: 6050228.0000 - fn: 66600.0000 - categorical_accuracy: 0.9589 - precision: 0.9617 - recall: 0.9564 - auc: 0.9924 - categorical_crossentropy: 0.1114 - val_loss: 0.1677 - val_accuracy: 0.9524 - val_tp: 264856.0000 - val_fp: 12477.0000 - val_tn: 1102059.0000 - val_fn: 13778.0000 - val_categorical_accuracy: 0.9524 - val_precision: 0.9550 - val_recall: 0.9506 - val_auc: 0.9882 - val_categorical_crossentropy: 0.1375 - 60s/epoch - 85ms/step\n",
      "Epoch 47: early stopping\n",
      "QBiLSTM_size_200__quad-layer_FullFeats_win3_\n",
      "340/340 [==============================] - 15s 35ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.985859  0.962727  0.974155  421748.000000\n",
      "1              0.940873  0.965510  0.953033  268545.000000\n",
      "2              0.847627  0.901627  0.873794   43528.000000\n",
      "3              0.563291  0.794643  0.659259     112.000000\n",
      "4              0.660079  0.724512  0.690796     461.000000\n",
      "accuracy       0.959948  0.959948  0.959948       0.959948\n",
      "macro avg      0.799546  0.869804  0.830207  734394.000000\n",
      "weighted avg   0.960947  0.959948  0.960257  734394.000000\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Saline-KA Holdout Testing Dataset\n",
      "129/129 [==============================] - 5s 35ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.975469  0.962590  0.968987  132934.000000\n",
      "1              0.957576  0.948046  0.952787  125591.000000\n",
      "2              0.787402  0.905067  0.842144   20109.000000\n",
      "accuracy       0.951883  0.951883  0.951883       0.951883\n",
      "macro avg      0.906816  0.938568  0.921306  278634.000000\n",
      "weighted avg   0.953831  0.951883  0.952531  278634.000000\n",
      "unique ground truth labels: [0, 1, 2]\n",
      "unique predicted labels: [0, 1, 2]\n",
      "Saline Control Validation Dataset\n",
      "707/707 [==============================] - 24s 34ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.984568  0.963309  0.973822  7.920640e+05\n",
      "1              0.954704  0.962423  0.958548  6.297490e+05\n",
      "2              0.844665  0.939978  0.889776  1.036950e+05\n",
      "3              0.659794  0.847682  0.742029  3.020000e+02\n",
      "4              0.821990  0.963190  0.887006  1.304000e+03\n",
      "accuracy       0.961336  0.961336  0.961336  9.613362e-01\n",
      "macro avg      0.853144  0.935316  0.890236  1.527114e+06\n",
      "weighted avg   0.962550  0.961336  0.961697  1.527114e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training Dataset\n",
      "1176/1176 [==============================] - 41s 35ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.984066  0.963055  0.973447  1.346746e+06\n",
      "1              0.951365  0.961469  0.956391  1.023885e+06\n",
      "2              0.838246  0.925806  0.879853  1.673320e+05\n",
      "3              0.631868  0.833333  0.718750  4.140000e+02\n",
      "4              0.781711  0.900850  0.837062  1.765000e+03\n",
      "accuracy       0.959898  0.959898  0.959898  9.598979e-01\n",
      "macro avg      0.837451  0.916903  0.873101  2.540142e+06\n",
      "weighted avg   0.961081  0.959898  0.960270  2.540142e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training, Validation, and Testing Overall\n",
      "Model: \"sequential_157\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_476 (LSTM)             (None, 7, 200)            240800    \n",
      "                                                                 \n",
      " dropout_238 (Dropout)       (None, 7, 200)            0         \n",
      "                                                                 \n",
      " flatten_157 (Flatten)       (None, 1400)              0         \n",
      "                                                                 \n",
      " dense_196 (Dense)           (None, 5)                 7005      \n",
      "                                                                 \n",
      " activation_157 (Activation)  (None, 5)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 247,805\n",
      "Trainable params: 247,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.50148, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.7992426753044128-0.717607--val-0.9488796591758728-0.897999.h5\n",
      "707/707 - 30s - loss: 1.2921 - accuracy: 0.7176 - tp: 499208.0000 - fp: 23432.0000 - tn: 6085024.0000 - fn: 1027906.0000 - categorical_accuracy: 0.7176 - precision: 0.9552 - recall: 0.3269 - auc: 0.7992 - categorical_crossentropy: 1.0139 - val_loss: 0.5015 - val_accuracy: 0.8980 - val_tp: 221221.0000 - val_fp: 10450.0000 - val_tn: 1104086.0000 - val_fn: 57413.0000 - val_categorical_accuracy: 0.8980 - val_precision: 0.9549 - val_recall: 0.7939 - val_auc: 0.9489 - val_categorical_crossentropy: 0.4799 - 30s/epoch - 43ms/step\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 2: val_loss improved from 0.50148 to 0.33099, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9378063678741455-0.883552--val-0.9640379548072815-0.912100.h5\n",
      "707/707 - 24s - loss: 0.6205 - accuracy: 0.8836 - tp: 1217349.0000 - fp: 86893.0000 - tn: 6021563.0000 - fn: 309765.0000 - categorical_accuracy: 0.8836 - precision: 0.9334 - recall: 0.7972 - auc: 0.9378 - categorical_crossentropy: 0.4579 - val_loss: 0.3310 - val_accuracy: 0.9121 - val_tp: 245934.0000 - val_fp: 16688.0000 - val_tn: 1097848.0000 - val_fn: 32700.0000 - val_categorical_accuracy: 0.9121 - val_precision: 0.9365 - val_recall: 0.8826 - val_auc: 0.9640 - val_categorical_crossentropy: 0.3065 - 24s/epoch - 33ms/step\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 3: val_loss improved from 0.33099 to 0.26470, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9588004350662231-0.904750--val-0.973980188369751-0.923434.h5\n",
      "707/707 - 23s - loss: 0.4582 - accuracy: 0.9048 - tp: 1324172.0000 - fp: 99930.0000 - tn: 6008526.0000 - fn: 202942.0000 - categorical_accuracy: 0.9048 - precision: 0.9298 - recall: 0.8671 - auc: 0.9588 - categorical_crossentropy: 0.3241 - val_loss: 0.2647 - val_accuracy: 0.9234 - val_tp: 252963.0000 - val_fp: 16898.0000 - val_tn: 1097638.0000 - val_fn: 25671.0000 - val_categorical_accuracy: 0.9234 - val_precision: 0.9374 - val_recall: 0.9079 - val_auc: 0.9740 - val_categorical_crossentropy: 0.2388 - 23s/epoch - 33ms/step\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 4: val_loss improved from 0.26470 to 0.23472, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9699541926383972-0.917649--val-0.9790811538696289-0.929445.h5\n",
      "707/707 - 23s - loss: 0.3782 - accuracy: 0.9176 - tp: 1369370.0000 - fp: 98606.0000 - tn: 6009850.0000 - fn: 157744.0000 - categorical_accuracy: 0.9176 - precision: 0.9328 - recall: 0.8967 - auc: 0.9700 - categorical_crossentropy: 0.2580 - val_loss: 0.2347 - val_accuracy: 0.9294 - val_tp: 256218.0000 - val_fp: 16606.0000 - val_tn: 1097930.0000 - val_fn: 22416.0000 - val_categorical_accuracy: 0.9294 - val_precision: 0.9391 - val_recall: 0.9196 - val_auc: 0.9791 - val_categorical_crossentropy: 0.2076 - 23s/epoch - 33ms/step\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 5: val_loss improved from 0.23472 to 0.21881, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9754806160926819-0.925118--val-0.9817824959754944-0.933074.h5\n",
      "707/707 - 25s - loss: 0.3343 - accuracy: 0.9251 - tp: 1391877.0000 - fp: 95678.0000 - tn: 6012778.0000 - fn: 135237.0000 - categorical_accuracy: 0.9251 - precision: 0.9357 - recall: 0.9114 - auc: 0.9755 - categorical_crossentropy: 0.2243 - val_loss: 0.2188 - val_accuracy: 0.9331 - val_tp: 257831.0000 - val_fp: 16202.0000 - val_tn: 1098334.0000 - val_fn: 20803.0000 - val_categorical_accuracy: 0.9331 - val_precision: 0.9409 - val_recall: 0.9253 - val_auc: 0.9818 - val_categorical_crossentropy: 0.1909 - 25s/epoch - 35ms/step\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 6: val_loss improved from 0.21881 to 0.20856, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9788514971733093-0.930306--val-0.9834914207458496-0.935668.h5\n",
      "707/707 - 25s - loss: 0.3062 - accuracy: 0.9303 - tp: 1404754.0000 - fp: 91845.0000 - tn: 6016611.0000 - fn: 122360.0000 - categorical_accuracy: 0.9303 - precision: 0.9386 - recall: 0.9199 - auc: 0.9789 - categorical_crossentropy: 0.2037 - val_loss: 0.2086 - val_accuracy: 0.9357 - val_tp: 258887.0000 - val_fp: 15760.0000 - val_tn: 1098776.0000 - val_fn: 19747.0000 - val_categorical_accuracy: 0.9357 - val_precision: 0.9426 - val_recall: 0.9291 - val_auc: 0.9835 - val_categorical_crossentropy: 0.1802 - 25s/epoch - 35ms/step\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 7: val_loss improved from 0.20856 to 0.20117, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9811477065086365-0.934169--val-0.9846212267875671-0.937646.h5\n",
      "707/707 - 25s - loss: 0.2866 - accuracy: 0.9342 - tp: 1414554.0000 - fp: 88216.0000 - tn: 6020240.0000 - fn: 112560.0000 - categorical_accuracy: 0.9342 - precision: 0.9413 - recall: 0.9263 - auc: 0.9811 - categorical_crossentropy: 0.1895 - val_loss: 0.2012 - val_accuracy: 0.9376 - val_tp: 259637.0000 - val_fp: 15372.0000 - val_tn: 1099164.0000 - val_fn: 18997.0000 - val_categorical_accuracy: 0.9376 - val_precision: 0.9441 - val_recall: 0.9318 - val_auc: 0.9846 - val_categorical_crossentropy: 0.1727 - 25s/epoch - 36ms/step\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 8: val_loss improved from 0.20117 to 0.19518, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.982839047908783-0.937208--val-0.9855046272277832-0.939343.h5\n",
      "707/707 - 25s - loss: 0.2718 - accuracy: 0.9372 - tp: 1421435.0000 - fp: 84680.0000 - tn: 6023776.0000 - fn: 105679.0000 - categorical_accuracy: 0.9372 - precision: 0.9438 - recall: 0.9308 - auc: 0.9828 - categorical_crossentropy: 0.1790 - val_loss: 0.1952 - val_accuracy: 0.9393 - val_tp: 260286.0000 - val_fp: 15056.0000 - val_tn: 1099480.0000 - val_fn: 18348.0000 - val_categorical_accuracy: 0.9393 - val_precision: 0.9453 - val_recall: 0.9342 - val_auc: 0.9855 - val_categorical_crossentropy: 0.1667 - 25s/epoch - 36ms/step\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 9: val_loss improved from 0.19518 to 0.19097, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9841437339782715-0.939708--val-0.9860779047012329-0.940503.h5\n",
      "707/707 - 25s - loss: 0.2598 - accuracy: 0.9397 - tp: 1426178.0000 - fp: 81854.0000 - tn: 6026602.0000 - fn: 100936.0000 - categorical_accuracy: 0.9397 - precision: 0.9457 - recall: 0.9339 - auc: 0.9841 - categorical_crossentropy: 0.1707 - val_loss: 0.1910 - val_accuracy: 0.9405 - val_tp: 260645.0000 - val_fp: 14769.0000 - val_tn: 1099767.0000 - val_fn: 17989.0000 - val_categorical_accuracy: 0.9405 - val_precision: 0.9464 - val_recall: 0.9354 - val_auc: 0.9861 - val_categorical_crossentropy: 0.1627 - 25s/epoch - 35ms/step\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 10: val_loss improved from 0.19097 to 0.18634, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9851552248001099-0.941786--val-0.9867311716079712-0.941920.h5\n",
      "707/707 - 25s - loss: 0.2505 - accuracy: 0.9418 - tp: 1429906.0000 - fp: 79333.0000 - tn: 6029123.0000 - fn: 97208.0000 - categorical_accuracy: 0.9418 - precision: 0.9474 - recall: 0.9363 - auc: 0.9852 - categorical_crossentropy: 0.1642 - val_loss: 0.1863 - val_accuracy: 0.9419 - val_tp: 261108.0000 - val_fp: 14508.0000 - val_tn: 1100028.0000 - val_fn: 17526.0000 - val_categorical_accuracy: 0.9419 - val_precision: 0.9474 - val_recall: 0.9371 - val_auc: 0.9867 - val_categorical_crossentropy: 0.1584 - 25s/epoch - 35ms/step\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 11: val_loss improved from 0.18634 to 0.18398, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9859632849693298-0.943672--val-0.9868808388710022-0.942731.h5\n",
      "707/707 - 25s - loss: 0.2424 - accuracy: 0.9437 - tp: 1433557.0000 - fp: 77204.0000 - tn: 6031252.0000 - fn: 93557.0000 - categorical_accuracy: 0.9437 - precision: 0.9489 - recall: 0.9387 - auc: 0.9860 - categorical_crossentropy: 0.1586 - val_loss: 0.1840 - val_accuracy: 0.9427 - val_tp: 261400.0000 - val_fp: 14275.0000 - val_tn: 1100261.0000 - val_fn: 17234.0000 - val_categorical_accuracy: 0.9427 - val_precision: 0.9482 - val_recall: 0.9381 - val_auc: 0.9869 - val_categorical_crossentropy: 0.1564 - 25s/epoch - 35ms/step\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 12: val_loss improved from 0.18398 to 0.18005, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9866766929626465-0.945140--val-0.9874507188796997-0.943998.h5\n",
      "707/707 - 25s - loss: 0.2355 - accuracy: 0.9451 - tp: 1436028.0000 - fp: 75304.0000 - tn: 6033152.0000 - fn: 91086.0000 - categorical_accuracy: 0.9451 - precision: 0.9502 - recall: 0.9404 - auc: 0.9867 - categorical_crossentropy: 0.1539 - val_loss: 0.1801 - val_accuracy: 0.9440 - val_tp: 261848.0000 - val_fp: 13985.0000 - val_tn: 1100551.0000 - val_fn: 16786.0000 - val_categorical_accuracy: 0.9440 - val_precision: 0.9493 - val_recall: 0.9398 - val_auc: 0.9875 - val_categorical_crossentropy: 0.1527 - 25s/epoch - 36ms/step\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 13: val_loss improved from 0.18005 to 0.17745, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9872775077819824-0.946624--val-0.9877676963806152-0.944716.h5\n",
      "707/707 - 25s - loss: 0.2296 - accuracy: 0.9466 - tp: 1438749.0000 - fp: 73373.0000 - tn: 6035083.0000 - fn: 88365.0000 - categorical_accuracy: 0.9466 - precision: 0.9515 - recall: 0.9421 - auc: 0.9873 - categorical_crossentropy: 0.1498 - val_loss: 0.1775 - val_accuracy: 0.9447 - val_tp: 262109.0000 - val_fp: 13843.0000 - val_tn: 1100693.0000 - val_fn: 16525.0000 - val_categorical_accuracy: 0.9447 - val_precision: 0.9498 - val_recall: 0.9407 - val_auc: 0.9878 - val_categorical_crossentropy: 0.1505 - 25s/epoch - 36ms/step\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 14: val_loss improved from 0.17745 to 0.17529, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9877684116363525-0.947881--val-0.9879257678985596-0.945631.h5\n",
      "707/707 - 25s - loss: 0.2246 - accuracy: 0.9479 - tp: 1441014.0000 - fp: 71849.0000 - tn: 6036607.0000 - fn: 86100.0000 - categorical_accuracy: 0.9479 - precision: 0.9525 - recall: 0.9436 - auc: 0.9878 - categorical_crossentropy: 0.1462 - val_loss: 0.1753 - val_accuracy: 0.9456 - val_tp: 262331.0000 - val_fp: 13694.0000 - val_tn: 1100842.0000 - val_fn: 16303.0000 - val_categorical_accuracy: 0.9456 - val_precision: 0.9504 - val_recall: 0.9415 - val_auc: 0.9879 - val_categorical_crossentropy: 0.1487 - 25s/epoch - 36ms/step\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 15: val_loss improved from 0.17529 to 0.17217, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9882501363754272-0.949000--val-0.9883124828338623-0.946546.h5\n",
      "707/707 - 25s - loss: 0.2196 - accuracy: 0.9490 - tp: 1442912.0000 - fp: 70406.0000 - tn: 6038050.0000 - fn: 84202.0000 - categorical_accuracy: 0.9490 - precision: 0.9535 - recall: 0.9449 - auc: 0.9883 - categorical_crossentropy: 0.1430 - val_loss: 0.1722 - val_accuracy: 0.9465 - val_tp: 262679.0000 - val_fp: 13519.0000 - val_tn: 1101017.0000 - val_fn: 15955.0000 - val_categorical_accuracy: 0.9465 - val_precision: 0.9511 - val_recall: 0.9427 - val_auc: 0.9883 - val_categorical_crossentropy: 0.1458 - 25s/epoch - 36ms/step\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 16: val_loss improved from 0.17217 to 0.16984, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9886366128921509-0.950141--val-0.9885784983634949-0.947300.h5\n",
      "707/707 - 25s - loss: 0.2152 - accuracy: 0.9501 - tp: 1444803.0000 - fp: 68915.0000 - tn: 6039541.0000 - fn: 82311.0000 - categorical_accuracy: 0.9501 - precision: 0.9545 - recall: 0.9461 - auc: 0.9886 - categorical_crossentropy: 0.1399 - val_loss: 0.1698 - val_accuracy: 0.9473 - val_tp: 262882.0000 - val_fp: 13332.0000 - val_tn: 1101204.0000 - val_fn: 15752.0000 - val_categorical_accuracy: 0.9473 - val_precision: 0.9517 - val_recall: 0.9435 - val_auc: 0.9886 - val_categorical_crossentropy: 0.1438 - 25s/epoch - 35ms/step\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 17: val_loss improved from 0.16984 to 0.16780, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9889745116233826-0.951059--val-0.9887349009513855-0.947860.h5\n",
      "707/707 - 25s - loss: 0.2116 - accuracy: 0.9511 - tp: 1446346.0000 - fp: 67673.0000 - tn: 6040783.0000 - fn: 80768.0000 - categorical_accuracy: 0.9511 - precision: 0.9553 - recall: 0.9471 - auc: 0.9890 - categorical_crossentropy: 0.1374 - val_loss: 0.1678 - val_accuracy: 0.9479 - val_tp: 263132.0000 - val_fp: 13216.0000 - val_tn: 1101320.0000 - val_fn: 15502.0000 - val_categorical_accuracy: 0.9479 - val_precision: 0.9522 - val_recall: 0.9444 - val_auc: 0.9887 - val_categorical_crossentropy: 0.1420 - 25s/epoch - 35ms/step\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 18: val_loss improved from 0.16780 to 0.16687, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9893520474433899-0.951825--val-0.9887809753417969-0.948179.h5\n",
      "707/707 - 25s - loss: 0.2079 - accuracy: 0.9518 - tp: 1447783.0000 - fp: 66605.0000 - tn: 6041851.0000 - fn: 79331.0000 - categorical_accuracy: 0.9518 - precision: 0.9560 - recall: 0.9481 - auc: 0.9894 - categorical_crossentropy: 0.1348 - val_loss: 0.1669 - val_accuracy: 0.9482 - val_tp: 263190.0000 - val_fp: 13132.0000 - val_tn: 1101404.0000 - val_fn: 15444.0000 - val_categorical_accuracy: 0.9482 - val_precision: 0.9525 - val_recall: 0.9446 - val_auc: 0.9888 - val_categorical_crossentropy: 0.1414 - 25s/epoch - 35ms/step\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 19: val_loss improved from 0.16687 to 0.16483, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9896324872970581-0.952687--val-0.9890189170837402-0.948786.h5\n",
      "707/707 - 25s - loss: 0.2048 - accuracy: 0.9527 - tp: 1449176.0000 - fp: 65688.0000 - tn: 6042768.0000 - fn: 77938.0000 - categorical_accuracy: 0.9527 - precision: 0.9566 - recall: 0.9490 - auc: 0.9896 - categorical_crossentropy: 0.1327 - val_loss: 0.1648 - val_accuracy: 0.9488 - val_tp: 263398.0000 - val_fp: 12976.0000 - val_tn: 1101560.0000 - val_fn: 15236.0000 - val_categorical_accuracy: 0.9488 - val_precision: 0.9530 - val_recall: 0.9453 - val_auc: 0.9890 - val_categorical_crossentropy: 0.1396 - 25s/epoch - 35ms/step\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 20: val_loss improved from 0.16483 to 0.16331, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9899035096168518-0.953525--val-0.9891941547393799-0.949220.h5\n",
      "707/707 - 25s - loss: 0.2017 - accuracy: 0.9535 - tp: 1450562.0000 - fp: 64556.0000 - tn: 6043900.0000 - fn: 76552.0000 - categorical_accuracy: 0.9535 - precision: 0.9574 - recall: 0.9499 - auc: 0.9899 - categorical_crossentropy: 0.1306 - val_loss: 0.1633 - val_accuracy: 0.9492 - val_tp: 263539.0000 - val_fp: 12895.0000 - val_tn: 1101641.0000 - val_fn: 15095.0000 - val_categorical_accuracy: 0.9492 - val_precision: 0.9534 - val_recall: 0.9458 - val_auc: 0.9892 - val_categorical_crossentropy: 0.1383 - 25s/epoch - 36ms/step\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 21: val_loss improved from 0.16331 to 0.16105, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9901372194290161-0.954036--val-0.9894522428512573-0.950099.h5\n",
      "707/707 - 25s - loss: 0.1992 - accuracy: 0.9540 - tp: 1451516.0000 - fp: 63943.0000 - tn: 6044513.0000 - fn: 75598.0000 - categorical_accuracy: 0.9540 - precision: 0.9578 - recall: 0.9505 - auc: 0.9901 - categorical_crossentropy: 0.1289 - val_loss: 0.1610 - val_accuracy: 0.9501 - val_tp: 263771.0000 - val_fp: 12705.0000 - val_tn: 1101831.0000 - val_fn: 14863.0000 - val_categorical_accuracy: 0.9501 - val_precision: 0.9540 - val_recall: 0.9467 - val_auc: 0.9895 - val_categorical_crossentropy: 0.1363 - 25s/epoch - 36ms/step\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 22: val_loss improved from 0.16105 to 0.16000, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9903578758239746-0.954608--val-0.9895363450050354-0.950390.h5\n",
      "707/707 - 26s - loss: 0.1966 - accuracy: 0.9546 - tp: 1452586.0000 - fp: 63071.0000 - tn: 6045385.0000 - fn: 74528.0000 - categorical_accuracy: 0.9546 - precision: 0.9584 - recall: 0.9512 - auc: 0.9904 - categorical_crossentropy: 0.1272 - val_loss: 0.1600 - val_accuracy: 0.9504 - val_tp: 263895.0000 - val_fp: 12658.0000 - val_tn: 1101878.0000 - val_fn: 14739.0000 - val_categorical_accuracy: 0.9504 - val_precision: 0.9542 - val_recall: 0.9471 - val_auc: 0.9895 - val_categorical_crossentropy: 0.1355 - 26s/epoch - 36ms/step\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 23: val_loss improved from 0.16000 to 0.15873, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9905569553375244-0.955318--val-0.9896785616874695-0.950760.h5\n",
      "707/707 - 25s - loss: 0.1942 - accuracy: 0.9553 - tp: 1453642.0000 - fp: 62243.0000 - tn: 6046213.0000 - fn: 73472.0000 - categorical_accuracy: 0.9553 - precision: 0.9589 - recall: 0.9519 - auc: 0.9906 - categorical_crossentropy: 0.1255 - val_loss: 0.1587 - val_accuracy: 0.9508 - val_tp: 263988.0000 - val_fp: 12550.0000 - val_tn: 1101986.0000 - val_fn: 14646.0000 - val_categorical_accuracy: 0.9508 - val_precision: 0.9546 - val_recall: 0.9474 - val_auc: 0.9897 - val_categorical_crossentropy: 0.1345 - 25s/epoch - 36ms/step\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 24: val_loss improved from 0.15873 to 0.15729, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9907954931259155-0.955789--val-0.9898502826690674-0.951155.h5\n",
      "707/707 - 25s - loss: 0.1919 - accuracy: 0.9558 - tp: 1454560.0000 - fp: 61665.0000 - tn: 6046791.0000 - fn: 72554.0000 - categorical_accuracy: 0.9558 - precision: 0.9593 - recall: 0.9525 - auc: 0.9908 - categorical_crossentropy: 0.1240 - val_loss: 0.1573 - val_accuracy: 0.9512 - val_tp: 264133.0000 - val_fp: 12460.0000 - val_tn: 1102076.0000 - val_fn: 14501.0000 - val_categorical_accuracy: 0.9512 - val_precision: 0.9550 - val_recall: 0.9480 - val_auc: 0.9899 - val_categorical_crossentropy: 0.1332 - 25s/epoch - 35ms/step\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.15729\n",
      "707/707 - 23s - loss: 0.1901 - accuracy: 0.9561 - tp: 1455126.0000 - fp: 61019.0000 - tn: 6047437.0000 - fn: 71988.0000 - categorical_accuracy: 0.9561 - precision: 0.9598 - recall: 0.9529 - auc: 0.9909 - categorical_crossentropy: 0.1227 - val_loss: 0.1575 - val_accuracy: 0.9511 - val_tp: 264153.0000 - val_fp: 12464.0000 - val_tn: 1102072.0000 - val_fn: 14481.0000 - val_categorical_accuracy: 0.9511 - val_precision: 0.9549 - val_recall: 0.9480 - val_auc: 0.9897 - val_categorical_crossentropy: 0.1337 - 23s/epoch - 32ms/step\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 26: val_loss improved from 0.15729 to 0.15630, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9911062717437744-0.956646--val-0.989834725856781-0.951517.h5\n",
      "707/707 - 24s - loss: 0.1880 - accuracy: 0.9566 - tp: 1456031.0000 - fp: 60629.0000 - tn: 6047827.0000 - fn: 71083.0000 - categorical_accuracy: 0.9566 - precision: 0.9600 - recall: 0.9535 - auc: 0.9911 - categorical_crossentropy: 0.1214 - val_loss: 0.1563 - val_accuracy: 0.9515 - val_tp: 264285.0000 - val_fp: 12377.0000 - val_tn: 1102159.0000 - val_fn: 14349.0000 - val_categorical_accuracy: 0.9515 - val_precision: 0.9553 - val_recall: 0.9485 - val_auc: 0.9898 - val_categorical_crossentropy: 0.1326 - 24s/epoch - 34ms/step\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 27: val_loss improved from 0.15630 to 0.15551, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9912360310554504-0.957176--val-0.989938497543335-0.951829.h5\n",
      "707/707 - 23s - loss: 0.1864 - accuracy: 0.9572 - tp: 1456953.0000 - fp: 59864.0000 - tn: 6048592.0000 - fn: 70161.0000 - categorical_accuracy: 0.9572 - precision: 0.9605 - recall: 0.9541 - auc: 0.9912 - categorical_crossentropy: 0.1203 - val_loss: 0.1555 - val_accuracy: 0.9518 - val_tp: 264370.0000 - val_fp: 12335.0000 - val_tn: 1102201.0000 - val_fn: 14264.0000 - val_categorical_accuracy: 0.9518 - val_precision: 0.9554 - val_recall: 0.9488 - val_auc: 0.9899 - val_categorical_crossentropy: 0.1320 - 23s/epoch - 33ms/step\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 28: val_loss improved from 0.15551 to 0.15455, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9914077520370483-0.957483--val-0.9900494813919067-0.951976.h5\n",
      "707/707 - 23s - loss: 0.1847 - accuracy: 0.9575 - tp: 1457605.0000 - fp: 59431.0000 - tn: 6049025.0000 - fn: 69509.0000 - categorical_accuracy: 0.9575 - precision: 0.9608 - recall: 0.9545 - auc: 0.9914 - categorical_crossentropy: 0.1191 - val_loss: 0.1545 - val_accuracy: 0.9520 - val_tp: 264442.0000 - val_fp: 12247.0000 - val_tn: 1102289.0000 - val_fn: 14192.0000 - val_categorical_accuracy: 0.9520 - val_precision: 0.9557 - val_recall: 0.9491 - val_auc: 0.9900 - val_categorical_crossentropy: 0.1313 - 23s/epoch - 33ms/step\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 29: val_loss improved from 0.15455 to 0.15448, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9914999008178711-0.957739--val-0.9899899363517761-0.952142.h5\n",
      "707/707 - 23s - loss: 0.1831 - accuracy: 0.9577 - tp: 1457997.0000 - fp: 59066.0000 - tn: 6049390.0000 - fn: 69117.0000 - categorical_accuracy: 0.9577 - precision: 0.9611 - recall: 0.9547 - auc: 0.9915 - categorical_crossentropy: 0.1182 - val_loss: 0.1545 - val_accuracy: 0.9521 - val_tp: 264482.0000 - val_fp: 12229.0000 - val_tn: 1102307.0000 - val_fn: 14152.0000 - val_categorical_accuracy: 0.9521 - val_precision: 0.9558 - val_recall: 0.9492 - val_auc: 0.9900 - val_categorical_crossentropy: 0.1314 - 23s/epoch - 33ms/step\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 30: val_loss improved from 0.15448 to 0.15387, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9916185140609741-0.958099--val-0.9900698661804199-0.952199.h5\n",
      "707/707 - 23s - loss: 0.1816 - accuracy: 0.9581 - tp: 1458578.0000 - fp: 58722.0000 - tn: 6049734.0000 - fn: 68536.0000 - categorical_accuracy: 0.9581 - precision: 0.9613 - recall: 0.9551 - auc: 0.9916 - categorical_crossentropy: 0.1172 - val_loss: 0.1539 - val_accuracy: 0.9522 - val_tp: 264530.0000 - val_fp: 12224.0000 - val_tn: 1102312.0000 - val_fn: 14104.0000 - val_categorical_accuracy: 0.9522 - val_precision: 0.9558 - val_recall: 0.9494 - val_auc: 0.9901 - val_categorical_crossentropy: 0.1309 - 23s/epoch - 33ms/step\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 31: val_loss improved from 0.15387 to 0.15276, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9917470216751099-0.958380--val-0.9902031421661377-0.952554.h5\n",
      "707/707 - 23s - loss: 0.1803 - accuracy: 0.9584 - tp: 1459053.0000 - fp: 58297.0000 - tn: 6050159.0000 - fn: 68061.0000 - categorical_accuracy: 0.9584 - precision: 0.9616 - recall: 0.9554 - auc: 0.9917 - categorical_crossentropy: 0.1163 - val_loss: 0.1528 - val_accuracy: 0.9526 - val_tp: 264601.0000 - val_fp: 12150.0000 - val_tn: 1102386.0000 - val_fn: 14033.0000 - val_categorical_accuracy: 0.9526 - val_precision: 0.9561 - val_recall: 0.9496 - val_auc: 0.9902 - val_categorical_crossentropy: 0.1300 - 23s/epoch - 33ms/step\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 32: val_loss improved from 0.15276 to 0.15206, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9919182658195496-0.958818--val-0.9902753829956055-0.952863.h5\n",
      "707/707 - 23s - loss: 0.1787 - accuracy: 0.9588 - tp: 1459862.0000 - fp: 57753.0000 - tn: 6050703.0000 - fn: 67252.0000 - categorical_accuracy: 0.9588 - precision: 0.9619 - recall: 0.9560 - auc: 0.9919 - categorical_crossentropy: 0.1153 - val_loss: 0.1521 - val_accuracy: 0.9529 - val_tp: 264706.0000 - val_fp: 12086.0000 - val_tn: 1102450.0000 - val_fn: 13928.0000 - val_categorical_accuracy: 0.9529 - val_precision: 0.9563 - val_recall: 0.9500 - val_auc: 0.9903 - val_categorical_crossentropy: 0.1295 - 23s/epoch - 33ms/step\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.15206\n",
      "707/707 - 23s - loss: 0.1774 - accuracy: 0.9590 - tp: 1460220.0000 - fp: 57466.0000 - tn: 6050990.0000 - fn: 66894.0000 - categorical_accuracy: 0.9590 - precision: 0.9621 - recall: 0.9562 - auc: 0.9920 - categorical_crossentropy: 0.1145 - val_loss: 0.1530 - val_accuracy: 0.9526 - val_tp: 264651.0000 - val_fp: 12180.0000 - val_tn: 1102356.0000 - val_fn: 13983.0000 - val_categorical_accuracy: 0.9526 - val_precision: 0.9560 - val_recall: 0.9498 - val_auc: 0.9901 - val_categorical_crossentropy: 0.1306 - 23s/epoch - 33ms/step\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 34: val_loss improved from 0.15206 to 0.15182, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9920662641525269-0.959243--val-0.9902396202087402-0.952960.h5\n",
      "707/707 - 24s - loss: 0.1763 - accuracy: 0.9592 - tp: 1460674.0000 - fp: 57285.0000 - tn: 6051171.0000 - fn: 66440.0000 - categorical_accuracy: 0.9592 - precision: 0.9623 - recall: 0.9565 - auc: 0.9921 - categorical_crossentropy: 0.1137 - val_loss: 0.1518 - val_accuracy: 0.9530 - val_tp: 264767.0000 - val_fp: 12067.0000 - val_tn: 1102469.0000 - val_fn: 13867.0000 - val_categorical_accuracy: 0.9530 - val_precision: 0.9564 - val_recall: 0.9502 - val_auc: 0.9902 - val_categorical_crossentropy: 0.1296 - 24s/epoch - 33ms/step\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 35: val_loss improved from 0.15182 to 0.15048, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9921280145645142-0.959626--val-0.9904122352600098-0.953369.h5\n",
      "707/707 - 25s - loss: 0.1752 - accuracy: 0.9596 - tp: 1461245.0000 - fp: 56625.0000 - tn: 6051831.0000 - fn: 65869.0000 - categorical_accuracy: 0.9596 - precision: 0.9627 - recall: 0.9569 - auc: 0.9921 - categorical_crossentropy: 0.1131 - val_loss: 0.1505 - val_accuracy: 0.9534 - val_tp: 264889.0000 - val_fp: 11986.0000 - val_tn: 1102550.0000 - val_fn: 13745.0000 - val_categorical_accuracy: 0.9534 - val_precision: 0.9567 - val_recall: 0.9507 - val_auc: 0.9904 - val_categorical_crossentropy: 0.1283 - 25s/epoch - 35ms/step\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 36: val_loss improved from 0.15048 to 0.14947, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9922427535057068-0.959827--val-0.9905341267585754-0.953660.h5\n",
      "707/707 - 25s - loss: 0.1740 - accuracy: 0.9598 - tp: 1461524.0000 - fp: 56485.0000 - tn: 6051971.0000 - fn: 65590.0000 - categorical_accuracy: 0.9598 - precision: 0.9628 - recall: 0.9570 - auc: 0.9922 - categorical_crossentropy: 0.1124 - val_loss: 0.1495 - val_accuracy: 0.9537 - val_tp: 265013.0000 - val_fp: 11892.0000 - val_tn: 1102644.0000 - val_fn: 13621.0000 - val_categorical_accuracy: 0.9537 - val_precision: 0.9571 - val_recall: 0.9511 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1274 - 25s/epoch - 36ms/step\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.14947\n",
      "707/707 - 25s - loss: 0.1727 - accuracy: 0.9601 - tp: 1461958.0000 - fp: 56145.0000 - tn: 6052311.0000 - fn: 65156.0000 - categorical_accuracy: 0.9601 - precision: 0.9630 - recall: 0.9573 - auc: 0.9923 - categorical_crossentropy: 0.1114 - val_loss: 0.1511 - val_accuracy: 0.9531 - val_tp: 264836.0000 - val_fp: 12044.0000 - val_tn: 1102492.0000 - val_fn: 13798.0000 - val_categorical_accuracy: 0.9531 - val_precision: 0.9565 - val_recall: 0.9505 - val_auc: 0.9903 - val_categorical_crossentropy: 0.1292 - 25s/epoch - 36ms/step\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.14947\n",
      "707/707 - 25s - loss: 0.1717 - accuracy: 0.9602 - tp: 1462281.0000 - fp: 55886.0000 - tn: 6052570.0000 - fn: 64833.0000 - categorical_accuracy: 0.9602 - precision: 0.9632 - recall: 0.9575 - auc: 0.9924 - categorical_crossentropy: 0.1108 - val_loss: 0.1498 - val_accuracy: 0.9535 - val_tp: 264969.0000 - val_fp: 11964.0000 - val_tn: 1102572.0000 - val_fn: 13665.0000 - val_categorical_accuracy: 0.9535 - val_precision: 0.9568 - val_recall: 0.9510 - val_auc: 0.9904 - val_categorical_crossentropy: 0.1281 - 25s/epoch - 35ms/step\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 39: val_loss improved from 0.14947 to 0.14917, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9925069808959961-0.960635--val-0.9904611110687256-0.953775.h5\n",
      "707/707 - 25s - loss: 0.1707 - accuracy: 0.9606 - tp: 1462942.0000 - fp: 55454.0000 - tn: 6053002.0000 - fn: 64172.0000 - categorical_accuracy: 0.9606 - precision: 0.9635 - recall: 0.9580 - auc: 0.9925 - categorical_crossentropy: 0.1102 - val_loss: 0.1492 - val_accuracy: 0.9538 - val_tp: 265026.0000 - val_fp: 11928.0000 - val_tn: 1102608.0000 - val_fn: 13608.0000 - val_categorical_accuracy: 0.9538 - val_precision: 0.9569 - val_recall: 0.9512 - val_auc: 0.9905 - val_categorical_crossentropy: 0.1276 - 25s/epoch - 36ms/step\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.14917\n",
      "707/707 - 25s - loss: 0.1700 - accuracy: 0.9607 - tp: 1463093.0000 - fp: 55456.0000 - tn: 6053000.0000 - fn: 64021.0000 - categorical_accuracy: 0.9607 - precision: 0.9635 - recall: 0.9581 - auc: 0.9926 - categorical_crossentropy: 0.1097 - val_loss: 0.1492 - val_accuracy: 0.9538 - val_tp: 265044.0000 - val_fp: 11921.0000 - val_tn: 1102615.0000 - val_fn: 13590.0000 - val_categorical_accuracy: 0.9538 - val_precision: 0.9570 - val_recall: 0.9512 - val_auc: 0.9904 - val_categorical_crossentropy: 0.1277 - 25s/epoch - 35ms/step\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 41: val_loss improved from 0.14917 to 0.14805, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.99265056848526-0.960918--val-0.9905650019645691-0.954047.h5\n",
      "707/707 - 25s - loss: 0.1689 - accuracy: 0.9609 - tp: 1463439.0000 - fp: 55006.0000 - tn: 6053450.0000 - fn: 63675.0000 - categorical_accuracy: 0.9609 - precision: 0.9638 - recall: 0.9583 - auc: 0.9927 - categorical_crossentropy: 0.1091 - val_loss: 0.1481 - val_accuracy: 0.9540 - val_tp: 265114.0000 - val_fp: 11867.0000 - val_tn: 1102669.0000 - val_fn: 13520.0000 - val_categorical_accuracy: 0.9540 - val_precision: 0.9572 - val_recall: 0.9515 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1267 - 25s/epoch - 35ms/step\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 42: val_loss improved from 0.14805 to 0.14782, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9926822185516357-0.961069--val-0.990605890750885-0.954112.h5\n",
      "707/707 - 25s - loss: 0.1681 - accuracy: 0.9611 - tp: 1463789.0000 - fp: 54888.0000 - tn: 6053568.0000 - fn: 63325.0000 - categorical_accuracy: 0.9611 - precision: 0.9639 - recall: 0.9585 - auc: 0.9927 - categorical_crossentropy: 0.1085 - val_loss: 0.1478 - val_accuracy: 0.9541 - val_tp: 265123.0000 - val_fp: 11834.0000 - val_tn: 1102702.0000 - val_fn: 13511.0000 - val_categorical_accuracy: 0.9541 - val_precision: 0.9573 - val_recall: 0.9515 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1266 - 25s/epoch - 36ms/step\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 43: val_loss improved from 0.14782 to 0.14716, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9927508234977722-0.961245--val-0.990665078163147-0.954374.h5\n",
      "707/707 - 25s - loss: 0.1672 - accuracy: 0.9612 - tp: 1464012.0000 - fp: 54664.0000 - tn: 6053792.0000 - fn: 63102.0000 - categorical_accuracy: 0.9612 - precision: 0.9640 - recall: 0.9587 - auc: 0.9928 - categorical_crossentropy: 0.1080 - val_loss: 0.1472 - val_accuracy: 0.9544 - val_tp: 265246.0000 - val_fp: 11793.0000 - val_tn: 1102743.0000 - val_fn: 13388.0000 - val_categorical_accuracy: 0.9544 - val_precision: 0.9574 - val_recall: 0.9520 - val_auc: 0.9907 - val_categorical_crossentropy: 0.1260 - 25s/epoch - 36ms/step\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.14716\n",
      "707/707 - 25s - loss: 0.1665 - accuracy: 0.9613 - tp: 1464186.0000 - fp: 54576.0000 - tn: 6053880.0000 - fn: 62928.0000 - categorical_accuracy: 0.9613 - precision: 0.9641 - recall: 0.9588 - auc: 0.9928 - categorical_crossentropy: 0.1075 - val_loss: 0.1477 - val_accuracy: 0.9543 - val_tp: 265199.0000 - val_fp: 11840.0000 - val_tn: 1102696.0000 - val_fn: 13435.0000 - val_categorical_accuracy: 0.9543 - val_precision: 0.9573 - val_recall: 0.9518 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1266 - 25s/epoch - 36ms/step\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 45: val_loss improved from 0.14716 to 0.14698, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9929251670837402-0.961539--val-0.9906415939331055-0.954521.h5\n",
      "707/707 - 25s - loss: 0.1657 - accuracy: 0.9615 - tp: 1464476.0000 - fp: 54200.0000 - tn: 6054256.0000 - fn: 62638.0000 - categorical_accuracy: 0.9615 - precision: 0.9643 - recall: 0.9590 - auc: 0.9929 - categorical_crossentropy: 0.1070 - val_loss: 0.1470 - val_accuracy: 0.9545 - val_tp: 265269.0000 - val_fp: 11779.0000 - val_tn: 1102757.0000 - val_fn: 13365.0000 - val_categorical_accuracy: 0.9545 - val_precision: 0.9575 - val_recall: 0.9520 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1260 - 25s/epoch - 35ms/step\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 46: val_loss improved from 0.14698 to 0.14679, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9929671287536621-0.961729--val-0.9906507730484009-0.954442.h5\n",
      "707/707 - 20s - loss: 0.1649 - accuracy: 0.9617 - tp: 1464776.0000 - fp: 54031.0000 - tn: 6054425.0000 - fn: 62338.0000 - categorical_accuracy: 0.9617 - precision: 0.9644 - recall: 0.9592 - auc: 0.9930 - categorical_crossentropy: 0.1065 - val_loss: 0.1468 - val_accuracy: 0.9544 - val_tp: 265236.0000 - val_fp: 11777.0000 - val_tn: 1102759.0000 - val_fn: 13398.0000 - val_categorical_accuracy: 0.9544 - val_precision: 0.9575 - val_recall: 0.9519 - val_auc: 0.9907 - val_categorical_crossentropy: 0.1260 - 20s/epoch - 29ms/step\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 47: val_loss improved from 0.14679 to 0.14668, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9930481910705566-0.961901--val-0.990642786026001-0.954593.h5\n",
      "707/707 - 24s - loss: 0.1639 - accuracy: 0.9619 - tp: 1465091.0000 - fp: 53763.0000 - tn: 6054693.0000 - fn: 62023.0000 - categorical_accuracy: 0.9619 - precision: 0.9646 - recall: 0.9594 - auc: 0.9930 - categorical_crossentropy: 0.1059 - val_loss: 0.1467 - val_accuracy: 0.9546 - val_tp: 265288.0000 - val_fp: 11778.0000 - val_tn: 1102758.0000 - val_fn: 13346.0000 - val_categorical_accuracy: 0.9546 - val_precision: 0.9575 - val_recall: 0.9521 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1259 - 24s/epoch - 34ms/step\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.14668\n",
      "707/707 - 25s - loss: 0.1633 - accuracy: 0.9621 - tp: 1465573.0000 - fp: 53521.0000 - tn: 6054935.0000 - fn: 61541.0000 - categorical_accuracy: 0.9621 - precision: 0.9648 - recall: 0.9597 - auc: 0.9931 - categorical_crossentropy: 0.1055 - val_loss: 0.1470 - val_accuracy: 0.9544 - val_tp: 265231.0000 - val_fp: 11806.0000 - val_tn: 1102730.0000 - val_fn: 13403.0000 - val_categorical_accuracy: 0.9544 - val_precision: 0.9574 - val_recall: 0.9519 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1264 - 25s/epoch - 35ms/step\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.14668\n",
      "707/707 - 24s - loss: 0.1626 - accuracy: 0.9622 - tp: 1465638.0000 - fp: 53378.0000 - tn: 6055078.0000 - fn: 61476.0000 - categorical_accuracy: 0.9622 - precision: 0.9649 - recall: 0.9597 - auc: 0.9932 - categorical_crossentropy: 0.1050 - val_loss: 0.1468 - val_accuracy: 0.9545 - val_tp: 265279.0000 - val_fp: 11791.0000 - val_tn: 1102745.0000 - val_fn: 13355.0000 - val_categorical_accuracy: 0.9545 - val_precision: 0.9574 - val_recall: 0.9521 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1263 - 24s/epoch - 34ms/step\n",
      "Epoch 50/60\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      "Epoch 50: val_loss improved from 0.14668 to 0.14632, saving model to ./Checkpoints\\Single_LSTM_size_200___weights.auc:0.9931873083114624-0.962372--val-0.9906293749809265-0.954686.h5\n",
      "707/707 - 25s - loss: 0.1618 - accuracy: 0.9624 - tp: 1466003.0000 - fp: 53165.0000 - tn: 6055291.0000 - fn: 61111.0000 - categorical_accuracy: 0.9624 - precision: 0.9650 - recall: 0.9600 - auc: 0.9932 - categorical_crossentropy: 0.1046 - val_loss: 0.1463 - val_accuracy: 0.9547 - val_tp: 265320.0000 - val_fp: 11757.0000 - val_tn: 1102779.0000 - val_fn: 13314.0000 - val_categorical_accuracy: 0.9547 - val_precision: 0.9576 - val_recall: 0.9522 - val_auc: 0.9906 - val_categorical_crossentropy: 0.1259 - 25s/epoch - 35ms/step\n",
      "Epoch 50: early stopping\n",
      "Single_LSTM_size_200___FullFeats_win3_\n",
      "340/340 [==============================] - 6s 15ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.986189  0.965925  0.975952  421748.000000\n",
      "1              0.944479  0.968869  0.956519  268545.000000\n",
      "2              0.868672  0.900822  0.884455   43528.000000\n",
      "3              0.751825  0.919643  0.827309     112.000000\n",
      "4              0.685252  0.826464  0.749263     461.000000\n",
      "accuracy       0.963048  0.963048  0.963048       0.963048\n",
      "macro avg      0.847283  0.916345  0.878700  734394.000000\n",
      "weighted avg   0.963747  0.963048  0.963258  734394.000000\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Saline-KA Holdout Testing Dataset\n",
      "129/129 [==============================] - 2s 16ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.975091  0.965878  0.970462  132934.000000\n",
      "1              0.960220  0.951000  0.955588  125591.000000\n",
      "2              0.803110  0.901437  0.849438   20109.000000\n",
      "accuracy       0.954521  0.954521  0.954521       0.954521\n",
      "macro avg      0.912807  0.939438  0.925163  278634.000000\n",
      "weighted avg   0.955976  0.954521  0.955023  278634.000000\n",
      "unique ground truth labels: [0, 1, 2]\n",
      "unique predicted labels: [0, 1, 2]\n",
      "Saline Control Validation Dataset\n",
      "707/707 [==============================] - 11s 15ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.982209  0.967855  0.974979  7.920640e+05\n",
      "1              0.958419  0.961718  0.960066  6.297490e+05\n",
      "2              0.859387  0.934896  0.895552  1.036950e+05\n",
      "3              0.743869  0.903974  0.816143  3.020000e+02\n",
      "4              0.816938  0.961656  0.883410  1.304000e+03\n",
      "accuracy       0.963068  0.963068  0.963068  9.630682e-01\n",
      "macro avg      0.872164  0.946020  0.906030  1.527114e+06\n",
      "weighted avg   0.963870  0.963068  0.963326  1.527114e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training Dataset\n",
      "1176/1176 [==============================] - 18s 16ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.982742  0.967055  0.974836  1.346746e+06\n",
      "1              0.954914  0.962279  0.958582  1.023885e+06\n",
      "2              0.854672  0.922011  0.887066  1.673320e+05\n",
      "3              0.746032  0.908213  0.819172  4.140000e+02\n",
      "4              0.781923  0.926346  0.848029  1.765000e+03\n",
      "accuracy       0.962125  0.962125  0.962125  9.621250e-01\n",
      "macro avg      0.864056  0.937181  0.897537  2.540142e+06\n",
      "weighted avg   0.962910  0.962125  0.962389  2.540142e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training, Validation, and Testing Overall\n"
     ]
    }
   ],
   "source": [
    "Channel_Masks=[[mask_ECoG, 'ECoG_only'],\n",
    "               [np.concatenate((mask_HPCL,mask_HPCR)), '2xHPC'],\n",
    "               [mask_HPCL, 'HPCL'],\n",
    "               [np.concatenate((mask_EMG,mask_HPCL,mask_HPCR,mask_RMS)), 'no_ECoG'],\n",
    "               [np.concatenate((mask_EMG,mask_RMS)), 'EMG_RMS_only' ],\n",
    "               [np.concatenate((mask_ECoG,mask_EMG,mask_RMS)), 'noHPC']]\n",
    "\n",
    "Feature_Masks = [[mask_DTRMS,'DTR+RMS only'],\n",
    "  [mask_FFT_only, 'FFT only'],\n",
    "  [mask_FullFeats, 'FullFeats']]\n",
    "\n",
    "# Only perform this loop over the window length of 3 and Full Feature mask \n",
    "\n",
    "# Generate training variables which are common between models\n",
    "batch_size, metrics, optimizer, activity_regularizer = common_training_parameters()\n",
    "\n",
    "# Number of epochs to train \n",
    "epochs=20\n",
    "\n",
    "for window_length in [3]:\n",
    "\n",
    "    for masks in [Feature_Masks[2]]:\n",
    "      \n",
    "        X_train_masked=X_train[:,masks[0]]\n",
    "        X_val_masked=X_val[:,masks[0]]\n",
    "        X_test_masked=X_test[:,masks[0]]\n",
    "    \n",
    "        # Mask name for training logs\n",
    "        submodel=masks[1]\n",
    "        print(submodel)  \n",
    "    \n",
    "        # Maximum number of features determined by last dimension in shape of masked array\n",
    "        max_feats=X_train_masked.shape[1]\n",
    "    \n",
    "        # Generate windowed sequences from full epoch arrays\n",
    "          \n",
    "        # Number of time points on either side of scored epoch to use for input variables...\n",
    "        # determined by window length.  i.e. window_length 3 leads to an input 7 epochs long\n",
    "        X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "        X_val_seq=generate_sequences(X_val_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "        X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "    \n",
    "        y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "        y_val_seq=generate_sequences(y_val, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "        y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "    \n",
    "        # Generate class weight array based on logarithmic modification of sk-learn class weighting\n",
    "        class_weight=norm_sklearn_classweight(y_train_seq, mu=False)\n",
    "        print(class_weight)\n",
    "\n",
    "        print(X_train_seq.shape)\n",
    "    \n",
    "        for neruon_num in [200]:\n",
    "            \n",
    "            # LSTM Model Training\n",
    "            model, model_history, logpath, csvpath, model_name = create_train_LSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=1, data_path=data_path)\n",
    "            gc.collect()\n",
    "            # LSTM Model Report\n",
    "            savepath='./Results_GPU/60_Epochs/LSTM/'\n",
    "            save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                                model_name=model_name+f'_{submodel}_win{window_length}_', \n",
    "                                model_num=nn, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n",
    "            gc.collect()\n",
    "\n",
    "            \n",
    "            # BiLSTM Model Training\n",
    "            model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=1, data_path=data_path)\n",
    "            \n",
    "            # BiLSTM Model Report\n",
    "            savepath='./Results_GPU/60_Epochs/BiLSTM/'\n",
    "            save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                                model_name=model_name+f'single-layer_{submodel}_win{window_length}_', \n",
    "                                model_num=nn, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n",
    "            gc.collect()\n",
    "\n",
    "            \n",
    "            # BiLSTM Model Report\n",
    "            model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=4, data_path=data_path)\n",
    "            gc.collect()\n",
    "            # BiLSTM Model Report\n",
    "            savepath='./Results_GPU/60_Epochs/Quad_BiLSTM/'\n",
    "            save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                                model_name=model_name+f'quad-layer_{submodel}_win{window_length}_', \n",
    "                                model_num=nn, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wuxMGcouipX",
    "tags": []
   },
   "source": [
    "## Train Interpretable Machine Learning Models via Channel-Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-DT5eRMUMiVf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask_DTRMS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m mask_RMS\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m80\u001b[39m,\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      7\u001b[0m Channel_Masks\u001b[38;5;241m=\u001b[39m[[mask_ECoG, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mECoG_only\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      8\u001b[0m                [np\u001b[38;5;241m.\u001b[39mconcatenate((mask_HPCL,mask_HPCR)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2xHPC\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m                [np\u001b[38;5;241m.\u001b[39mconcatenate((mask_EMG,mask_HPCL,mask_HPCR,mask_RMS)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_ECoG\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m                [np\u001b[38;5;241m.\u001b[39mconcatenate((mask_EMG,mask_RMS)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMG_RMS_only\u001b[39m\u001b[38;5;124m'\u001b[39m ],\n\u001b[0;32m     11\u001b[0m                [np\u001b[38;5;241m.\u001b[39mconcatenate((mask_ECoG,mask_EMG,mask_RMS)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoHPC\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m                [mask_HPCL, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHPCL\u001b[39m\u001b[38;5;124m'\u001b[39m],]\n\u001b[1;32m---> 14\u001b[0m Feature_Masks \u001b[38;5;241m=\u001b[39m [[\u001b[43mmask_DTRMS\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDTR+RMS only\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m                 [mask_FFT_only, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFFT only\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     16\u001b[0m                 [mask_FullFeats, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFullFeats\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Generate training variables which are common between models\u001b[39;00m\n\u001b[0;32m     19\u001b[0m batch_size, metrics, optimizer, activity_regularizer \u001b[38;5;241m=\u001b[39m common_training_parameters()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mask_DTRMS' is not defined"
     ]
    }
   ],
   "source": [
    "mask_ECoG=np.arange(0,20)\n",
    "mask_EMG=np.arange(20,40)\n",
    "mask_HPCL=np.arange(40,60)\n",
    "mask_HPCR=np.arange(60,80)\n",
    "mask_RMS=np.arange(80,100)\n",
    "\n",
    "Channel_Masks=[[mask_ECoG, 'ECoG_only'],\n",
    "               [np.concatenate((mask_HPCL,mask_HPCR)), '2xHPC'],\n",
    "               [np.concatenate((mask_EMG,mask_HPCL,mask_HPCR,mask_RMS)), 'no_ECoG'],\n",
    "               [np.concatenate((mask_EMG,mask_RMS)), 'EMG_RMS_only' ],\n",
    "               [np.concatenate((mask_ECoG,mask_EMG,mask_RMS)), 'noHPC'],\n",
    "               [mask_HPCL, 'HPCL'],]\n",
    "\n",
    "Feature_Masks = [[mask_DTRMS,'DTR+RMS only'],\n",
    "                [mask_FFT_only, 'FFT only'],\n",
    "                [mask_FullFeats, 'FullFeats']]\n",
    "\n",
    "# Generate training variables which are common between models\n",
    "batch_size, metrics, optimizer, activity_regularizer = common_training_parameters()\n",
    "\n",
    "# Number of epochs to train \n",
    "epochs=20\n",
    "\n",
    "for window_length in [3]:\n",
    "    for masks in Channel_Masks:\n",
    "        X_train_masked=X_train[:,masks[0]]\n",
    "        X_val_masked=X_val[:,masks[0]]\n",
    "        X_test_masked=X_test[:,masks[0]]\n",
    "    \n",
    "        # Mask name for training logs\n",
    "        submodel=masks[1]\n",
    "        print(submodel)  \n",
    "    \n",
    "        # Maximum number of features determined by last dimension in shape of masked array\n",
    "        max_feats=X_train_masked.shape[1]\n",
    "    \n",
    "        # Number of time points on either side of scored epoch to use for input variables...\n",
    "        # determined by window length.  i.e. window_length 3 leads to an input 7 epochs long\n",
    "        X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "        X_val_seq=generate_sequences(X_val_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "        X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "    \n",
    "        y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "        y_val_seq=generate_sequences(y_val, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "        y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "        \n",
    "        # Generate class weight array based on logarithmic modification of sk-learn class weighting\n",
    "        class_weight=norm_sklearn_classweight(y_train_seq, mu=False)\n",
    "        print(class_weight)\n",
    "        \n",
    "        print(X_train_seq.shape)\n",
    "    \n",
    "        for nn in [200]:\n",
    "            # Train Interpretable BiLSTM Model\n",
    "            gc.collect()\n",
    "            model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                        epochs=epochs, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=1, data_path=data_path)\n",
    "            gc.collect()\n",
    "            \n",
    "            # Save Interpretable BiLSTM Report\n",
    "            savepath='./Results_GPU/Interpretable/'\n",
    "            \n",
    "            save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, \n",
    "                        X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, \n",
    "                        X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                        model_name=model_name+f'single-layer_{submodel}_win{window_length}_', \n",
    "                        model_num=nn, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n",
    "            \n",
    "            save_model_report_svg(model, model_history, ctrls=2, run=2, x=None, y=None, \n",
    "                        X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, \n",
    "                        X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                        model_name=model_name+f'_{submodel}_win{window_length}', \n",
    "                        model_num=nn, data_path=data_path, savepath=savepath, textsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNFof7tMgHgN",
    "tags": []
   },
   "source": [
    "## Generate Classification Matrices for Previous Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "lbBcpwnoSoir"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_155\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_195 (Bidirect  (None, 7, 400)           481600    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dropout_233 (Dropout)       (None, 7, 400)            0         \n",
      "                                                                 \n",
      " flatten_155 (Flatten)       (None, 2800)              0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 5)                 14005     \n",
      "                                                                 \n",
      " activation_155 (Activation)  (None, 5)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 495,605\n",
      "Trainable params: 495,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "BiLSTM_size_200___BiLSTM_200_win3\n",
      "340/340 [==============================] - 7s 17ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.985964  0.968201  0.977002  421748.000000\n",
      "1              0.946782  0.969141  0.957831  268545.000000\n",
      "2              0.877756  0.901833  0.889632   43528.000000\n",
      "3              0.801527  0.937500  0.864198     112.000000\n",
      "4              0.708333  0.774403  0.739896     461.000000\n",
      "accuracy       0.964485  0.964485  0.964485       0.964485\n",
      "macro avg      0.864072  0.910216  0.885712  734394.000000\n",
      "weighted avg   0.965020  0.964485  0.964647  734394.000000\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Saline-KA Holdout Testing Dataset\n",
      "129/129 [==============================] - 2s 19ms/step\n",
      "              precision    recall  f1-score        support\n",
      "0              0.975412  0.966886  0.971130  132934.000000\n",
      "1              0.961718  0.950952  0.956305  125591.000000\n",
      "2              0.803942  0.906609  0.852195   20109.000000\n",
      "accuracy       0.955354  0.955354  0.955354       0.955354\n",
      "macro avg      0.913691  0.941482  0.926543  278634.000000\n",
      "weighted avg   0.956865  0.955354  0.955864  278634.000000\n",
      "unique ground truth labels: [0, 1, 2]\n",
      "unique predicted labels: [0, 1, 2]\n",
      "Saline Control Validation Dataset\n",
      "707/707 [==============================] - 18s 25ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.983728  0.969370  0.976496  7.920640e+05\n",
      "1              0.960403  0.964287  0.962341  6.297490e+05\n",
      "2              0.865117  0.937933  0.900055  1.036950e+05\n",
      "3              0.815029  0.933775  0.870370  3.020000e+02\n",
      "4              0.826649  0.980061  0.896842  1.304000e+03\n",
      "accuracy       0.965141  0.965141  0.965141  9.651414e-01\n",
      "macro avg      0.890185  0.957085  0.921221  1.527114e+06\n",
      "weighted avg   0.965888  0.965141  0.965380  1.527114e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training Dataset\n",
      "1176/1176 [==============================] - 27s 23ms/step\n",
      "              precision    recall  f1-score       support\n",
      "0              0.983600  0.968759  0.976123  1.346746e+06\n",
      "1              0.956931  0.963925  0.960415  1.023885e+06\n",
      "2              0.860545  0.924778  0.891506  1.673320e+05\n",
      "3              0.811321  0.934783  0.868687  4.140000e+02\n",
      "4              0.797561  0.926346  0.857143  1.765000e+03\n",
      "accuracy       0.963878  0.963878  0.963878  9.638780e-01\n",
      "macro avg      0.881992  0.943718  0.910775  2.540142e+06\n",
      "weighted avg   0.964587  0.963878  0.964117  2.540142e+06\n",
      "unique ground truth labels: [0, 1, 2, 3, 4]\n",
      "unique predicted labels: [0, 1, 2, 3, 4]\n",
      "Training, Validation, and Testing Overall\n"
     ]
    }
   ],
   "source": [
    "# Generate Classification Matrices for full-featured model \n",
    "\n",
    "# Max X arrays\n",
    "mask=mask_FullFeats \n",
    "submodel=\"FullFeats\"\n",
    "X_train_masked=X_train[:,mask]\n",
    "X_val_masked=X_val[:,mask]\n",
    "X_test_masked=X_test[:,mask]\n",
    "\n",
    "# Generate Windowed Sequences for X and Y arrays\n",
    "window_length=3 \n",
    "\n",
    "X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=100)\n",
    "X_val_seq=generate_sequences(X_val_masked, windows=window_length, x_or_y='X', max_feats=100)\n",
    "X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=100)\n",
    "\n",
    "y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=100)\n",
    "y_val_seq=generate_sequences(y_val, windows=window_length, x_or_y='Y', max_feats=100)\n",
    "y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=100)\n",
    "\n",
    "# Load model\n",
    "model_path=data_path+'Results_GPU/Final Model/Final Model BiLSTM200.h5'\n",
    "model=load_model(model_path)\n",
    "model.summary()\n",
    "model_name='BiLSTM_200'\n",
    "\n",
    "savepath=data_path+'Results_GPU/Final Model/'\n",
    "\n",
    "# Save Model Report\n",
    "save_model_report_svg(model, model_history, ctrls=2, run=2, x=None, y=None, \n",
    "                      X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, \n",
    "                      y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                      model_name=model_name+f'_{submodel}_win{window_length}', \n",
    "                      model_num=nn, data_path=data_path, savepath=savepath, textsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tRbb2-2buLN",
    "tags": []
   },
   "source": [
    "# Train SWISC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tRbb2-2buLN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "masks=[mask_FullFeats, 'FullFeats']\n",
    "\n",
    "print(f'\\n{masks[1]}')\n",
    "X_train_masked=X_train[:,masks[0]]\n",
    "X_val_masked=X_val[:,masks[0]]\n",
    "X_test_masked=X_test[:,masks[0]]\n",
    "\n",
    "submodel=masks[1]\n",
    "\n",
    "window length=3\n",
    "    \n",
    "# Maximum number of features determined by last dimension in shape of masked array\n",
    "max_feats=X_train_masked.shape[1]\n",
    "\n",
    "# Number of time points on either side of scored epoch to use for input variables...\n",
    "# determined by window length.  i.e. window_length 3 leads to an input 7 epochs long\n",
    "X_train_seq=generate_sequences(X_train_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "X_val_seq=generate_sequences(X_val_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "X_test_seq=generate_sequences(X_test_masked, windows=window_length, x_or_y='X', max_feats=max_feats)\n",
    "\n",
    "y_train_seq=generate_sequences(y_train, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "y_val_seq=generate_sequences(y_val, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "y_test_seq=generate_sequences(y_test, windows=window_length, x_or_y='Y', max_feats=max_feats)\n",
    "\n",
    "print(X_train_seq.shape)\n",
    "\n",
    "# Generate training variables which are common between models\n",
    "batch_size, metrics, optimizer, activity_regularizer = common_training_parameters()\n",
    "\n",
    "# Number of epochs to train \n",
    "epochs=20\n",
    "\n",
    "# Generate class weight array based on logarithmic modification of sk-learn class weighting\n",
    "class_weight=norm_sklearn_classweight(y_train_seq, mu=False)\n",
    "print(class_weight)\n",
    "\n",
    "for neuron_num in [200]:\n",
    "\n",
    "  gc.collect()\n",
    "    \n",
    " model, model_history, logpath, csvpath, model_name = create_train_BiLSTM_model(X_train_seq, X_val_seq, y_train_seq, y_val_seq, class_weight=class_weight, \n",
    "                              epochs=epochs, steps_per_epoch=None, layer_size=neuron_num, batch_size=batch_size, optimizer=optimizer, metrics=metrics, activity_regularizer=activity_regularizer, prefix=\"\", layers=1, data_path=data_path)\n",
    "  gc.collect()\n",
    "\n",
    "  savepath='./Results_GPU/60_Epochs/BiLSTM/'\n",
    "\n",
    "  save_model_report_seq(model, model_history, ctrls=2, run=2, x=None, y=None, X_train_seq=X_train_seq, X_val_seq=X_val_seq, y_train_seq=y_train_seq, y_val_seq=y_val_seq, X_test_seq=X_test_seq, y_test_seq=y_test_seq,\n",
    "                        model_name=model_name+f'single-layer_{submodel}_win{window_length}_', \n",
    "                        model_num=neuron_num, data_path=data_path, logpath=logpath, csvpath=csvpath, savepath=savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tRbb2-2buLN",
    "tags": []
   },
   "source": [
    "# Final Scoring of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23643,
     "status": "ok",
     "timestamp": 1681322281712,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "IWfVHX9J9bAA",
    "outputId": "5db20f5a-d9db-4313-a10a-20a5ee33fecb"
   },
   "outputs": [],
   "source": [
    "model_path=data_path+'Results_GPU/Final Model/Final Model BiLSTM200.h5'\n",
    "model=load_model(modelpath)\n",
    "model.compile()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 23643,
     "status": "ok",
     "timestamp": 1681322281712,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "IWfVHX9J9bAA",
    "outputId": "5db20f5a-d9db-4313-a10a-20a5ee33fecb"
   },
   "outputs": [],
   "source": [
    "held_cohorts = []\n",
    "\n",
    "# held_dates=held_cohorts+controls+held_dates\n",
    "held_dates=[]\n",
    "\n",
    "scoretypes=[[0,'unscored'],[1,'class_scored'],[2,'expert_scored']]\n",
    "\n",
    "errors=[]\n",
    "paths=(data_path,path_Fourier_scored,path_Fourier_unscored,path_output,path_conf)\n",
    "\n",
    "for flag, types in scoretypes:\n",
    "  print(f'Generating CSVs for {types}')\n",
    "  score_data(paths, errors, load_dropped_data=0, held_dates=[], scored=flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur7ksSSCDClB"
   },
   "source": [
    "# CSV Exports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 20779,
     "status": "ok",
     "timestamp": 1681322317472,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "-BTCzd_VIDTx",
    "outputId": "8b93ceff-b5e4-4969-a4e6-624f1ad9707c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Items left: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Items left: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Items left: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort through files and iterate over unscored files, classifier scores for previously-scored files, and manual scores for previously-scored files\n",
    "# Copy these files to a directory where they are split by animal and named by category \n",
    "\n",
    "for sel in [0,1,2]:\n",
    "\n",
    "    filepath = [path_csv,path_holes,path_expert][sel]\n",
    "    filelist=os.listdir(filepath)\n",
    "    print(len(filelist))\n",
    "    count=0\n",
    "    if count==0:\n",
    "        dh1 = display(f'Items left: {len(filelist)}',display_id=True)\n",
    "\n",
    "    for item in sorted(filelist)[::1]:\n",
    "        # Skip if file is not .csv (i.e. if it is a folder path)\n",
    "        if '.csv' in item:\n",
    "            count+=1    \n",
    "\n",
    "            # Update counter in intervals of 50\n",
    "            if count%50==0 or (len(filelist)-count)==0:\n",
    "                dh1.update(f'Items left: {len(filelist)-count}')\n",
    "    \n",
    "            ## Extract Mouse Name *specific to Pedersen Lab*\n",
    "            ind_name = item.find('NPM')\n",
    "            name = item[ind_name:ind_name+6]\n",
    "      \n",
    "            path_out=f'{mouse_sort_csvs}{name}/'\n",
    "\n",
    "            # Make mouse-specific folder\n",
    "            if os.path.exists(path_out)==0:\n",
    "                os.mkdir(path_out)\n",
    "\n",
    "            # Copy file to mouse-specific folder\n",
    "            if os.path.isfile(f'{path_out}{item}')==0:\n",
    "                shutil.copy(f'{filepath}{item}', f'{path_out}{item}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n443QNIbrrW",
    "tags": []
   },
   "source": [
    "# Rechtschaffen & Kales Scoring Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdHbLo84u3Oj"
   },
   "outputs": [],
   "source": [
    "# Specific animal numbers for Pedersen Lab KA cohorts, used for scoring evaluation and output\n",
    "KA_list=[576,577,578,580,  589,590,591,  593,594,595,  605,606,607, 608,610,611,612,\n",
    "         613,615,616,617,  618,619,620,621,622,623,624,625,626,627,628,629,630,631,\n",
    "         632,633,634,636,637,638,639,640,642,648,649,650,651,652,653,\n",
    "         654,655,656,657,658,659,661,662,663,664,665,666,667,668,688,690,691,693,695,696,697]\n",
    "\n",
    "Excl_KA=[568,573,575,577,578,591,594,616,618,619,623,627,630,631,642,658,659,660,662,665]\n",
    "Excl_no_SE=[575,589,590,605,606,607,608,610]\n",
    "Excl_death=[565,568,573,577,578,591,594,616,618,619,623,627,630,631,662,665]\n",
    "Excl_no_Epilepsy=[611,615,617,625,634,640,667,668]\n",
    "Excl_no_Epilepsy=[]\n",
    "Excluded=[630,631,641,643,648,651,655,657,658,662,663]\n",
    "Excl_WT=[614,644,645,646,647,648,649,650,651,652,653,654,655,656,657]\n",
    "Excl_early=[572,571,570,569,567,566,564]\n",
    "    Sal_pre_cannula=[569,570,571,572,574]\n",
    "\n",
    "Excluded_paper=Excl_KA+Excluded+Excl_WT+Excl_no_SE+Excl_no_Epilepsy+Excl_death\n",
    "Excluded_agreement=Excl_death+KA_pre_cannula+Excl_KA+Sal_pre_cannula\n",
    "Excluded=Excl_KA+Excluded+Excl_no_SE+Excl_no_Epilepsy+Excl_death+Excl_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdHbLo84u3Oj"
   },
   "outputs": [],
   "source": [
    "# Set figure size to 20 x 10\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "mplstyle.use('fast')\n",
    "\n",
    "# List all mouse folders\n",
    "folderlist=sorted(os.listdir(mouse_sort_csvs))\n",
    "folderlist=[f for f in sorted(os.listdir(mouse_sort_csvs)) if '.csv' not in f]\n",
    "print(folderlist)\n",
    "\n",
    "# Flags to use either expert scores or classifier scores\n",
    "use_expert=0\n",
    "skipped_type='expert_scored'\n",
    "use_type='classifier'\n",
    "\n",
    "score_label='Trained or Tested Data'\n",
    "\n",
    "slope_sum=np.zeros((4,1))\n",
    "avg_slope=np.zeros((4,1))\n",
    "\n",
    "# Display handles for R&K agreement\n",
    "dh1=display(f'agreement: ',display_id=True)\n",
    "dh2=display(f'violation: ',display_id=True)\n",
    "\n",
    "# Tracking variables for use in for loop\n",
    "global_agreement=0\n",
    "n_animals=0\n",
    "global_count=0\n",
    "violations_total=np.zeros((3))\n",
    "\n",
    "# Iterate over all animals, to evaluate by treatment group\n",
    "for Controls in [False,True]:\n",
    "    for folder in folderlist[::-1]:\n",
    "\n",
    "        # Reset flags for each animal's folder\n",
    "        KA_flag=False\n",
    "        skip=0\n",
    "        mouse_folder = f'{mouse_sort_csvs}{folder}/'\n",
    "        filelist=sorted(os.listdir(mouse_folder))\n",
    "\n",
    "        filelist = [f for f in filelist if use_type in f]   \n",
    "        \n",
    "        # Check if animal is a KA-treated animal\n",
    "        for KA in KA_list:\n",
    "            if f'{KA}' in folder:\n",
    "                KA_flag=True\n",
    "\n",
    "        # Check if animal is in exclusion list\n",
    "        for Excl in Excluded:\n",
    "            if f'{Excl}' in folder:\n",
    "                skip=1\n",
    "\n",
    "        # If animal matches treatment group flag and is not excluded then\n",
    "        if KA_flag!=Controls and skip==0:\n",
    "            if len(filelist)>0:\n",
    "                # Iterate over all files\n",
    "                for item in filelist[::1]:\n",
    "                    path = open(f'{mouse_folder}{item}')\n",
    "\n",
    "                    # Load only classifier-scored files\n",
    "                    if 'expert_scored' not in item:\n",
    "                        y_pred = np.loadtxt(path, delimiter=\",\",dtype='int')   \n",
    "\n",
    "                    # Run R&K evaluation algorithm and log agreements\n",
    "                    agree_pct, agreement, violations_total = R_and_K_evaluation(y_pred, violations_total):\n",
    "                    \n",
    "                    # Add agreement percent to running tally\n",
    "                    global_agreement+=agree_pct            \n",
    "\n",
    "                    global_count+=1\n",
    "                    # Calculate global agreement, and update display with this along with type of R&K violations\n",
    "                    dh1.update(f'{global_agreement/global_count}')\n",
    "                    dh2.update(f'violations: {np.sum(violations_total)}')\n",
    "\n",
    "# Print total file count\n",
    "print(global_count)\n",
    "# Print percent of R&K errors\n",
    "print(1-global_agreement/global_count)\n",
    "# Print total number of R&K errors\n",
    "print(violations_total)\n",
    "# Print relative percentage of types of R&K errors\n",
    "print(violations_total/(np.sum(violations_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpkCJZLiWyFt",
    "tags": []
   },
   "source": [
    "# Plot Agreement Between Expert and Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YbVxBz9JXpT",
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "9YbVxBz9JXpT"
   },
   "outputs": [],
   "source": [
    "del init_displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "9YbVxBz9JXpT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.style as mplstyle\n",
    "\n",
    "\n",
    "def get_figure_params(mouse_sort_csvs,use_expert):\n",
    "    folderlist=sorted(os.listdir(mouse_sort_csvs))\n",
    "    folderlist=[f for f in sorted(os.listdir(mouse_sort_csvs)) if '.csv' not in f]\n",
    "\n",
    "    print(folderlist)\n",
    "    KA_pre_cannula=[554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,573,576]\n",
    "\n",
    "    KA_list_paper=[576,577,578, 589,590,591,  593,594,595,  605,606,607, 608,610,611,612,\n",
    "         613,615,616,617,  618,619,620,621,622,623,624,625,626,627,628,629,630,631,\n",
    "         632,633,634,635,636,637,638,639,640,642,648,649,650,651,652,653,\n",
    "         654,655,656,657,658,659,661,662,663,664,665,666,667,668,688,690,691,693,695,696,697]\n",
    "\n",
    "    # returned values\n",
    "    if use_expert==1:\n",
    "      skipped_type='classifier'\n",
    "      use_type='expert_scored'\n",
    "\n",
    "      score_label='Expert Scoring'\n",
    "    else:\n",
    "      skipped_type='expert_scored'\n",
    "      use_type='classifier'\n",
    "\n",
    "      score_label='Trained or Tested Data'\n",
    "    \n",
    "    return folderlist, use_type, skipped_type, KA_list_paper, KA_pre_cannula\n",
    "    \n",
    "    \n",
    "# def init_displays():\n",
    "#     dh_no=display(f'file: ',display_id=True)\n",
    "#     dh1=display(f'agreement: ',display_id=True)\n",
    "#     dh_maxagree=display(f'max agreement: ',display_id=True)\n",
    "#     dh_minagree=display(f'min agreement: ',display_id=True)\n",
    "#     dh_stddev=display(f'std dev of agreement: ',display_id=True)\n",
    "\n",
    "#     dh_conf=display(f'confidence: ',display_id=True)\n",
    "    \n",
    "#     return dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf\n",
    "\n",
    "\n",
    "\n",
    "def init_vars():\n",
    "    global_agreement=0\n",
    "    global_confidence=0\n",
    "    max_agree=0\n",
    "    min_agree=100\n",
    "    n_animals=0\n",
    "    \n",
    "    return global_agreement, global_confidence, max_agree, min_agree, n_animals\n",
    "\n",
    "\n",
    "\n",
    "def update_vars(array, array2, confidence_gradient, min_agree, max_agree, global_agreement, agreement_list, std_dev, global_confidence, count, global_count):\n",
    "    agreement = array==array2[3:-3]\n",
    "\n",
    "    agreement_score=np.zeros(len(agreement))\n",
    "    agreement_score[agreement==True]=1\n",
    "    agreement_score[agreement==False]=0\n",
    "\n",
    "    maxlen=len(array)\n",
    "    agree_pct=len(agreement[agreement==True])/(maxlen-6)\n",
    "    agreement_list.append(agree_pct)\n",
    "    if len(agreement_list)>1:\n",
    "        std_dev=np.std(agreement_list)\n",
    "\n",
    "    if agree_pct>max_agree:\n",
    "            max_agree=agree_pct\n",
    "    if agree_pct<min_agree:\n",
    "            min_agree=agree_pct\n",
    "\n",
    "    if np.isnan(confidence_gradient.mean())==False:\n",
    "                    global_confidence=global_confidence+confidence_gradient.mean()\n",
    "\n",
    "    count+=1\n",
    "    global_count+=1\n",
    "    \n",
    "    global_agreement=global_agreement+agree_pct\n",
    "    \n",
    "    return agreement, min_agree, max_agree, global_agreement, agreement_list, std_dev, global_confidence, count, global_count\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def update_displays(min_agree, max_agree, global_agreement, std_dev, global_confidence, count, global_count):\n",
    "    dh_no.update(f'file: {global_count}')\n",
    "    dh1.update(f'agreement: {global_agreement/global_count}')\n",
    "    dh_maxagree.update(f'max agreement: {max_agree}')\n",
    "    dh_minagree.update(f'min agreement: {min_agree}')\n",
    "    dh_stddev.update(f'std dev of agreement: {std_dev}')\n",
    "    if np.isnan(confidence_gradient.mean())==False:\n",
    "            dh_conf.update(f'confidence: {global_confidence/global_count}')\n",
    "    \n",
    "    return dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf\n",
    "\n",
    "def load_expert_and_classifier(mouse_folder, item):\n",
    "    path = open(f'{mouse_folder}{item}')\n",
    "\n",
    "    if 'expert_scored' in item:\n",
    "        expert_array = np.loadtxt(path, delimiter=\",\",dtype='float')\n",
    "        expert_array = expert_array.astype('int')\n",
    "        classifier_filename=item.replace(\"expert\", \"classifier\")\n",
    "     \n",
    "    else:\n",
    "        array2 = np.loadtxt(path, delimiter=\",\",dtype='int')\n",
    "        item2=item\n",
    "\n",
    "    path = open(f'{mouse_folder}{classifier_filename}')\n",
    "    classifier_array = np.loadtxt(path, delimiter=\",\",dtype='int')     \n",
    "\n",
    "    idx_name=item.find('npy_')\n",
    "    conf_item='conf_'+item[:idx_name-1]+'.npy'\n",
    "    conf = np.load(path_conf+conf_item)\n",
    "\n",
    "    return expert_array, classifier_array, conf\n",
    "\n",
    "def plot_agreement_figure(array, array2, agreement, agreement_score, confidence_gradient):\n",
    "                x1 = np.arange(0,len(array2))\n",
    "\n",
    "                plt.figure(figsize=(width,height), dpi=2000)\n",
    "                ax1=plt.subplot(2,4, (1,3))    \n",
    "# Ax1\n",
    "                \n",
    "                ax1.bar(x1[3:maxlen-3], agreement_score,color='k', width=1)\n",
    "                ax1.bar(np.ma.masked_where(~agreement, x1[3:maxlen-3]), np.ma.masked_where(agreement, agreement_score+1), color='grey', width=1)\n",
    "\n",
    "                # ax1.title.set_text(f'Sleep Record Scoring Comparision')\n",
    "\n",
    "                black_patch = mpatches.Patch(color='k', label='Agreement')\n",
    "                grey_patch = mpatches.Patch(color='grey', label='Difference')\n",
    "\n",
    "                ax1.legend(handles=[black_patch, grey_patch], bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "                ax1.text(x_legend_offset, .5, f'Classifier and \\n Expert \\n Agreement', horizontalalignment='center', \n",
    "                                 verticalalignment='center', transform=ax1.transAxes)\n",
    "                ax1.set_xticks(range(0,maxlen+int(maxlen/12),int(maxlen/12)))\n",
    "\n",
    "                if Z_time=='night':\n",
    "                    time_vect=[str(x)+\":00\" for x in list(range(19,24))+list(range(0,8))]\n",
    "                else:\n",
    "                    time_vect=[str(x)+\":00\" for x in range(7,20)]\n",
    "\n",
    "\n",
    "                ax1.set_xticklabels(time_vect)\n",
    "\n",
    "# Ax2\n",
    "\n",
    "                ax2=plt.subplot(2,4,(5,7))    \n",
    "\n",
    "                codes=['PI', 'Sz', 'W', 'N', 'R']\n",
    "                types=max(array)\n",
    "                # print(f'max legend = {types}')         \n",
    "                for i in range(len(array)):\n",
    "                        if confidence_gradient[i]>0:\n",
    "                                confidence=confidence_gradient[i][0]\n",
    "                                c=round(confidence,2)\n",
    "                        else:\n",
    "                                c=.1\n",
    "                        ax2.vlines(x=i,ymin=3,ymax=3.5, color=[0,c,0])\n",
    "\n",
    "                array[array==4]=-2\n",
    "                array[array==3]=-1\n",
    "\n",
    "                array2[array2==4]=-2\n",
    "                array2[array2==3]=-1\n",
    "                \n",
    "                ax2.plot(x1[3:maxlen-3], array, 'red',linewidth=.3)\n",
    "                ax2.plot(x1[3:maxlen-3], array2[3:maxlen-3], color='grey',linewidth=.3)\n",
    "\n",
    "# agreement plotting\n",
    "\n",
    "                grey_patch = mpatches.Patch(color='grey', label='Expert Scoring')\n",
    "                black_patch = mpatches.Patch(color='k', label='Classifier Scoring')\n",
    "                ax2.legend(handles=[black_patch, grey_patch], bbox_to_anchor=(1.02, 1),\n",
    "                                                 loc='upper left', borderaxespad=0.)\n",
    "\n",
    "                ax2.text(x_legend_offset, .5, f'Hypnogram', horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)\n",
    "                ax2.set_xticks(range(0,maxlen+int(maxlen/12),int(maxlen/12)))\n",
    "                ax2.set_xticklabels(time_vect)\n",
    "                ax2.set_xlabel(\"Time\")\n",
    "                if Z_time=='night':\n",
    "                    ax2.set_facecolor(\"lightgray\")\n",
    "# y tick assignment            \n",
    "                \n",
    "                ticklist=ax2.get_yticks()\n",
    "                # print(ticklist)\n",
    "                low_legend=min([min(array), min(array2)])\n",
    "                # print(f'low legend = {low_legend}')                \n",
    "\n",
    "                high_legend=max([max(array), max(array2)])\n",
    "                # print(f'high legend = {high_legend}')\n",
    "\n",
    "\n",
    "\n",
    "                ax2.set_yticks(range(-2,3))\n",
    "                ax2.set_yticklabels(labels=codes)\n",
    "                ax2.invert_yaxis()                \n",
    "\n",
    "\n",
    "# Ax3\n",
    "                ax3=plt.subplot(2,4, (4))\n",
    "                ax3.pie([agree_pct,1-agree_pct], colors=['k', 'grey'],autopct='%1.1f%%', pctdistance=1.7, center=(0,-5))\n",
    "                pie = ax3.get_position()\n",
    "                pie.y0 = pie.y0 - 0.05\n",
    "                pie.y1 = pie.y1 - 0.05\n",
    "\n",
    "                ax3.set_position(pie)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "9YbVxBz9JXpT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.style as mplstyle\n",
    "\n",
    "\n",
    "def get_figure_params(mouse_sort_csvs,use_expert):\n",
    "    KA_pre_cannula=[554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,573,576]\n",
    "\n",
    "    KA_list_paper=[576,577,578, 589,590,591,  593,594,595,  605,606,607, 608,610,611,612,\n",
    "             613,615,616,617,  618,619,620,621,622,623,624,625,626,627,628,629,630,631,\n",
    "             632,633,634,635,636,637,638,639,640,642,648,649,650,651,652,653,\n",
    "             654,655,656,657,658,659,661,662,663,664,665,666,667,668,688,690,691,693,695,696,697]\n",
    "\n",
    "    Excl_KA=[568,573,575,578,580,591,594,616,618,619,623,627,630,631,642,658,659,660,662,665]\n",
    "    Excl_no_SE=[575,589,590,605,606,607,608,610]\n",
    "    # Excl_no_SE=[]\n",
    "    Excl_death=[565,568,573,578,591,594,616,618,619,623,627,630,631,662,665]\n",
    "    Excl_no_Epilepsy=[611,615,617,625,634,640,667,668]\n",
    "    Excl_no_Epilepsy=[]\n",
    "    Excluded=[630,631,641,643,648,651,655,657,658,662,663,577]\n",
    "    Excl_WT=[614,644,645,646,647,648,649,650,651,652,653,654,655,656,657]\n",
    "    Sal_pre_cannula=[569,570,571,572,574]\n",
    "\n",
    "    # returned value\n",
    "    Excluded_paper=Excl_KA+Excluded+Excl_WT+Excl_no_SE+Excl_no_Epilepsy+Excl_death\n",
    "    Excluded_agreement=Excl_death+KA_pre_cannula+Excl_KA+Sal_pre_cannula\n",
    "    folderlist=sorted(os.listdir(mouse_sort_csvs))\n",
    "    # returned value\n",
    "    folderlist=[f for f in sorted(os.listdir(mouse_sort_csvs)) if '.csv' not in f]\n",
    "\n",
    "    print(folderlist)\n",
    "\n",
    "    \n",
    "    # returned values\n",
    "    if use_expert==1:\n",
    "      skipped_type='classifier'\n",
    "      use_type='expert_scored'\n",
    "\n",
    "      score_label='Expert Scoring'\n",
    "    else:\n",
    "      skipped_type='expert_scored'\n",
    "      use_type='classifier'\n",
    "\n",
    "      score_label='Trained or Tested Data'\n",
    "    \n",
    "    return folderlist, use_type, skipped_type, Excluded_paper, Excluded_agreement, KA_list_paper, KA_pre_cannula\n",
    "    \n",
    "    \n",
    "def init_displays():\n",
    "    dh_no=display(f'file: ',display_id=True)\n",
    "    dh1=display(f'agreement: ',display_id=True)\n",
    "    dh_maxagree=display(f'max agreement: ',display_id=True)\n",
    "    dh_minagree=display(f'min agreement: ',display_id=True)\n",
    "    dh_stddev=display(f'std dev of agreement: {std_dev}',display_id=True)\n",
    "\n",
    "    dh_conf=display(f'confidence: ',display_id=True)\n",
    "    \n",
    "    return dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf\n",
    "\n",
    "\n",
    "\n",
    "def init_vars():\n",
    "    global_agreement=0\n",
    "    global_confidence=0\n",
    "    max_agree=0\n",
    "    min_agree=100\n",
    "    n_animals=0\n",
    "    \n",
    "    return global_agreement, global_confidence, max_agree, min_agree, n_animals\n",
    "\n",
    "\n",
    "\n",
    "def update_vars(array, array2, confidence_gradient, min_agree, max_agree, global_agreement, agreement_list, std_dev, global_confidence, count, global_count):\n",
    "    agreement = array==array2[3:-3]\n",
    "\n",
    "    agreement_score=np.zeros(len(agreement))\n",
    "    agreement_score[agreement==True]=1\n",
    "    agreement_score[agreement==False]=0\n",
    "\n",
    "    maxlen=len(array)\n",
    "    agree_pct=len(agreement[agreement==True])/(maxlen-6)\n",
    "    agreement_list.append(agree_pct)\n",
    "    if len(agreement_list)>1:\n",
    "        std_dev=np.std(agreement_list)\n",
    "    # print(len(agreement[agreement==False]))\n",
    "    # print(f'agreement:{agree_pct}')\n",
    "    # print(np.corrcoef( array, array2[3:-3])[0,1])\n",
    "\n",
    "    # print(f'mean confidence: {confidence_gradient.mean()}')\n",
    "\n",
    "    if agree_pct>max_agree:\n",
    "            max_agree=agree_pct\n",
    "    if agree_pct<min_agree:\n",
    "            min_agree=agree_pct\n",
    "\n",
    "    # print(f'agreement:{agree_pct}')\n",
    "\n",
    "    # print(confidence_gradient[array==0])\n",
    "\n",
    "#                 np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#                 display(pd.DataFrame([emg_trace, array]).transpose())\n",
    "#                 np.set_printoptions(threshold=False)\n",
    "\n",
    "    if np.isnan(confidence_gradient.mean())==False:\n",
    "                    global_confidence=global_confidence+confidence_gradient.mean()\n",
    "\n",
    "    count+=1\n",
    "    global_count+=1\n",
    "    \n",
    "    global_agreement=global_agreement+agree_pct\n",
    "    \n",
    "    return agreement, min_agree, max_agree, global_agreement, agreement_list, std_dev, global_confidence, count, global_count\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def update_displays(min_agree, max_agree, global_agreement, std_dev, global_confidence, count, global_count):\n",
    "    dh_no.update(f'file: {global_count}')\n",
    "    dh1.update(f'agreement: {global_agreement/global_count}')\n",
    "    dh_maxagree.update(f'max agreement: {max_agree}')\n",
    "    dh_minagree.update(f'min agreement: {min_agree}')\n",
    "    dh_stddev.update(f'std dev of agreement: {std_dev}')\n",
    "    if np.isnan(confidence_gradient.mean())==False:\n",
    "            dh_conf.update(f'confidence: {global_confidence/global_count}')\n",
    "    \n",
    "    return dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_agreement_figure(array, array2, agreement, agreement_score, confidence_gradient):\n",
    "                x1 = np.arange(0,len(array2))\n",
    "\n",
    "                plt.figure(figsize=(width,height), dpi=2000)\n",
    "                ax1=plt.subplot(2,4, (1,3))    \n",
    "# Ax1\n",
    "                \n",
    "                ax1.bar(x1[3:maxlen-3], agreement_score,color='k', width=1)\n",
    "                ax1.bar(np.ma.masked_where(~agreement, x1[3:maxlen-3]), np.ma.masked_where(agreement, agreement_score+1), color='grey', width=1)\n",
    "\n",
    "                # ax1.title.set_text(f'Sleep Record Scoring Comparision')\n",
    "\n",
    "                black_patch = mpatches.Patch(color='k', label='Agreement')\n",
    "                grey_patch = mpatches.Patch(color='grey', label='Difference')\n",
    "\n",
    "                ax1.legend(handles=[black_patch, grey_patch], bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "                ax1.text(x_legend_offset, .5, f'Classifier and \\n Expert \\n Agreement', horizontalalignment='center', \n",
    "                                 verticalalignment='center', transform=ax1.transAxes)\n",
    "                ax1.set_xticks(range(0,maxlen+int(maxlen/12),int(maxlen/12)))\n",
    "\n",
    "                if Z_time=='night':\n",
    "                    time_vect=[str(x)+\":00\" for x in list(range(19,24))+list(range(0,8))]\n",
    "                else:\n",
    "                    time_vect=[str(x)+\":00\" for x in range(7,20)]\n",
    "\n",
    "\n",
    "                ax1.set_xticklabels(time_vect)\n",
    "\n",
    "# Ax2\n",
    "\n",
    "                ax2=plt.subplot(2,4,(5,7))    \n",
    "\n",
    "                codes=['PI', 'Sz', 'W', 'N', 'R']\n",
    "                types=max(array)\n",
    "                # print(f'max legend = {types}')         \n",
    "                for i in range(len(array)):\n",
    "                        if confidence_gradient[i]>0:\n",
    "                                confidence=confidence_gradient[i][0]\n",
    "                                c=round(confidence,2)\n",
    "                        else:\n",
    "                                c=.1\n",
    "                        ax2.vlines(x=i,ymin=3,ymax=3.5, color=[0,c,0])\n",
    "\n",
    "                array[array==4]=-2\n",
    "                array[array==3]=-1\n",
    "\n",
    "                array2[array2==4]=-2\n",
    "                array2[array2==3]=-1\n",
    "                \n",
    "                ax2.plot(x1[3:maxlen-3], array, 'red',linewidth=.3)\n",
    "                ax2.plot(x1[3:maxlen-3], array2[3:maxlen-3], color='grey',linewidth=.3)\n",
    "\n",
    "# agreement plotting\n",
    "\n",
    "                grey_patch = mpatches.Patch(color='grey', label='Expert Scoring')\n",
    "                black_patch = mpatches.Patch(color='k', label='Classifier Scoring')\n",
    "                ax2.legend(handles=[black_patch, grey_patch], bbox_to_anchor=(1.02, 1),\n",
    "                                                 loc='upper left', borderaxespad=0.)\n",
    "\n",
    "                ax2.text(x_legend_offset, .5, f'Hypnogram', horizontalalignment='center', verticalalignment='center', transform=ax2.transAxes)\n",
    "                ax2.set_xticks(range(0,maxlen+int(maxlen/12),int(maxlen/12)))\n",
    "                ax2.set_xticklabels(time_vect)\n",
    "                ax2.set_xlabel(\"Time\")\n",
    "                if Z_time=='night':\n",
    "                    ax2.set_facecolor(\"lightgray\")\n",
    "# y tick assignment            \n",
    "                \n",
    "                ticklist=ax2.get_yticks()\n",
    "                # print(ticklist)\n",
    "                low_legend=min([min(array), min(array2)])\n",
    "                # print(f'low legend = {low_legend}')                \n",
    "\n",
    "                high_legend=max([max(array), max(array2)])\n",
    "                # print(f'high legend = {high_legend}')\n",
    "\n",
    "\n",
    "\n",
    "                ax2.set_yticks(range(-2,3))\n",
    "                ax2.set_yticklabels(labels=codes)\n",
    "                ax2.invert_yaxis()                \n",
    "\n",
    "\n",
    "# Ax3\n",
    "                ax3=plt.subplot(2,4, (4))\n",
    "                ax3.pie([agree_pct,1-agree_pct], colors=['k', 'grey'],autopct='%1.1f%%', pctdistance=1.7, center=(0,-5))\n",
    "                pie = ax3.get_position()\n",
    "                pie.y0 = pie.y0 - 0.05\n",
    "                pie.y1 = pie.y1 - 0.05\n",
    "\n",
    "                ax3.set_position(pie)\n",
    "                                \n",
    "\n",
    "\n",
    "                # plt.plot(x1[3:2160-3], array, 'b--')\n",
    "                # plt.plot(x1, array2, 'r-')\n",
    "                # plt.plot(x1[3:2160-3], agreement_score)\n",
    "                # figname=f'{path_figs}{folder}/{item}_agreement.jpg'\n",
    "                # plt.savefig(figname, bbox_inches='tight')\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 992588,
     "status": "ok",
     "timestamp": 1681342023309,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "fWSzSWrZofvI",
    "outputId": "b2a0d1bd-41df-435e-980a-a61c0090ccbe"
   },
   "source": [
    "## Print Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 992588,
     "status": "ok",
     "timestamp": 1681342023309,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "fWSzSWrZofvI",
    "outputId": "b2a0d1bd-41df-435e-980a-a61c0090ccbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NPM564', 'NPM565', 'NPM566', 'NPM567', 'NPM569', 'NPM570', 'NPM571', 'NPM572', 'NPM573', 'NPM574', 'NPM575', 'NPM576', 'NPM577', 'NPM578', 'NPM579', 'NPM580', 'NPM589', 'NPM590', 'NPM591', 'NPM592', 'NPM593', 'NPM594', 'NPM595', 'NPM596', 'NPM604', 'NPM605', 'NPM606', 'NPM607', 'NPM608', 'NPM609', 'NPM610', 'NPM611', 'NPM612', 'NPM613', 'NPM614', 'NPM615', 'NPM616', 'NPM617', 'NPM618', 'NPM619', 'NPM620', 'NPM621', 'NPM622', 'NPM623', 'NPM624', 'NPM625', 'NPM626', 'NPM627', 'NPM628', 'NPM629', 'NPM630', 'NPM631', 'NPM632', 'NPM633', 'NPM634', 'NPM635', 'NPM636', 'NPM637', 'NPM638', 'NPM639', 'NPM640', 'NPM641', 'NPM642', 'NPM644', 'NPM645', 'NPM646', 'NPM647', 'NPM648', 'NPM649', 'NPM650', 'NPM651', 'NPM652', 'NPM653', 'NPM654', 'NPM655', 'NPM656', 'NPM657', 'NPM658', 'NPM659', 'NPM661', 'NPM662', 'NPM663', 'NPM664', 'NPM665', 'NPM666', 'NPM667', 'NPM668']\n",
      "[565, 568, 573, 578, 591, 594, 616, 618, 619, 623, 627, 630, 631, 662, 665, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 573, 576, 568, 573, 575, 578, 580, 591, 594, 616, 618, 619, 623, 627, 630, 631, 642, 658, 659, 660, 662, 665, 569, 570, 571, 572, 574]\n",
      "Validation Dataset Agreement\n",
      "Kainic Acid Treatment Agreement\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'agreement: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'max agreement: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'min agreement: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'std_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaline Control Agreement\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf \u001b[38;5;241m=\u001b[39m \u001b[43minit_displays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m global_agreement, global_confidence, max_agree, min_agree, n_animals \u001b[38;5;241m=\u001b[39m init_vars()\n\u001b[0;32m     33\u001b[0m agreement_list\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[1;32mIn[135], line 52\u001b[0m, in \u001b[0;36minit_displays\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m dh_maxagree\u001b[38;5;241m=\u001b[39mdisplay(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax agreement: \u001b[39m\u001b[38;5;124m'\u001b[39m,display_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m dh_minagree\u001b[38;5;241m=\u001b[39mdisplay(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin agreement: \u001b[39m\u001b[38;5;124m'\u001b[39m,display_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 52\u001b[0m dh_stddev\u001b[38;5;241m=\u001b[39mdisplay(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd dev of agreement: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_dev\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,display_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     54\u001b[0m dh_conf\u001b[38;5;241m=\u001b[39mdisplay(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence: \u001b[39m\u001b[38;5;124m'\u001b[39m,display_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf\n",
      "\u001b[1;31mNameError\u001b[0m: name 'std_dev' is not defined"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [80, 10]\n",
    "mplstyle.use('fast')\n",
    "\n",
    "\n",
    "folderlist, use_type, skipped_type, Excluded_paper, Excluded_agreement, KA_list_paper, KA_pre_cannula = get_figure_params( mouse_sort_csvs, use_expert=1 )\n",
    "\n",
    "KA_list=KA_list_paper+KA_pre_cannula\n",
    "Excluded=Excluded_agreement\n",
    "print(Excluded)\n",
    "\n",
    "# Controls: 644,645,646,647,\n",
    "slope_sum=np.zeros((4,1))\n",
    "avg_slope=np.zeros((4,1))\n",
    "\n",
    "Controls=False\n",
    "plot=True\n",
    "global_count=0\n",
    "maxlen=2160\n",
    "for validation_only in [True, False]:\n",
    "\n",
    "    if validation_only==True:\n",
    "        print('Validation Dataset Agreement')\n",
    "    else:\n",
    "        print('Whole Dataset Agreement')\n",
    "    for Controls in [False, True]:\n",
    "        if Controls==False:\n",
    "            print('Kainic Acid Treatment Agreement')\n",
    "        else:\n",
    "            print('Saline Control Agreement')\n",
    "\n",
    "        dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf = init_displays()\n",
    "        global_agreement, global_confidence, max_agree, min_agree, n_animals = init_vars()\n",
    "        agreement_list=[]\n",
    "        std_dev=0\n",
    "        global_count=0\n",
    "        for folder in folderlist[::1]:\n",
    "        \n",
    "            \n",
    "            Control=0\n",
    "            KA_flag=False\n",
    "            skip=0\n",
    "            bad_flag=0\n",
    "            Sz_occur=0\n",
    "            mouse_folder = f'{mouse_sort_csvs}{folder}/'\n",
    "            filelist=sorted(os.listdir(mouse_folder))\n",
    "        \n",
    "            count_state = np.zeros((len(filelist),6))\n",
    "            count_state_total = np.zeros((len(filelist),5))\n",
    "            cumu_sum = np.zeros((len(filelist),5))\n",
    "        \n",
    "            if os.path.isdir(f'{path_figs}{folder}')==False:\n",
    "                os.mkdir(f'{path_figs}{folder}')\n",
    "        \n",
    "            filelist = [f for f in filelist if use_type in f]     \n",
    "            for KA in KA_list:\n",
    "                if f'{KA}' in folder:\n",
    "                    KA_flag=True\n",
    "        \n",
    "            for Excl in Excluded:\n",
    "                if f'{Excl}' in folder:\n",
    "                    skip=1\n",
    "    \n",
    "            if validation_only==True:\n",
    "                if folder not in testing_names:\n",
    "                    skip=1\n",
    "                \n",
    "            if KA_flag!=Controls and skip==0:\n",
    "                print(folder)\n",
    "                sz=0\n",
    "                total = len(filelist)\n",
    "                print(total)\n",
    "                arr = np.zeros((total,2))\n",
    "                count=0\n",
    "                \n",
    "        \n",
    "                if len(filelist)>0:\n",
    "                    file_accuracy=np.ndarray((len(filelist),2), dtype='object')\n",
    "                    for item in filelist[::1]:\n",
    "                        if 'un' not in item:\n",
    "        \n",
    "\n",
    "                            \n",
    "                            idx=filelist.index(item)\n",
    "                            file_accuracy[idx,0]=item\n",
    "                            \n",
    "                            expert_array, classifier_array, conf = load_expert_and_classifier(mouse_folder, item)\n",
    "            \n",
    "                            agreement = array == array2[3:-3]\n",
    "    \n",
    "                            agree_pct=len(agreement[agreement==True])/len(agreement)\n",
    "                        \n",
    "                            file_accuracy[idx,1]=agree_pct\n",
    "            \n",
    "                            # np.savetxt(f'{basepath}/Results_GPU/{folder}_agreement_by_file.txt',file_accuracy,fmt='%s')\n",
    "                            ind_name = item.find('NPM')\n",
    "                            name = item[ind_name:ind_name+6]\n",
    "                            date = item[ind_name+7:ind_name+13]\n",
    "                            time = item[ind_name+14:ind_name+20]\n",
    "                            \n",
    "                            confidence_gradient=np.zeros(1)\n",
    "                            agreement, min_agree, max_agree, global_agreement, agreement_list, std_dev, global_confidence, count, global_count = update_vars(expert_array, classifier_array, confidence_gradient, min_agree, max_agree, global_agreement, agreement_list, std_dev, global_confidence, count, global_count)\n",
    "                            dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf = update_displays(min_agree, max_agree, global_agreement, std_dev, global_confidence, count, global_count)\n",
    "            \n",
    "            \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 992588,
     "status": "ok",
     "timestamp": 1681342023309,
     "user": {
      "displayName": "Brandon Harvey",
      "userId": "12508064718982313309"
     },
     "user_tz": 420
    },
    "id": "fWSzSWrZofvI",
    "outputId": "b2a0d1bd-41df-435e-980a-a61c0090ccbe"
   },
   "outputs": [],
   "source": [
    "folderlist, use_type, skipped_type, Excluded_paper, Excluded_agreement, KA_list_paper, KA_pre_cannula = get_figure_params( mouse_sort_csvs, use_expert=1 )\n",
    "\n",
    "KA_list=KA_list_paper+KA_pre_cannula\n",
    "Excluded=Excluded_agreement\n",
    "print(Excluded)\n",
    "\n",
    "slope_sum=np.zeros((4,1))\n",
    "avg_slope=np.zeros((4,1))\n",
    "\n",
    "Controls=False\n",
    "plot=True\n",
    "global_count=0\n",
    "maxlen=2160\n",
    "for validation_only in [True, False]:\n",
    "    if validation_only==True:\n",
    "        print('Validation Dataset Agreement')\n",
    "    else:\n",
    "        print('Whole Dataset Agreement')\n",
    "    for Controls in [False, True]:\n",
    "        if Controls==False:\n",
    "            print('Kainic Acid Treatment Agreement')\n",
    "        else:\n",
    "            print('Saline Control Agreement')\n",
    "\n",
    "        dh_no, dh1, dh_maxagree, dh_minagree, dh_stddev, dh_conf = init_displays()\n",
    "        global_agreement, global_confidence, max_agree, min_agree, n_animals = init_vars()\n",
    "        agreement_list=[]\n",
    "        std_dev=0\n",
    "        global_count=0\n",
    "        for folder in folderlist[::1]:\n",
    "            Control=0\n",
    "            KA_flag=False\n",
    "            skip=0\n",
    "            bad_flag=0\n",
    "            Sz_occur=0\n",
    "            mouse_folder = f'{mouse_sort_csvs}{folder}/'\n",
    "            filelist=sorted(os.listdir(mouse_folder))\n",
    "        \n",
    "            count_state = np.zeros((len(filelist),6))\n",
    "            count_state_total = np.zeros((len(filelist),5))\n",
    "            cumu_sum = np.zeros((len(filelist),5))\n",
    "        \n",
    "            if os.path.isdir(f'{path_figs}{folder}')==False:\n",
    "                os.mkdir(f'{path_figs}{folder}')\n",
    "        \n",
    "            filelist = [f for f in filelist if use_type in f]     \n",
    "            for KA in KA_list:\n",
    "                if f'{KA}' in folder:\n",
    "                    KA_flag=True\n",
    "        \n",
    "            for Excl in Excluded:\n",
    "                if f'{Excl}' in folder:\n",
    "                    skip=1\n",
    "    \n",
    "            if validation_only==True:\n",
    "                if folder not in testing_names:\n",
    "                    skip=1\n",
    "                \n",
    "            if KA_flag!=Controls and skip==0:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tnlkiajy4Q8x"
   ],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1gtFvBEnKQqls6_jkoXVGdVtMfp-Nd8gS",
     "timestamp": 1684798882715
    },
    {
     "file_id": "1zpbdnTY2M0uz3UOlSBARB2ZSJsDgwAFU",
     "timestamp": 1681154731635
    },
    {
     "file_id": "1SGyzMONFBtYkfsWAkNFYxl7Y-7mrmp7n",
     "timestamp": 1671046770467
    },
    {
     "file_id": "1omIYsg5uSiJuMlLRZQsB4jAtN-rybuxe",
     "timestamp": 1669239307423
    },
    {
     "file_id": "1yB1ssoUeicnSWFhQLxxXt8V_Ix5gwzzz",
     "timestamp": 1654806526194
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
